{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test-time Adaptation for Image Classification\n",
    "\n",
    "**Authors**: Davide Cavicchini, Laurence Bonat, Lorenzo Orsingher  \n",
    "**Date**: 01/06/2024\n",
    "\n",
    "## Introduction\n",
    "This report presents a study of various deep learning techniques for image classification, specifically focusing on \"Test-time Prompt Tuning\" (TPT) and \"Test Time Robustness via Adaptation and Augmentation\" (MEMO). We introduce several novel contributions, including MEMO with dropout, a simple ensemble with probability marginalization, and an ensemble of multiple models.\n",
    "\n",
    "## Reference Techniques\n",
    "### Baselines\n",
    "The results we get using RN50/RNXT, CLIP, ...\n",
    "\n",
    "### MEMO\n",
    "MEMO, similar to above but trains the whole network. (maybe put before)\n",
    "\n",
    "### Test-time Prompt Tuning (TPT)\n",
    "Test-time Prompt Tuning (TPT) is a technique that trains the embeddings used in the text prompt of CLIP to improve the classification error, by minimizing the entropy of the marginalized distribution over N augmentations of a test image.\n",
    "\n",
    "## Our Contributions\n",
    "### MEMO with Dropout\n",
    "If an image should be classified as something, then most of the features extracted should point toward it, but there might be some noise from other features that influence the result or may even strongly polarize the classification. With dropout we hope to stochastically remove these features, while keeping the correct ones that (hopefully) are the majority.\n",
    "\n",
    "This approach can also be implemented to be extremely efficient since we only need to pass the last classification head multiple times, while the image has to go through the network only once.\n",
    "\n",
    "### TPT with Alignment Steps\n",
    "Since the augmented images should be representing the same this to us, we first apply a gradient update to the image classifier to close the make the embeddings closer to each other. As TPT we apply confidence selection to take into account only relevant augmentations that hepefully do not destroy the content of the image.\n",
    "BAD :(, possible improvement: contrastive with prediction (put together embeddings that produce the same class)\n",
    "\n",
    "### Simple Ensemble with Probability Marginalization\n",
    "Since we are maximizing the probability of the class predicted by the ensemble, why not just return this? No need to backpropagate the network.\n",
    "\n",
    "### Ensemble of Multiple Models\n",
    "What if we instead of only passing the augmented images through a single network we use multiple networks to get the marginalized probability? In this way we hope to be able to have them \"cover\" for each other weaknesses.\n",
    "\n",
    "# Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/lollo/.local/lib/python3.10/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Requirement already satisfied: ftfy in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (6.2.0)\n",
      "Requirement already satisfied: regex in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /home/lollo/.local/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jjbjg5wp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jjbjg5wp\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: packaging in /home/lollo/.local/lib/python3.10/site-packages (from clip==1.0) (24.0)\n",
      "Requirement already satisfied: regex in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from clip==1.0) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /home/lollo/.local/lib/python3.10/site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from clip==1.0) (2.2.1)\n",
      "Requirement already satisfied: torchvision in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from clip==1.0) (0.17.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torch->clip==1.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.4.99)\n",
      "Requirement already satisfied: numpy in /home/lollo/.local/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lollo/miniconda3/envs/dlenv/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.v2 import AugMix\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision.models as torch_models\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from clip import load, tokenize\n",
    "import csv\n",
    "import boto3\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results\", exist_ok=True)\n",
    "RESULTS_PATH = f\"results/results_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "#set the seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "memo_tests = True\n",
    "drop_tests = True\n",
    "ensemble_tests = True\n",
    "baseline_tests = True\n",
    "augs_no_selection = 8\n",
    "augs_selection = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = 5\n",
    "# WANDB_SECRET = \"\"\n",
    "DATASET_ROOT = \"datasets\"\n",
    "\n",
    "# if WANDB_SECRET != \"\":\n",
    "#     wandb.login(key=WANDB_SECRET)\n",
    "\n",
    "base_test = {\n",
    "    \"name\": \"Base\",\n",
    "    \"dataset\": \"A\",\n",
    "    \"augs\": 64,\n",
    "    \"ttt_steps\": 1,\n",
    "    \"align_steps\": 0,\n",
    "    \"ensemble\": False,\n",
    "    \"test_stop\": -1,\n",
    "    \"confidence\": 0.10,\n",
    "    \"base_prompt\": \"A photo of a [CLS]\",\n",
    "    \"arch\": \"RN50\",\n",
    "    \"splt_ctx\": True,\n",
    "    \"lr\": 0.005,\n",
    "    \"device\": \"cuda:0\",\n",
    "}\n",
    "\n",
    "# test_step stops the testing after a certain number of samples\n",
    "# to run the entire dataset keep it at -1\n",
    "# tests using V2 are too big to fit into the GPU, look at the repo for the configurations used to get the results in the report\n",
    "tests = [\n",
    "        {\n",
    "            \"name\": \"TPT_sel_A\",\n",
    "            \"dataset\": \"A\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_nosel_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"augs\": 8,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_sel_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 0.10,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_align_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"align_steps\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_nosel_V2\",\n",
    "            \"dataset\": \"V2\",\n",
    "            \"augs\": 8,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_sel_V2\",\n",
    "            \"dataset\": \"V2\",\n",
    "            \"augs\": 64,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 0.1,\n",
    "        },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSTests = {\n",
    "        \"ImageNet-A RN50 + RNXT\": {\n",
    "            \"imageNetA\" : True,\n",
    "            \"naug\" : 64,\n",
    "            \"top\" : 0.1,\n",
    "            \"niter\" : 1,\n",
    "            \"testSingleModels\" : True,\n",
    "            \"simple_ensemble\" : True,\n",
    "            \"device\" : \"cuda\",\n",
    "            \n",
    "            \"models_type\" : [\"memo\", \"memo\"],\n",
    "            \"args\" : [\n",
    "                {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "                {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RNXT\"}\n",
    "                ],\n",
    "            \"temps\" : [1, 1],\n",
    "            \"names\" : [\"MEMO RN50\", \"MEMO RNXT\"],\n",
    "        },\n",
    "\n",
    "        \"ImageNet-V2 TPT RN50 + RNXT\": {\n",
    "            \"imageNetA\" : False,\n",
    "            \"naug\" : 64,\n",
    "            \"top\" : 0.2,\n",
    "            \"niter\" : 1,\n",
    "            \"testSingleModels\" : True,\n",
    "            \"simple_ensemble\" : True,\n",
    "            \"device\" : \"cuda\",\n",
    "            \n",
    "            \"models_type\" : [\"memo\", \"memo\"],\n",
    "            \"args\" : [\n",
    "                {\"device\": \"cuda\", \"naug\": 64, \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "                {\"device\": \"cuda\", \"naug\": 64, \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RNXT\"}\n",
    "                ],\n",
    "            \"temps\" : [1, 1],\n",
    "            \"names\" : [\"MEMO RN50\", \"MEMO RNXT\"],\n",
    "        },\n",
    "\n",
    "        \"ImageNet-A TPT + MEMO\": {\n",
    "            \"imageNetA\" : True,\n",
    "            \"naug\" : 64,\n",
    "            \"top\" : 0.2,\n",
    "            \"niter\" : 1,\n",
    "            \"testSingleModels\" : True,\n",
    "            \"simple_ensemble\" : True,\n",
    "            \"device\" : \"cuda\",\n",
    "            \n",
    "            \"models_type\" : [\"memo\", \"tpt\"],\n",
    "            \"args\" : [\n",
    "                {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "                {\"device\": \"cuda\", \"ttt_steps\": 1, \"align_steps\": 0, \"arch\": \"RN50\"}\n",
    "                ],\n",
    "            \"temps\" : [1.55, 0.7],\n",
    "            \"names\" : [\"MEMO\", \"TPT\"],\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Class names mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classes = [\"tench\", \"goldfish\", \"great white shark\", \"tiger shark\", \"hammerhead shark\", \"electric ray\", \"stingray\", \"rooster\", \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\", \"junco\", \"indigo bunting\", \"American robin\", \"bulbul\", \"jay\", \"magpie\", \"chickadee\", \"American dipper\", \"kite (bird of prey)\", \"bald eagle\", \"vulture\", \"great grey owl\", \"fire salamander\", \"smooth newt\", \"newt\", \"spotted salamander\", \"axolotl\", \"American bullfrog\", \"tree frog\", \"tailed frog\", \"loggerhead sea turtle\", \"leatherback sea turtle\", \"mud turtle\", \"terrapin\", \"box turtle\", \"banded gecko\", \"green iguana\", \"Carolina anole\", \"desert grassland whiptail lizard\", \"agama\", \"frilled-necked lizard\", \"alligator lizard\", \"Gila monster\", \"European green lizard\", \"chameleon\", \"Komodo dragon\", \"Nile crocodile\", \"American alligator\", \"triceratops\", \"worm snake\", \"ring-necked snake\", \"eastern hog-nosed snake\", \"smooth green snake\", \"kingsnake\", \"garter snake\", \"water snake\", \"vine snake\", \"night snake\", \"boa constrictor\", \"African rock python\", \"Indian cobra\", \"green mamba\", \"sea snake\", \"Saharan horned viper\", \"eastern diamondback rattlesnake\", \"sidewinder rattlesnake\", \"trilobite\", \"harvestman\", \"scorpion\", \"yellow garden spider\", \"barn spider\", \"European garden spider\", \"southern black widow\", \"tarantula\", \"wolf spider\", \"tick\", \"centipede\", \"black grouse\", \"ptarmigan\", \"ruffed grouse\", \"prairie grouse\", \"peafowl\", \"quail\", \"partridge\", \"african grey parrot\", \"macaw\", \"sulphur-crested cockatoo\", \"lorikeet\", \"coucal\", \"bee eater\", \"hornbill\", \"hummingbird\", \"jacamar\", \"toucan\", \"duck\", \"red-breasted merganser\", \"goose\", \"black swan\", \"tusker\", \"echidna\", \"platypus\", \"wallaby\", \"koala\", \"wombat\", \"jellyfish\", \"sea anemone\", \"brain coral\", \"flatworm\", \"nematode\", \"conch\", \"snail\", \"slug\", \"sea slug\", \"chiton\", \"chambered nautilus\", \"Dungeness crab\", \"rock crab\", \"fiddler crab\", \"red king crab\", \"American lobster\", \"spiny lobster\", \"crayfish\", \"hermit crab\", \"isopod\", \"white stork\", \"black stork\", \"spoonbill\", \"flamingo\", \"little blue heron\", \"great egret\", \"bittern bird\", \"crane bird\", \"limpkin\", \"common gallinule\", \"American coot\", \"bustard\", \"ruddy turnstone\", \"dunlin\", \"common redshank\", \"dowitcher\", \"oystercatcher\", \"pelican\", \"king penguin\", \"albatross\", \"grey whale\", \"killer whale\", \"dugong\", \"sea lion\", \"Chihuahua\", \"Japanese Chin\", \"Maltese\", \"Pekingese\", \"Shih Tzu\", \"King Charles Spaniel\", \"Papillon\", \"toy terrier\", \"Rhodesian Ridgeback\", \"Afghan Hound\", \"Basset Hound\", \"Beagle\", \"Bloodhound\", \"Bluetick Coonhound\", \"Black and Tan Coonhound\", \"Treeing Walker Coonhound\", \"English foxhound\", \"Redbone Coonhound\", \"borzoi\", \"Irish Wolfhound\", \"Italian Greyhound\", \"Whippet\", \"Ibizan Hound\", \"Norwegian Elkhound\", \"Otterhound\", \"Saluki\", \"Scottish Deerhound\", \"Weimaraner\", \"Staffordshire Bull Terrier\", \"American Staffordshire Terrier\", \"Bedlington Terrier\", \"Border Terrier\", \"Kerry Blue Terrier\", \"Irish Terrier\", \"Norfolk Terrier\", \"Norwich Terrier\", \"Yorkshire Terrier\", \"Wire Fox Terrier\", \"Lakeland Terrier\", \"Sealyham Terrier\", \"Airedale Terrier\", \"Cairn Terrier\", \"Australian Terrier\", \"Dandie Dinmont Terrier\", \"Boston Terrier\", \"Miniature Schnauzer\", \"Giant Schnauzer\", \"Standard Schnauzer\", \"Scottish Terrier\", \"Tibetan Terrier\", \"Australian Silky Terrier\", \"Soft-coated Wheaten Terrier\", \"West Highland White Terrier\", \"Lhasa Apso\", \"Flat-Coated Retriever\", \"Curly-coated Retriever\", \"Golden Retriever\", \"Labrador Retriever\", \"Chesapeake Bay Retriever\", \"German Shorthaired Pointer\", \"Vizsla\", \"English Setter\", \"Irish Setter\", \"Gordon Setter\", \"Brittany dog\", \"Clumber Spaniel\", \"English Springer Spaniel\", \"Welsh Springer Spaniel\", \"Cocker Spaniel\", \"Sussex Spaniel\", \"Irish Water Spaniel\", \"Kuvasz\", \"Schipperke\", \"Groenendael dog\", \"Malinois\", \"Briard\", \"Australian Kelpie\", \"Komondor\", \"Old English Sheepdog\", \"Shetland Sheepdog\", \"collie\", \"Border Collie\", \"Bouvier des Flandres dog\", \"Rottweiler\", \"German Shepherd Dog\", \"Dobermann\", \"Miniature Pinscher\", \"Greater Swiss Mountain Dog\", \"Bernese Mountain Dog\", \"Appenzeller Sennenhund\", \"Entlebucher Sennenhund\", \"Boxer\", \"Bullmastiff\", \"Tibetan Mastiff\", \"French Bulldog\", \"Great Dane\", \"St. Bernard\", \"husky\", \"Alaskan Malamute\", \"Siberian Husky\", \"Dalmatian\", \"Affenpinscher\", \"Basenji\", \"pug\", \"Leonberger\", \"Newfoundland dog\", \"Great Pyrenees dog\", \"Samoyed\", \"Pomeranian\", \"Chow Chow\", \"Keeshond\", \"brussels griffon\", \"Pembroke Welsh Corgi\", \"Cardigan Welsh Corgi\", \"Toy Poodle\", \"Miniature Poodle\", \"Standard Poodle\", \"Mexican hairless dog (xoloitzcuintli)\", \"grey wolf\", \"Alaskan tundra wolf\", \"red wolf or maned wolf\", \"coyote\", \"dingo\", \"dhole\", \"African wild dog\", \"hyena\", \"red fox\", \"kit fox\", \"Arctic fox\", \"grey fox\", \"tabby cat\", \"tiger cat\", \"Persian cat\", \"Siamese cat\", \"Egyptian Mau\", \"cougar\", \"lynx\", \"leopard\", \"snow leopard\", \"jaguar\", \"lion\", \"tiger\", \"cheetah\", \"brown bear\", \"American black bear\", \"polar bear\", \"sloth bear\", \"mongoose\", \"meerkat\", \"tiger beetle\", \"ladybug\", \"ground beetle\", \"longhorn beetle\", \"leaf beetle\", \"dung beetle\", \"rhinoceros beetle\", \"weevil\", \"fly\", \"bee\", \"ant\", \"grasshopper\", \"cricket insect\", \"stick insect\", \"cockroach\", \"praying mantis\", \"cicada\", \"leafhopper\", \"lacewing\", \"dragonfly\", \"damselfly\", \"red admiral butterfly\", \"ringlet butterfly\", \"monarch butterfly\", \"small white butterfly\", \"sulphur butterfly\", \"gossamer-winged butterfly\", \"starfish\", \"sea urchin\", \"sea cucumber\", \"cottontail rabbit\", \"hare\", \"Angora rabbit\", \"hamster\", \"porcupine\", \"fox squirrel\", \"marmot\", \"beaver\", \"guinea pig\", \"common sorrel horse\", \"zebra\", \"pig\", \"wild boar\", \"warthog\", \"hippopotamus\", \"ox\", \"water buffalo\", \"bison\", \"ram (adult male sheep)\", \"bighorn sheep\", \"Alpine ibex\", \"hartebeest\", \"impala (antelope)\", \"gazelle\", \"arabian camel\", \"llama\", \"weasel\", \"mink\", \"European polecat\", \"black-footed ferret\", \"otter\", \"skunk\", \"badger\", \"armadillo\", \"three-toed sloth\", \"orangutan\", \"gorilla\", \"chimpanzee\", \"gibbon\", \"siamang\", \"guenon\", \"patas monkey\", \"baboon\", \"macaque\", \"langur\", \"black-and-white colobus\", \"proboscis monkey\", \"marmoset\", \"white-headed capuchin\", \"howler monkey\", \"titi monkey\", \"Geoffroy's spider monkey\", \"common squirrel monkey\", \"ring-tailed lemur\", \"indri\", \"Asian elephant\", \"African bush elephant\", \"red panda\", \"giant panda\", \"snoek fish\", \"eel\", \"silver salmon\", \"rock beauty fish\", \"clownfish\", \"sturgeon\", \"gar fish\", \"lionfish\", \"pufferfish\", \"abacus\", \"abaya\", \"academic gown\", \"accordion\", \"acoustic guitar\", \"aircraft carrier\", \"airliner\", \"airship\", \"altar\", \"ambulance\", \"amphibious vehicle\", \"analog clock\", \"apiary\", \"apron\", \"trash can\", \"assault rifle\", \"backpack\", \"bakery\", \"balance beam\", \"balloon\", \"ballpoint pen\", \"Band-Aid\", \"banjo\", \"baluster / handrail\", \"barbell\", \"barber chair\", \"barbershop\", \"barn\", \"barometer\", \"barrel\", \"wheelbarrow\", \"baseball\", \"basketball\", \"bassinet\", \"bassoon\", \"swimming cap\", \"bath towel\", \"bathtub\", \"station wagon\", \"lighthouse\", \"beaker\", \"military hat (bearskin or shako)\", \"beer bottle\", \"beer glass\", \"bell tower\", \"baby bib\", \"tandem bicycle\", \"bikini\", \"ring binder\", \"binoculars\", \"birdhouse\", \"boathouse\", \"bobsleigh\", \"bolo tie\", \"poke bonnet\", \"bookcase\", \"bookstore\", \"bottle cap\", \"hunting bow\", \"bow tie\", \"brass memorial plaque\", \"bra\", \"breakwater\", \"breastplate\", \"broom\", \"bucket\", \"buckle\", \"bulletproof vest\", \"high-speed train\", \"butcher shop\", \"taxicab\", \"cauldron\", \"candle\", \"cannon\", \"canoe\", \"can opener\", \"cardigan\", \"car mirror\", \"carousel\", \"tool kit\", \"cardboard box / carton\", \"car wheel\", \"automated teller machine\", \"cassette\", \"cassette player\", \"castle\", \"catamaran\", \"CD player\", \"cello\", \"mobile phone\", \"chain\", \"chain-link fence\", \"chain mail\", \"chainsaw\", \"storage chest\", \"chiffonier\", \"bell or wind chime\", \"china cabinet\", \"Christmas stocking\", \"church\", \"movie theater\", \"cleaver\", \"cliff dwelling\", \"cloak\", \"clogs\", \"cocktail shaker\", \"coffee mug\", \"coffeemaker\", \"spiral or coil\", \"combination lock\", \"computer keyboard\", \"candy store\", \"container ship\", \"convertible\", \"corkscrew\", \"cornet\", \"cowboy boot\", \"cowboy hat\", \"cradle\", \"construction crane\", \"crash helmet\", \"crate\", \"infant bed\", \"Crock Pot\", \"croquet ball\", \"crutch\", \"cuirass\", \"dam\", \"desk\", \"desktop computer\", \"rotary dial telephone\", \"diaper\", \"digital clock\", \"digital watch\", \"dining table\", \"dishcloth\", \"dishwasher\", \"disc brake\", \"dock\", \"dog sled\", \"dome\", \"doormat\", \"drilling rig\", \"drum\", \"drumstick\", \"dumbbell\", \"Dutch oven\", \"electric fan\", \"electric guitar\", \"electric locomotive\", \"entertainment center\", \"envelope\", \"espresso machine\", \"face powder\", \"feather boa\", \"filing cabinet\", \"fireboat\", \"fire truck\", \"fire screen\", \"flagpole\", \"flute\", \"folding chair\", \"football helmet\", \"forklift\", \"fountain\", \"fountain pen\", \"four-poster bed\", \"freight car\", \"French horn\", \"frying pan\", \"fur coat\", \"garbage truck\", \"gas mask or respirator\", \"gas pump\", \"goblet\", \"go-kart\", \"golf ball\", \"golf cart\", \"gondola\", \"gong\", \"gown\", \"grand piano\", \"greenhouse\", \"radiator grille\", \"grocery store\", \"guillotine\", \"hair clip\", \"hair spray\", \"half-track\", \"hammer\", \"hamper\", \"hair dryer\", \"hand-held computer\", \"handkerchief\", \"hard disk drive\", \"harmonica\", \"harp\", \"combine harvester\", \"hatchet\", \"holster\", \"home theater\", \"honeycomb\", \"hook\", \"hoop skirt\", \"gymnastic horizontal bar\", \"horse-drawn vehicle\", \"hourglass\", \"iPod\", \"clothes iron\", \"carved pumpkin\", \"jeans\", \"jeep\", \"T-shirt\", \"jigsaw puzzle\", \"rickshaw\", \"joystick\", \"kimono\", \"knee pad\", \"knot\", \"lab coat\", \"ladle\", \"lampshade\", \"laptop computer\", \"lawn mower\", \"lens cap\", \"letter opener\", \"library\", \"lifeboat\", \"lighter\", \"limousine\", \"ocean liner\", \"lipstick\", \"slip-on shoe\", \"lotion\", \"music speaker\", \"loupe magnifying glass\", \"sawmill\", \"magnetic compass\", \"messenger bag\", \"mailbox\", \"tights\", \"one-piece bathing suit\", \"manhole cover\", \"maraca\", \"marimba\", \"mask\", \"matchstick\", \"maypole\", \"maze\", \"measuring cup\", \"medicine cabinet\", \"megalith\", \"microphone\", \"microwave oven\", \"military uniform\", \"milk can\", \"minibus\", \"miniskirt\", \"minivan\", \"missile\", \"mitten\", \"mixing bowl\", \"mobile home\", \"ford model t\", \"modem\", \"monastery\", \"monitor\", \"moped\", \"mortar and pestle\", \"graduation cap\", \"mosque\", \"mosquito net\", \"vespa\", \"mountain bike\", \"tent\", \"computer mouse\", \"mousetrap\", \"moving van\", \"muzzle\", \"metal nail\", \"neck brace\", \"necklace\", \"baby pacifier\", \"notebook computer\", \"obelisk\", \"oboe\", \"ocarina\", \"odometer\", \"oil filter\", \"pipe organ\", \"oscilloscope\", \"overskirt\", \"bullock cart\", \"oxygen mask\", \"product packet / packaging\", \"paddle\", \"paddle wheel\", \"padlock\", \"paintbrush\", \"pajamas\", \"palace\", \"pan flute\", \"paper towel\", \"parachute\", \"parallel bars\", \"park bench\", \"parking meter\", \"railroad car\", \"patio\", \"payphone\", \"pedestal\", \"pencil case\", \"pencil sharpener\", \"perfume\", \"Petri dish\", \"photocopier\", \"plectrum\", \"Pickelhaube\", \"picket fence\", \"pickup truck\", \"pier\", \"piggy bank\", \"pill bottle\", \"pillow\", \"ping-pong ball\", \"pinwheel\", \"pirate ship\", \"drink pitcher\", \"block plane\", \"planetarium\", \"plastic bag\", \"plate rack\", \"farm plow\", \"plunger\", \"Polaroid camera\", \"pole\", \"police van\", \"poncho\", \"pool table\", \"soda bottle\", \"plant pot\", \"potter's wheel\", \"power drill\", \"prayer rug\", \"printer\", \"prison\", \"missile\", \"projector\", \"hockey puck\", \"punching bag\", \"purse\", \"quill\", \"quilt\", \"race car\", \"racket\", \"radiator\", \"radio\", \"radio telescope\", \"rain barrel\", \"recreational vehicle\", \"fishing casting reel\", \"reflex camera\", \"refrigerator\", \"remote control\", \"restaurant\", \"revolver\", \"rifle\", \"rocking chair\", \"rotisserie\", \"eraser\", \"rugby ball\", \"ruler measuring stick\", \"sneaker\", \"safe\", \"safety pin\", \"salt shaker\", \"sandal\", \"sarong\", \"saxophone\", \"scabbard\", \"weighing scale\", \"school bus\", \"schooner\", \"scoreboard\", \"CRT monitor\", \"screw\", \"screwdriver\", \"seat belt\", \"sewing machine\", \"shield\", \"shoe store\", \"shoji screen / room divider\", \"shopping basket\", \"shopping cart\", \"shovel\", \"shower cap\", \"shower curtain\", \"ski\", \"balaclava ski mask\", \"sleeping bag\", \"slide rule\", \"sliding door\", \"slot machine\", \"snorkel\", \"snowmobile\", \"snowplow\", \"soap dispenser\", \"soccer ball\", \"sock\", \"solar thermal collector\", \"sombrero\", \"soup bowl\", \"keyboard space bar\", \"space heater\", \"space shuttle\", \"spatula\", \"motorboat\", \"spider web\", \"spindle\", \"sports car\", \"spotlight\", \"stage\", \"steam locomotive\", \"through arch bridge\", \"steel drum\", \"stethoscope\", \"scarf\", \"stone wall\", \"stopwatch\", \"stove\", \"strainer\", \"tram\", \"stretcher\", \"couch\", \"stupa\", \"submarine\", \"suit\", \"sundial\", \"sunglasses\", \"sunglasses\", \"sunscreen\", \"suspension bridge\", \"mop\", \"sweatshirt\", \"swim trunks / shorts\", \"swing\", \"electrical switch\", \"syringe\", \"table lamp\", \"tank\", \"tape player\", \"teapot\", \"teddy bear\", \"television\", \"tennis ball\", \"thatched roof\", \"front curtain\", \"thimble\", \"threshing machine\", \"throne\", \"tile roof\", \"toaster\", \"tobacco shop\", \"toilet seat\", \"torch\", \"totem pole\", \"tow truck\", \"toy store\", \"tractor\", \"semi-trailer truck\", \"tray\", \"trench coat\", \"tricycle\", \"trimaran\", \"tripod\", \"triumphal arch\", \"trolleybus\", \"trombone\", \"hot tub\", \"turnstile\", \"typewriter keyboard\", \"umbrella\", \"unicycle\", \"upright piano\", \"vacuum cleaner\", \"vase\", \"vaulted or arched ceiling\", \"velvet fabric\", \"vending machine\", \"vestment\", \"viaduct\", \"violin\", \"volleyball\", \"waffle iron\", \"wall clock\", \"wallet\", \"wardrobe\", \"military aircraft\", \"sink\", \"washing machine\", \"water bottle\", \"water jug\", \"water tower\", \"whiskey jug\", \"whistle\", \"hair wig\", \"window screen\", \"window shade\", \"Windsor tie\", \"wine bottle\", \"airplane wing\", \"wok\", \"wooden spoon\", \"wool\", \"split-rail fence\", \"shipwreck\", \"sailboat\", \"yurt\", \"website\", \"comic book\", \"crossword\", \"traffic or street sign\", \"traffic light\", \"dust jacket\", \"menu\", \"plate\", \"guacamole\", \"consomme\", \"hot pot\", \"trifle\", \"ice cream\", \"popsicle\", \"baguette\", \"bagel\", \"pretzel\", \"cheeseburger\", \"hot dog\", \"mashed potatoes\", \"cabbage\", \"broccoli\", \"cauliflower\", \"zucchini\", \"spaghetti squash\", \"acorn squash\", \"butternut squash\", \"cucumber\", \"artichoke\", \"bell pepper\", \"cardoon\", \"mushroom\", \"Granny Smith apple\", \"strawberry\", \"orange\", \"lemon\", \"fig\", \"pineapple\", \"banana\", \"jackfruit\", \"cherimoya (custard apple)\", \"pomegranate\", \"hay\", \"carbonara\", \"chocolate syrup\", \"dough\", \"meatloaf\", \"pizza\", \"pot pie\", \"burrito\", \"red wine\", \"espresso\", \"tea cup\", \"eggnog\", \"mountain\", \"bubble\", \"cliff\", \"coral reef\", \"geyser\", \"lakeshore\", \"promontory\", \"sandbar\", \"beach\", \"valley\", \"volcano\", \"baseball player\", \"bridegroom\", \"scuba diver\", \"rapeseed\", \"daisy\", \"yellow lady's slipper\", \"corn\", \"acorn\", \"rose hip\", \"horse chestnut seed\", \"coral fungus\", \"agaric\", \"gyromitra\", \"stinkhorn mushroom\", \"earth star fungus\", \"hen of the woods mushroom\", \"bolete\", \"corn cob\", \"toilet paper\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ImageNet-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images from the ImageNet-A dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, csvMapFile=\"wordNetIDs2Classes.csv\", transform=None\n",
    "    ):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        #print(objects)\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        mapping = {}\n",
    "        csv_file = csv.reader(open(csvMapFile, \"r\"))\n",
    "        for id, wordnet, name in csv_file:\n",
    "            if id == \"resnet_label\":\n",
    "                continue\n",
    "            mapping[int(wordnet)] = {\"id\": id, \"name\": name}\n",
    "\n",
    "        # print(mapping)\n",
    "        self.classnames = {}\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "\n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = int(path.parent.name[1:])\n",
    "            name = mapping[label][\"name\"]\n",
    "            self.classnames[mapping[label][\"id\"]] = name\n",
    "            label = int(mapping[label][\"id\"])\n",
    "\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, name, key))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, name, key = self.instances[idx]\n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            img_bytes.seek(0)  # Ensure the BytesIO object is at the start\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return {\"img\": img, \"label\": label, \"name\": name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ImageNet-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images from the ImageNet-V2 dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, csvMapFile=\"wordNetIDs2Classes.csv\", transform=None\n",
    "    ):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        mapping = {}\n",
    "        csv_file = csv.reader(open(csvMapFile, \"r\"))\n",
    "        for id, _, name in csv_file:\n",
    "            if id == \"resnet_label\":\n",
    "                continue\n",
    "            mapping[id] = name\n",
    "\n",
    "        self.classnames = {}\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "\n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "            name = mapping[label]\n",
    "            self.classnames[label] = name\n",
    "\n",
    "            label = int(label)\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, name, key))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, name, key = self.instances[idx]\n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            img_bytes.seek(0)  # Ensure the BytesIO object is at the start\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return {\"img\": img, \"label\": label, \"name\": name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EasyAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyAgumenter(object):\n",
    "    def __init__(self, base_transform, preprocess, augmentation, n_views=63):\n",
    "        self.base_transform = base_transform\n",
    "        self.preprocess = preprocess\n",
    "        self.n_views = n_views\n",
    "\n",
    "        if augmentation == 'augmix':\n",
    "\n",
    "            self.preaugment = transforms.Compose(\n",
    "                [\n",
    "                    AugMix(),\n",
    "                    transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "                    transforms.CenterCrop(224),\n",
    "                ]\n",
    "            )\n",
    "        elif augmentation == 'identity':\n",
    "            self.preaugment = self.base_transform\n",
    "        elif augmentation == 'cut':\n",
    "            self.preaugment = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Augmentation type not recognized')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = transforms.ToPILImage()(x)\n",
    "\n",
    "        image = self.preprocess(self.base_transform(x))\n",
    "\n",
    "        views = [self.preprocess(self.preaugment(x)) for _ in range(self.n_views)]\n",
    "\n",
    "        return [image] + views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(root, transform=None, csvMapFile=\"wordNetIDs2Classes.csv\"):\n",
    "    \"\"\"\n",
    "    Returns the dataloader of the dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "    root_A = \"imagenet-a\"\n",
    "    imageNet_A = ImageNetA(root_A, transform=transform, csvMapFile=csvMapFile)\n",
    "    root_V2 = \"imagenetv2-matched-frequency-format-val\"\n",
    "    imageNet_V2 = ImageNetV2(root_V2, transform=transform, csvMapFile=csvMapFile)\n",
    "\n",
    "    return imageNet_A, imageNet_V2\n",
    "\n",
    "def get_classes_names(csvMapFile=\"wordNetIDs2Classes.csv\"):\n",
    "    \"\"\"\n",
    "    Returns the class names of the dataset.\n",
    "\n",
    "    Args:\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "    \"\"\"\n",
    "    names = [\"\"]*1000\n",
    "    csv_file = csv.reader(open(csvMapFile, 'r'))\n",
    "    for id, wordnet, name in csv_file:\n",
    "        if id == 'resnet_label':\n",
    "            continue\n",
    "        names[int(id)] = name\n",
    "    \n",
    "    return names\n",
    "\n",
    "def memo_get_datasets(augmentation, augs=64):\n",
    "    \"\"\"\n",
    "    Returns the ImageNetA and ImageNetV2 datasets for the memo model\n",
    "    Args:\n",
    "        augmentation (str): What type of augmentation to use in EasyAugmenter. Can be 'augmix', 'identity' or 'cut'\n",
    "        augs (int): The number of augmentations to compute. Must be greater than 1\n",
    "\n",
    "    Returns: The ImageNetA and ImageNetV2 datasets for the memo model, with the Augmentations already applied\n",
    "\n",
    "    \"\"\"\n",
    "    assert augs > 0, 'The number of augmentations must be greater than 0'\n",
    "    memo_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224)])\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform = EasyAgumenter(memo_transforms, preprocess, augmentation, augs - 1)\n",
    "    imageNet_A, imageNet_V2 = get_dataloaders('datasets', transform)\n",
    "    return imageNet_A, imageNet_V2\n",
    "\n",
    "\n",
    "def tpt_get_transforms(augs=64):\n",
    "\n",
    "    base_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_transform = EasyAgumenter(\n",
    "        base_transform,\n",
    "        preprocess,\n",
    "        n_views=augs - 1,\n",
    "    )\n",
    "\n",
    "    return data_transform\n",
    "\n",
    "\n",
    "def tpt_get_datasets(data_root, augmix=False, augs=64, all_classes=True):\n",
    "    \"\"\"\n",
    "    Returns the ImageNetA and ImageNetV2 datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - data_root (str): The root directory of the datasets.\n",
    "    - augmix (bool): Whether to use AugMix or not.\n",
    "    - augs (int): The number of augmentations to use.\n",
    "    - all_classes (bool): Whether to use all classes or not.\n",
    "\n",
    "    Returns:\n",
    "    - imageNet_A (ImageNetA): The ImageNetA dataset.\n",
    "    - ima_names (list): The original classnames in ImageNetA.\n",
    "    - ima_custom_names (list): The retouched  classnames in ImageNetA.\n",
    "    - ima_id_mapping (list): The mapping between the index of the classname and the ImageNet label\n",
    "\n",
    "    same for ImageNetV2\n",
    "\n",
    "    For instance the first element of ima_names corresponds to the label '90'.  After running the\n",
    "    inference run the predicted output through the ima_id_mapping to recover the correct class label.\n",
    "\n",
    "    out = tpt(inputs)\n",
    "    pred = out.argmax().item()\n",
    "    out_id = ima_id_mapping[pred]\n",
    "\n",
    "    \"\"\"\n",
    "    base_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_transform = EasyAgumenter(\n",
    "        base_transform,\n",
    "        preprocess,\n",
    "        augmentation=(\"augmix\" if augmix else \"cut\"),\n",
    "        n_views=augs - 1,\n",
    "    )\n",
    "\n",
    "    imageNet_A = ImageNetA(\n",
    "        \"imagenet-a\", transform=data_transform\n",
    "    )\n",
    "    imageNet_V2 = ImageNetV2(\n",
    "        \"imagenetv2-matched-frequency-format-val\",\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "    imv2_label_mapping = list(imageNet_V2.classnames.keys())\n",
    "    imv2_names = list(imageNet_V2.classnames.values())\n",
    "    imv2_custom_names = [imagenet_classes[int(i)] for i in imv2_label_mapping]\n",
    "\n",
    "    ima_label_mapping = list(imageNet_A.classnames.keys())\n",
    "    ima_names = list(imageNet_A.classnames.values())\n",
    "    ima_custom_names = [imagenet_classes[int(i)] for i in ima_label_mapping]\n",
    "\n",
    "    if all_classes:\n",
    "        ima_names += [name for name in imv2_names if name not in ima_names]\n",
    "        ima_custom_names += [\n",
    "            name for name in imv2_custom_names if name not in ima_custom_names\n",
    "        ]\n",
    "        ima_label_mapping += [\n",
    "            map for map in imv2_label_mapping if map not in ima_label_mapping\n",
    "        ]\n",
    "\n",
    "    return (\n",
    "        imageNet_A,\n",
    "        ima_names,\n",
    "        ima_custom_names,\n",
    "        ima_label_mapping,\n",
    "        imageNet_V2,\n",
    "        imv2_names,\n",
    "        imv2_custom_names,\n",
    "        imv2_label_mapping,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EasyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EasyModel, self).__init__()\n",
    "\n",
    "    def select_confident_samples(self, logits, top):\n",
    "        \"\"\"\n",
    "        Performs confidence selection, will return the indexes of the\n",
    "        augmentations with the highest confidence as well as the filtered\n",
    "        logits\n",
    "\n",
    "        Parameters:\n",
    "        - logits (torch.Tensor): the logits of the model [NAUGS, NCLASSES]\n",
    "        - top (float): the percentage of top augmentations to use\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): the filtered logits of the model [NAUGS*top, NCLASSES]\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "        return logits[idx], idx\n",
    "    \n",
    "    def avg_entropy(self, outputs):\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model outputs\n",
    "\n",
    "        Parameters:\n",
    "        - outputs (torch.Tensor): the logits of the model [NAUGS, NCLASSES]\n",
    "        \n",
    "        Returns:\n",
    "        - avg_entropy (torch.Tensor): the average entropy of the model outputs [1]\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(EasyModel, self).forward(x)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EasyPromptLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is responsible for learning the prompt for the TPT model,\n",
    "    it takes the classnames and the base prompt and creates the prompt\n",
    "    for each class. The prompts get tokenized and embedded, the embeddings\n",
    "    of the base prompt are then used to create the context for each class.\n",
    "    It's possible to put the context in any part of the prompt\n",
    "    using the [CLS] token. It's also possible to choose wether to\n",
    "    split the context into separate learning parameters for the prefix and\n",
    "    suffix or to keep them together.\n",
    "\n",
    "    Parameters:\n",
    "    - device (str): the device to run the model\n",
    "    - clip (torch.nn.Module): the clip model\n",
    "    - base_prompt (str): the base prompt to use\n",
    "    - splt_ctx (bool): split the context or not\n",
    "    - classnames (list): the classnames to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        clip,\n",
    "        base_prompt=\"a photo of [CLS]\",\n",
    "        splt_ctx=False,\n",
    "        classnames=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.base_prompt = base_prompt\n",
    "        self.tkn_embedder = clip.token_embedding\n",
    "        self.tkn_embedder.requires_grad_(False)\n",
    "\n",
    "        self.split_ctx = splt_ctx\n",
    "\n",
    "        self.prepare_prompts(classnames)\n",
    "\n",
    "    def prepare_prompts(self, classnames):\n",
    "        \"\"\"\n",
    "        Prepares the prompts for the TPT model, this method tokenizes,\n",
    "        embeds and prepares the context for each class and the base prompt.\n",
    "\n",
    "        Parameters:\n",
    "        - classnames (list): the classnames to use\n",
    "        \"\"\"\n",
    "        print(\"[PromptLearner] Preparing prompts\")\n",
    "\n",
    "        self.classnames = classnames\n",
    "\n",
    "        # get number of classes\n",
    "        self.cls_num = len(self.classnames)\n",
    "\n",
    "        # get prompt text prefix and suffix\n",
    "        txt_prefix = self.base_prompt.split(\"[CLS]\")[0]\n",
    "        txt_suffix = self.base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # tokenize the prefix and suffix\n",
    "        tkn_prefix = tokenize(txt_prefix)\n",
    "        tkn_suffix = tokenize(txt_suffix)\n",
    "        tkn_pad = tokenize(\"\")\n",
    "        tkn_cls = tokenize(self.classnames)\n",
    "\n",
    "        # get the index of the last element of the prefix and suffix\n",
    "        idx = torch.arange(tkn_prefix.shape[1], 0, -1)\n",
    "        self.indp = torch.argmax((tkn_prefix == 0) * idx, 1, keepdim=True)\n",
    "        self.inds = torch.argmax((tkn_suffix == 0) * idx, 1, keepdim=True)\n",
    "\n",
    "        # token length for each class\n",
    "        self.indc = torch.argmax((tkn_cls == 0) * idx, 1, keepdim=True)\n",
    "\n",
    "        # get the prefix, suffix, SOT and EOT\n",
    "        self.tkn_sot = tkn_prefix[:, :1]\n",
    "        self.tkn_prefix = tkn_prefix[:, 1 : self.indp - 1]\n",
    "        self.tkn_suffix = tkn_suffix[:, 1 : self.inds - 1]\n",
    "        self.tkn_eot = tkn_suffix[:, self.inds - 1 : self.inds]\n",
    "        self.tkn_pad = tkn_pad[:, 2:]\n",
    "\n",
    "        # load segments to CUDA, be ready to be embedded\n",
    "        self.tkn_sot = self.tkn_sot.to(self.device)\n",
    "        self.tkn_prefix = self.tkn_prefix.to(self.device)\n",
    "        self.tkn_suffix = self.tkn_suffix.to(self.device)\n",
    "        self.tkn_eot = self.tkn_eot.to(self.device)\n",
    "        self.tkn_pad = self.tkn_pad.to(self.device)\n",
    "\n",
    "        self.tkn_cls = tkn_cls.to(self.device)\n",
    "\n",
    "        # gets the embeddings\n",
    "        with torch.no_grad():\n",
    "            self.emb_sot = self.tkn_embedder(self.tkn_sot)\n",
    "            self.emb_prefix = self.tkn_embedder(self.tkn_prefix)\n",
    "            self.emb_suffix = self.tkn_embedder(self.tkn_suffix)\n",
    "            self.emb_eot = self.tkn_embedder(self.tkn_eot)\n",
    "            self.emb_cls = self.tkn_embedder(self.tkn_cls)\n",
    "            self.emb_pad = self.tkn_embedder(self.tkn_pad)\n",
    "\n",
    "        # take out the embeddings of the class tokens (they are different lenghts)\n",
    "        self.all_cls = []\n",
    "        for i in range(self.cls_num):\n",
    "            self.all_cls.append(self.emb_cls[i][1 : self.indc[i] - 1])\n",
    "\n",
    "        # prepare the prompts, they are needed for text encoding\n",
    "        self.txt_prompts = [\n",
    "            self.base_prompt.replace(\"[CLS]\", cls) for cls in self.classnames\n",
    "        ]\n",
    "        self.tkn_prompts = tokenize(self.txt_prompts)\n",
    "\n",
    "        # set the inital context, this will be reused at every new inference\n",
    "        # this is the context that will be optimized\n",
    "\n",
    "        if self.split_ctx:\n",
    "            self.pre_init_state = self.emb_prefix.detach().clone()\n",
    "            self.suf_init_state = self.emb_suffix.detach().clone()\n",
    "            self.emb_prefix = nn.Parameter(self.emb_prefix)\n",
    "            self.emb_suffix = nn.Parameter(self.emb_suffix)\n",
    "            self.register_parameter(\"emb_prefix\", self.emb_prefix)\n",
    "            self.register_parameter(\"emb_suffix\", self.emb_suffix)\n",
    "        else:\n",
    "            self.ctx = torch.cat((self.emb_prefix, self.emb_suffix), dim=1)\n",
    "            self.ctx_init_state = self.ctx.detach().clone()\n",
    "            self.ctx = nn.Parameter(self.ctx)\n",
    "            self.register_parameter(\"ctx\", self.ctx)\n",
    "\n",
    "    def build_ctx(self):\n",
    "        \"\"\"\n",
    "        While the context will be optimized, the embedded classnames\n",
    "        must stay the same, this method builds the context for each class\n",
    "        at each forward pass, using the optimized context.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: the embedded prompt for each class\n",
    "        \"\"\"\n",
    "\n",
    "        prompts = []\n",
    "        for i in range(self.cls_num):\n",
    "\n",
    "            # get the size of the padding (length depends on the classname size)\n",
    "            pad_size = self.emb_cls.shape[1] - (\n",
    "                self.emb_prefix.shape[1]\n",
    "                + self.indc[i].item()\n",
    "                + self.emb_suffix.shape[1]\n",
    "            )\n",
    "\n",
    "            if self.split_ctx:\n",
    "                prefix = self.emb_prefix\n",
    "                suffix = self.emb_suffix\n",
    "            else:\n",
    "                prefix = self.ctx[:, : self.emb_prefix.shape[1]]\n",
    "                suffix = self.ctx[:, self.emb_prefix.shape[1] :]\n",
    "\n",
    "            # concatenates all elements to build the prompt\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.emb_sot,\n",
    "                    prefix,\n",
    "                    self.all_cls[i].unsqueeze(0),\n",
    "                    suffix,\n",
    "                    self.emb_eot,\n",
    "                    self.emb_pad[:, :pad_size],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self):\n",
    "        return self.build_ctx()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This functions resets the context to the initial state, it\n",
    "        has to be run before each new inference to bring the context\n",
    "        to the initial state.\n",
    "        \"\"\"\n",
    "        if self.split_ctx:\n",
    "            self.emb_prefix.data.copy_(self.pre_init_state)  # to be optimized\n",
    "            self.emb_suffix.data.copy_(self.suf_init_state)  # to be optimized\n",
    "        else:\n",
    "            self.ctx.data.copy_(self.ctx_init_state)  # to be optimized\n",
    "\n",
    "\n",
    "class EasyTPT(EasyModel):\n",
    "    \"\"\"\n",
    "    This class is the main class for the TPT, it contains\n",
    "    the logic for running the TPT model in all its configurations,\n",
    "    as well as EasyPromptLearner, which is responsible for the\n",
    "    prompt learning.\n",
    "\n",
    "    Modes:\n",
    "    - Ensemble: in this mode the model won't preform tuning steps on the prompt,\n",
    "    instead it will run the inference on all the augmentations and take the prediction\n",
    "    that maximizes probability marginalized over the agumentations.\n",
    "    - Alignment: when align_steps > 0 the model will also perform align_steps tuning\n",
    "    steps on the image encoder in an effort to minimize the distance between the\n",
    "    embeddings of the augmentations.\n",
    "\n",
    "    Parameters:\n",
    "    - device (str): the device to run the model\n",
    "    - base_prompt (str): the base prompt to use\n",
    "    - arch (str): the architecture to use for CLIP\n",
    "    - splt_ctx (bool): split the context or not\n",
    "    - classnames (list): the classnames to use\n",
    "    - ensemble (bool): run TPT in ensemble mode\n",
    "    - ttt_steps (int): number of test time tuning steps\n",
    "    - lr (float): the learning rate\n",
    "    - align_steps (int): number of alignment steps\n",
    "    - confidence (float): confidence threshold for the confidence selection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        base_prompt=\"a photo of a [CLS]\",\n",
    "        arch=\"RN50\",\n",
    "        splt_ctx=False,\n",
    "        classnames=None,\n",
    "        ensemble=False,\n",
    "        ttt_steps=1,\n",
    "        lr=0.005,\n",
    "        align_steps=0,\n",
    "        confidence=0.10,\n",
    "    ):\n",
    "        super(EasyTPT, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        ###TODO: tobe parametrized\n",
    "        DOWNLOAD_ROOT = \"~/.cache/clip\"\n",
    "        ###\n",
    "\n",
    "        self.base_prompt = base_prompt\n",
    "        self.ttt_steps = ttt_steps\n",
    "        self.selected_idx = None\n",
    "        self.ensemble = ensemble\n",
    "        self.align_steps = align_steps\n",
    "        self.confidence = confidence\n",
    "\n",
    "        # Load clip\n",
    "        clip, self.preprocess = load(\n",
    "            arch, device=device, download_root=DOWNLOAD_ROOT, jit=False\n",
    "        )\n",
    "\n",
    "        if align_steps > 0:  # clip tuning must run in float\n",
    "            clip.float()\n",
    "\n",
    "        self.clip = clip\n",
    "        self.dtype = clip.dtype\n",
    "        self.image_encoder = clip.encode_image\n",
    "        # self.text_encoder = clip.encode_text\n",
    "\n",
    "        # freeze the parameters\n",
    "        for name, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        # create the prompt learner\n",
    "        self.prompt_learner = EasyPromptLearner(\n",
    "            device, clip, base_prompt, splt_ctx, classnames\n",
    "        )\n",
    "\n",
    "        # create optimizer and save the state\n",
    "        trainable_param = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"[EasyTPT TPT] Training parameter: {name}\")\n",
    "                trainable_param.append(param)\n",
    "        self.optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "        self.optim_state = deepcopy(self.optimizer.state_dict())\n",
    "\n",
    "        if align_steps > 0:\n",
    "            emb_trainable_param = []\n",
    "            # unfreeze the image encoder\n",
    "            for name, param in self.clip.visual.named_parameters():\n",
    "                # if parameter is not attnpoll\n",
    "                if \"attnpool\" not in name:\n",
    "                    param.requires_grad_(True)\n",
    "                    emb_trainable_param.append(param)\n",
    "                    print(f\"[EasyTPT Emb] Training parameter: {name}\")\n",
    "\n",
    "            self.emb_optimizer = torch.optim.AdamW(emb_trainable_param, 0.0001)\n",
    "            self.emb_optim_state = deepcopy(self.emb_optimizer.state_dict())\n",
    "            self.clip_init_state = deepcopy(self.clip.visual.state_dict())\n",
    "\n",
    "        if self.ensemble:\n",
    "            print(\"[EasyTPT] Running TPT in Ensemble mode\")\n",
    "\n",
    "        if self.align_steps > 0:\n",
    "            print(\"[EasyTPT] Running TPT with alignment\")\n",
    "\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(f\"[EasyTPT] Training parameter: {name}\")\n",
    "\n",
    "    def forward(self, x, top=-1):\n",
    "        \"\"\"\n",
    "        If x is a list of augmentations, run the confidence selection,\n",
    "        otherwise just run the inference\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor or list): the image(s) to run the inference. One\n",
    "        image for the final prediction or a list of augmentations for the\n",
    "        tuning steps.\n",
    "        - top (int): the top percentage of samples to select\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): the logits of the inference\n",
    "        \"\"\"\n",
    "\n",
    "        if top == -1:\n",
    "            top = self.confidence\n",
    "\n",
    "        self.eval()\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x).to(self.device)\n",
    "            logits = self.inference(x)\n",
    "            if self.selected_idx is None:\n",
    "                logits, self.selected_idx = self.select_confident_samples(logits, top)\n",
    "            else:\n",
    "                logits = logits[self.selected_idx]\n",
    "        else:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = x.to(self.device)\n",
    "\n",
    "            logits = self.inference(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Basically CLIP's forward method, but with the custom\n",
    "        encoder to use our embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_feat = self.image_encoder(x)\n",
    "            image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        emb_prompts = self.prompt_learner()\n",
    "\n",
    "        txt_features = self.custom_encoder(emb_prompts, self.prompt_learner.tkn_prompts)\n",
    "        txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.clip.logit_scale.exp()\n",
    "        logits = logit_scale * image_feat @ txt_features.t()\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def predict(self, images, niter=1):\n",
    "\n",
    "        if self.ensemble:\n",
    "            with torch.no_grad():\n",
    "                out = self(images)\n",
    "                marginal_prob = F.softmax(out, dim=1).mean(0)\n",
    "                out_id = marginal_prob.argmax().item()\n",
    "        else:\n",
    "            if self.align_steps > 0:\n",
    "                self.align_embeddings(images)\n",
    "\n",
    "            for _ in range(niter):\n",
    "                out = self(images)\n",
    "                loss = self.avg_entropy(out)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                out = self(images[0])\n",
    "                out_id = out.argmax(1).item()\n",
    "                prediction = self.prompt_learner.classnames[out_id]\n",
    "\n",
    "        # return out_id, prediction\n",
    "        return out_id\n",
    "\n",
    "    def align_emb_loss(self, image_feat):\n",
    "\n",
    "        norm_feat = torch.nn.functional.normalize(image_feat, p=2, dim=1)\n",
    "\n",
    "        cos_sim = torch.mm(norm_feat, norm_feat.T)\n",
    "\n",
    "        # noself_mean = (cos_sim.sum() - torch.trace(cos_sim)) / (\n",
    "        #     cos_sim.numel() - cos_sim.shape[0]\n",
    "        # )\n",
    "        loss = 1 - cos_sim.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def align_embeddings(self, x):\n",
    "        \"\"\"\n",
    "        Aligns the embeddings of the image encoder\n",
    "        \"\"\"\n",
    "\n",
    "        self.forward(x)\n",
    "        self.clip.visual.train()\n",
    "        x = torch.stack(x).to(self.device)\n",
    "        selected_augs = torch.index_select(x, 0, self.selected_idx)\n",
    "        for _ in range(self.align_steps):\n",
    "            image_feat = self.clip.visual(selected_augs.type(self.dtype))\n",
    "            loss = self.align_emb_loss(image_feat)\n",
    "            self.emb_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # print(\"distance before: \", loss.item())\n",
    "            self.emb_optimizer.step()\n",
    "        image_feat = self.clip.visual(selected_augs.type(self.dtype))\n",
    "        loss = self.align_emb_loss(image_feat)\n",
    "        # print(\"distance after: \", loss.item())\n",
    "        self.clip.visual.eval()\n",
    "\n",
    "    def custom_encoder(self, prompts, tokenized_prompts):\n",
    "        \"\"\"\n",
    "        Custom clip text encoder, unlike the original clip encoder this one\n",
    "        takes the prompts embeddings from the prompt learner\n",
    "        \"\"\"\n",
    "        x = prompts + self.clip.positional_embedding\n",
    "        x = x.permute(1, 0, 2).type(self.dtype)  # NLD -> LND\n",
    "        x = self.clip.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip.ln_final(x).type(self.dtype)\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "            @ self.clip.text_projection\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the optimizer and the prompt learner to their initial state,\n",
    "        this has to be run before each new test\n",
    "        \"\"\"\n",
    "        self.optimizer.load_state_dict(deepcopy(self.optim_state))\n",
    "        self.prompt_learner.reset()\n",
    "        self.selected_idx = None\n",
    "\n",
    "        if self.align_steps > 0:\n",
    "            # print(\"[EasyTPT] Resetting embeddings optimizer\")\n",
    "            self.emb_optimizer.load_state_dict(deepcopy(self.emb_optim_state))\n",
    "            self.clip.visual.load_state_dict(deepcopy(self.clip_init_state))\n",
    "\n",
    "    def select_closest_samples(self, x, top):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = self.clip.visual(x.type(self.dtype))\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Compute cosine similarities\n",
    "            sims = F.cosine_similarity(feat[0].unsqueeze(0), feat[1:], dim=1)\n",
    "            vals, idxs = torch.topk(sims, int(sims.shape[0] * top))\n",
    "\n",
    "        return idxs\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Returns the optimizer\n",
    "\n",
    "        Returns:\n",
    "        - torch.optim: the optimizer\n",
    "        \"\"\"\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modified_bn_forward(self, input):\n",
    "    est_mean = torch.zeros(self.running_mean.shape, device=self.running_mean.device)\n",
    "    est_var = torch.ones(self.running_var.shape, device=self.running_var.device)\n",
    "    nn.functional.batch_norm(input, est_mean, est_var, None, None, True, 1.0, self.eps)\n",
    "    running_mean = self.prior * self.running_mean + (1 - self.prior) * est_mean\n",
    "    running_var = self.prior * self.running_var + (1 - self.prior) * est_var\n",
    "    return nn.functional.batch_norm(input, running_mean, running_var, self.weight, self.bias, False, 0, self.eps)\n",
    "\n",
    "\n",
    "class EasyMemo(EasyModel):\n",
    "    \"\"\"\n",
    "    A class to wrap a neural network with the MEMO TTA method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, device, classes_mask, prior_strength: float = 1.0, lr=0.005, weight_decay=0.0001, opt='sgd',\n",
    "                 niter=1, top=0.1, ensemble=False):\n",
    "        \"\"\"\n",
    "        Initializes the EasyMemo model with various arguments\n",
    "        Args:\n",
    "            net: The model to wrap with EasyMemo\n",
    "            device: The device to run the model on(usually 'CPU' or 'CUDA')\n",
    "            classes_mask: The classes to consider for the model(used for Imagenet-A)\n",
    "            prior_strength: The strength of the prior to use in the modified BN forward pass\n",
    "            lr: The Learning rate for the optimizer of the model\n",
    "            weight_decay: The weight decay for the optimizer of the model\n",
    "            opt: Which optimizer to use for this model between 'sgd' and 'adamw' for the respective optimizers\n",
    "            niter: The number of iterations to run the memo pass for\n",
    "            top: The percentage of the top logits to consider for confidence selection\n",
    "            ensemble: Whether to use the ensemble method or not\n",
    "        \"\"\"\n",
    "        super(EasyMemo, self).__init__()\n",
    "\n",
    "        self.ens = ensemble\n",
    "        self.device = device\n",
    "        self.prior_strength = prior_strength\n",
    "        self.net = net.to(device)\n",
    "        self.optimizer = self.get_optimizer(lr=lr, weight_decay=weight_decay, opt=opt)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.opt = opt\n",
    "        self.confidence_idx = None\n",
    "        if not ensemble:\n",
    "            self.memo_modify_bn_pass()\n",
    "        self.criterion = self.avg_entropy\n",
    "        self.niter = niter\n",
    "        self.top = top\n",
    "        self.initial_state = deepcopy(self.net.state_dict())\n",
    "        self.classes_mask = classes_mask\n",
    "\n",
    "    def forward(self, x, top=-1):\n",
    "        \"\"\"\n",
    "        Forward pass where we check which type of input we have and we call the inference on the input image Tensor\n",
    "        Args:\n",
    "            top: How many samples to select from the batch\n",
    "            x: A Tensor of shape (N, C, H, W) or a list of Tensors of shape (N, C, H, W)\n",
    "\n",
    "        Returns: The logits after the inference pass\n",
    "\n",
    "        \"\"\"\n",
    "        self.top = top if top > 0 else self.top\n",
    "        # print(f\"Shape forward: {x.shape}\")\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x).to(self.device)\n",
    "            # print(f\"Shape forward: {x.shape}\")\n",
    "            logits = self.inference(x)\n",
    "            logits, self.confidence_idx = self.select_confident_samples(logits, self.top)\n",
    "        else:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = x.to(self.device)\n",
    "            logits = self.inference(x)\n",
    "\n",
    "        # print(f\"[EasyMemo] input shape: {x.shape}\")\n",
    "        # print(f\"[EasyMemo] logits shape: {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Return the logits of the image in input x\n",
    "        Args:\n",
    "            x: A Tensor of shape (N, C, H, W) of an Image\n",
    "\n",
    "        Returns: The logits for that Tensor image\n",
    "\n",
    "        \"\"\"\n",
    "        if self.ens:\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "        outputs = self.net(x)\n",
    "\n",
    "        out_app = torch.zeros(outputs.shape[0], len(self.classes_mask)).to(self.device)\n",
    "        for i, out in enumerate(outputs):\n",
    "            out_app[i] = out[self.classes_mask]\n",
    "        return out_app\n",
    "\n",
    "    def predict(self, x, niter=1):\n",
    "        \"\"\"\n",
    "        Predicts the class of the input x, which is an image\n",
    "        Args:\n",
    "            niter: The number of iteration on which to run the memo pass\n",
    "            x: Tensor of shape (N, C, H, W)\n",
    "\n",
    "        Returns: The predicted classes\n",
    "\n",
    "        \"\"\"\n",
    "        self.niter = niter\n",
    "        if self.ens:\n",
    "            self.net.train()\n",
    "            predicted = self.ensemble(x)\n",
    "        else:\n",
    "            self.net.eval()\n",
    "            for iteration in range(self.niter):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(x)\n",
    "                loss = self.criterion(outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.net(x[0].unsqueeze(0).to(self.device))\n",
    "                outs = torch.zeros(outputs.shape[0], len(self.classes_mask)).to(self.device)\n",
    "                for i, out in enumerate(outputs):\n",
    "                    outs[i] = out[self.classes_mask]\n",
    "                predicted = outs.argmax(1).item()\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the model to its initial state\"\"\"\n",
    "        del self.optimizer\n",
    "        self.optimizer = self.get_optimizer(lr=self.lr, weight_decay=self.weight_decay, opt=self.opt)\n",
    "        self.confidence_idx = None\n",
    "        self.net.load_state_dict(deepcopy(self.initial_state))\n",
    "\n",
    "    def memo_modify_bn_pass(self):\n",
    "        print('modifying BN forward pass')\n",
    "        nn.BatchNorm2d.prior = self.prior_strength\n",
    "        nn.BatchNorm2d.forward = _modified_bn_forward\n",
    "\n",
    "    def get_optimizer(self, lr=0.005, weight_decay=0.0001, opt='sgd'):\n",
    "        \"\"\"\n",
    "        Initializes the optimizer for the memo model\n",
    "        Args:\n",
    "            lr: The learning rate for the optimizer\n",
    "            weight_decay: The weight decay for the optimizer\n",
    "            opt: Which optimizer to use\n",
    "\n",
    "        Returns: The optimizer for the memo model\n",
    "\n",
    "        \"\"\"\n",
    "        if opt == 'sgd':\n",
    "            optimizer = optim.SGD(self.net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif opt == 'adamw':\n",
    "            optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer selected')\n",
    "        return optimizer\n",
    "\n",
    "    def ensemble(self, x):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            outputs = nn.functional.softmax(outputs, dim=1)\n",
    "            prediction = outputs.sum(0).argmax().item()\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble class. Implements an ensemble of models with entropy minimization.\n",
    "\n",
    "    Attributes:\n",
    "        models (list[EasyModel]): A list of models to be used in the ensemble.\n",
    "        temps (list): A list of temperature values corresponding to each model.\n",
    "        test_single_models (bool): Whether to test each individual model in addition to the ensemble.\n",
    "        simple_ensemble (bool): Whether to perform the entropy minimization step.\n",
    "        device (str): The device to be used for computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, models:list[EasyModel], temps, device=\"cuda\", test_single_models=False, simple_ensemble=False):\n",
    "        \"\"\"\n",
    "        Initializes an Ensemble object.\n",
    "\n",
    "        Args:\n",
    "            models (list[EasyModel]): A list of models to be used in the ensemble.\n",
    "            temps (list): A list of temperature values corresponding to each model.\n",
    "            device (str, optional): The device to be used for computation. Defaults to \"cuda\".\n",
    "            test_single_models (bool, optional): Whether to test each individual model in addition to the ensemble. Defaults to False.\n",
    "            simple_ensemble (bool, optional): Whether to perform the entropy minimization step. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.temps = temps\n",
    "        self.test_single_models = test_single_models\n",
    "        self.device = device\n",
    "        self.simple_ensemble = simple_ensemble\n",
    "\n",
    "    def entropy(self, logits):\n",
    "        \"\"\"\n",
    "        Computes the entropy of a set of logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits to compute the entropy of.\n",
    "        \"\"\"\n",
    "        return -(torch.exp(logits) * logits).sum(dim=-1)\n",
    "\n",
    "    def marginal_distribution(self, models_logits):\n",
    "        \"\"\"\n",
    "        Computes the marginal distribution of the ensemble.\n",
    "\n",
    "        Args:\n",
    "            models_logits (torch.Tensor): The logits of the models in the ensemble.\n",
    "        \"\"\"\n",
    "        # average logits for each model\n",
    "        avg_models_logits = torch.Tensor(models_logits.shape[0], models_logits.shape[2]).to(self.device)\n",
    "        for i, model_logits in enumerate(models_logits):\n",
    "            avg_outs = torch.logsumexp(model_logits, dim=0) - torch.log(torch.tensor(model_logits.shape[0]))\n",
    "            min_real = torch.finfo(avg_outs.dtype).min\n",
    "            avg_outs = torch.clamp(avg_outs, min=min_real)\n",
    "            avg_outs /= self.temps[i]\n",
    "            avg_models_logits[i] = torch.log_softmax(avg_outs, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            entropies = torch.stack([self.entropy(logits) for logits in avg_models_logits]).to(self.device)\n",
    "            sum_entropies = torch.sum(entropies, dim=0)\n",
    "            scale = torch.stack([sum_entropies/entopy for entopy in entropies]).to(self.device)\n",
    "            #normalize sum to 1\n",
    "            scale = scale / torch.sum(scale)\n",
    "\n",
    "        # print(\"\\t\\t[Ensemble] Entropies: \", entropies)\n",
    "        # print(\"\\t\\t[Ensemble] Scales: \", scale)\n",
    "\n",
    "        avg_logits = torch.sum(torch.stack([scale[i].item() * avg_models_logits[i] for i in range(len(avg_models_logits))]), dim=0)\n",
    "\n",
    "        return avg_logits\n",
    "\n",
    "    def get_models_outs(self, inputs, top=0.1):\n",
    "        \"\"\"\n",
    "        Computes the outputs of the models in the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        model_outs = torch.stack([model(inputs[i], top).to(self.device) for i, model in enumerate(self.models)]).to(self.device)\n",
    "        return model_outs.to(self.device)\n",
    "\n",
    "    def get_models_predictions(self, inputs):\n",
    "        \"\"\"\n",
    "        Computes the predictions of the single models in the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "        \"\"\"\n",
    "        models_pred = [model.predict(inputs[i]) for i, model in enumerate(self.models)]\n",
    "        return models_pred\n",
    "\n",
    "    def entropy_minimization(self, inputs, niter=1, top=0.1):\n",
    "        \"\"\"\n",
    "        Test time adaptation step. Minimizes the entropy of the ensemble's predictions.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            niter (int, optional): The number of iterations to perform. Defaults to 1.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        for i in range(niter):\n",
    "            outs = self.get_models_outs(inputs, top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "\n",
    "            loss = self.entropy(avg_logit)\n",
    "            loss.backward()\n",
    "            for model in self.models:\n",
    "                model.optimizer.step()\n",
    "                model.optimizer.zero_grad()\n",
    "\n",
    "    def forward(self, inputs, niter=1, top=0.1):\n",
    "        \"\"\"\n",
    "        Forward pass of the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            niter (int, optional): The number of iterations to perform. Defaults to 1.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        # get models outputs\n",
    "        self.reset()\n",
    "        models_pred = []\n",
    "        if self.test_single_models: \n",
    "            models_pred = self.get_models_predictions(inputs)\n",
    "\n",
    "        self.reset()\n",
    "        self.entropy_minimization(inputs, niter, top)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outs = self.get_models_outs([i[0] for i in inputs], top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "            prediction = torch.argmax(avg_logit, dim=0)\n",
    "\n",
    "        if self.simple_ensemble:\n",
    "            self.reset()\n",
    "            outs = self.get_models_outs(inputs, top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "            prediction_no_back = torch.argmax(avg_logit, dim=0)\n",
    "\n",
    "        return models_pred, prediction_no_back, prediction\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the models in the ensemble.\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEMO adapts the entire network at test time to improve robustness. By incorporating strategies such as dropout and ensemble methods, MEMO seeks to enhance the model's ability to generalize and produce reliable predictions under diverse conditions.\n",
    "\n",
    "Our addition:\n",
    "If an image should be classified as something, then most of the features extracted should point toward it, but there might be some noise from other features that influence the result or may even strongly polarize the classification.\n",
    "With dropout we hope to stochastically remove these features, while keeping the correct ones that (hopefully) are the majority.\n",
    "\n",
    "This approach can also be implemented to be extremely efficient since we only need to pass the last classification head multiple times, while the image has to go through the network only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MEMO_testing_step(model, dataset, mapping: bool | list, test):\n",
    "    print(f\"Starting {test} evaluation...\")\n",
    "    start = time.time()\n",
    "\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    index = np.random.permutation(range(len(dataset)))\n",
    "    iterate = tqdm(index)\n",
    "    for i in iterate:\n",
    "        data = dataset[i]\n",
    "        image = data[\"img\"]\n",
    "        label = int(data[\"label\"])\n",
    "        prediction = model.predict(image)\n",
    "        model.reset()\n",
    "        correct += mapping[prediction] == label\n",
    "        cnt += 1\n",
    "        iterate.set_description(desc=f\"Current accuracy {(correct / cnt) * 100:.2f}\")\n",
    "    print(f\"Accuracy: {(correct / cnt) * 100:.2f}, Time: {timedelta(seconds=time.time() - start)}\")\n",
    "    print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageNet_A, imageNet_V2 = memo_get_datasets('augmix', 1)\n",
    "# mapping_a = [int(x) for x in imageNet_A.classnames.keys()]\n",
    "# mapping_v2 = [int(x) for x in imageNet_V2.classnames.keys()]\n",
    "# net = torch_models.resnet50(weights=torch_models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# # Baseline tests\n",
    "# if baseline_tests:\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=1, top=1, ensemble=True)\n",
    "#     test_name = \"Baseline ImageNetA\"\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"Baseline ImageNetV2\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=1, top=1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "\n",
    "#     # test_name = \"ResNet50 V1 weights ImageNetA\"\n",
    "#     # del memo\n",
    "#     # net = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "#     # memo = EasyMemo(net.to(device), device, mapping_a, prior_strength=1, top=1, ensemble=True)\n",
    "#     # testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     # test_name = \"ResNet50 V1 weights ImageNetV2\"\n",
    "#     # del memo\n",
    "#     # memo = EasyMemo(net.to(device), device, mapping_v2, prior_strength=1, top=1, ensemble=True)\n",
    "#     # testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "\n",
    "#     del memo\n",
    "# del net, imageNet_A, imageNet_V2\n",
    "# net = torch_models.resnet50(weights=torch_models.ResNet50_Weights.DEFAULT)\n",
    "# imageNet_A, imageNet_V2 = memo_get_datasets('cut', augs_no_selection)\n",
    "# if memo_tests:\n",
    "#     test_name = \"MEMO ImageNetA, without topk selection\"\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=0.94, top=1)\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"MEMO ImageNetV2, without topk selection\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=0.94, top=1)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "\n",
    "#     test_name = \"MEMO ImageNetA, with topk selection\"\n",
    "#     del memo, imageNet_V2, imageNet_A\n",
    "#     imageNet_A, imageNet_V2 = memo_get_datasets('cut', augs_selection)\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=0.94, top=0.1)\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"MEMO ImageNetV2, with topk selection\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=0.94, top=0.1)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "#     del memo\n",
    "\n",
    "# # Dropout tests\n",
    "# if drop_tests:\n",
    "#     test_name = \"DROP ImageNetA, without topk selection\"\n",
    "#     net.layer4.add_module('dropout', nn.Dropout(0.5, inplace=True))\n",
    "#     del memo, imageNet_V2, imageNet_A\n",
    "#     imageNet_A, imageNet_V2 = memo_get_datasets('identity', augs_no_selection)\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=1, top=1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"DROP ImageNetV2, without topk selection\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=1, top=1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "\n",
    "#     test_name = \"DROP ImageNetA, with topk selection\"\n",
    "#     del memo, imageNet_V2, imageNet_A\n",
    "#     imageNet_A, imageNet_V2 = memo_get_datasets('identity', augs_selection)\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=1, top=0.1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"DROP ImageNetV2, with topk selection\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=1, top=0.1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "#     del memo\n",
    "\n",
    "# if ensemble_tests:\n",
    "#     test_name = \"DROP ImageNetA, cut ensemble without topk selection\"\n",
    "#     del memo, imageNet_V2, imageNet_A, net\n",
    "#     net = torch_models.resnet50(weights=torch_models.ResNet50_Weights.DEFAULT)\n",
    "#     net.layer4.add_module('dropout', nn.Dropout(0, inplace=True))\n",
    "#     imageNet_A, imageNet_V2 = memo_get_datasets('cut', augs_no_selection)\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=1, top=1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"DROP ImageNetV2, cut ensemble  without topk selection\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=1, top=1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "\n",
    "#     test_name = \"DROP ImageNetA, cut ensemble  with topk selection\"\n",
    "#     del memo, imageNet_V2, imageNet_A\n",
    "#     imageNet_A, imageNet_V2 = memo_get_datasets('cut', augs_selection)\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_a, prior_strength=1, top=0.1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_A, mapping_a, test_name)\n",
    "\n",
    "#     test_name = \"DROP ImageNetV2, cut ensemble  with topk selection\"\n",
    "#     del memo\n",
    "#     memo = EasyMemo(net.to(memo_device), memo_device, mapping_v2, prior_strength=1, top=0.1, ensemble=True)\n",
    "#     MEMO_testing_step(memo, imageNet_V2, mapping_v2, test_name)\n",
    "#     del memo\n",
    "\n",
    "# del net, imageNet_A, imageNet_V2\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'exist_ok' is an invalid keyword argument for mkdir()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m results_path_tpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESULTS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/TPT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_path_tpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, settings \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tests):\n\u001b[1;32m      7\u001b[0m     test \u001b[38;5;241m=\u001b[39m base_test \u001b[38;5;241m|\u001b[39m settings\n",
      "\u001b[0;31mTypeError\u001b[0m: 'exist_ok' is an invalid keyword argument for mkdir()"
     ]
    }
   ],
   "source": [
    "\n",
    "results_path_tpt = f\"{RESULTS_PATH}/TPT\"\n",
    "os.mkdir(results_path_tpt)\n",
    "\n",
    "\n",
    "for idx, settings in enumerate(tests):\n",
    "\n",
    "    test = base_test | settings\n",
    "\n",
    "    dataset_name = test[\"dataset\"]\n",
    "    test_name = test[\"name\"]\n",
    "    device = test[\"device\"]\n",
    "\n",
    "    BASE_PROMPT = test[\"base_prompt\"]\n",
    "    ARCH = test[\"arch\"]\n",
    "    SPLT_CTX = test[\"splt_ctx\"]\n",
    "    LR = test[\"lr\"]\n",
    "    AUGS = test[\"augs\"]\n",
    "    TTT_STEPS = test[\"ttt_steps\"]\n",
    "    ALIGN_STEPS = test[\"align_steps\"]\n",
    "    ENSEMBLE = test[\"ensemble\"]\n",
    "    TEST_STOP = test[\"test_stop\"]\n",
    "    CONFIDENCE = test[\"confidence\"]\n",
    "\n",
    "    f = open(f\"{results_path_tpt}/{test_name}.txt\", \"w\")\n",
    "    sys.stdout = f\n",
    "\n",
    "    # if WANDB_SECRET != \"\":\n",
    "    #     timestamp = time.strftime(\"%m%d%H%M%S\")\n",
    "    #     run_name = f\"{test_name}_{timestamp}\"\n",
    "    #     wandb.init(\n",
    "    #         project=\"MEMOTPT\",\n",
    "    #         name=run_name,\n",
    "    #         config={\n",
    "    #             \"test_name\": test_name,\n",
    "    #             \"dataset_name\": dataset_name,\n",
    "    #             \"base_prompt\": BASE_PROMPT,\n",
    "    #             \"arch\": ARCH,\n",
    "    #             \"splt_ctx\": str(SPLT_CTX),\n",
    "    #             \"lr\": LR,\n",
    "    #             \"augs\": AUGS,\n",
    "    #             \"ttt_steps\": TTT_STEPS,\n",
    "    #             \"align_steps\": ALIGN_STEPS,\n",
    "    #             \"ensemble\": ENSEMBLE,\n",
    "    #             \"test_stop\": TEST_STOP,\n",
    "    #             \"confidence\": CONFIDENCE,\n",
    "    #         },\n",
    "    #     )\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"[TEST] Running test {idx + 1} of {len(tests)}: {test_name} \\n{test}\")\n",
    "\n",
    "    print(f\"[TEST] loading datasets with {AUGS} augmentation...\")\n",
    "    datasetRoot = DATASET_ROOT\n",
    "    (\n",
    "        imageNetA,\n",
    "        _,\n",
    "        imageNetACustomNames,\n",
    "        imageNetAMap,\n",
    "        imageNetV2,\n",
    "        _,\n",
    "        imageNetV2CustomNames,\n",
    "        imageNetV2Map,\n",
    "    ) = tpt_get_datasets(datasetRoot, augs=AUGS, all_classes=False)\n",
    "    print(\"[TEST] datasets loaded.\")\n",
    "\n",
    "    if dataset_name == \"A\":\n",
    "        print(\"[TEST] using ImageNet A\")\n",
    "        dataset = imageNetA\n",
    "        classnames = imageNetACustomNames\n",
    "        id_mapping = imageNetAMap\n",
    "        del imageNetV2, imageNetV2CustomNames, imageNetV2Map\n",
    "    elif dataset_name == \"V2\":\n",
    "        print(\"[TEST] using ImageNet V2\")\n",
    "        dataset = imageNetV2\n",
    "        classnames = imageNetV2CustomNames\n",
    "        id_mapping = imageNetV2Map\n",
    "        del imageNetA, imageNetACustomNames, imageNetAMap\n",
    "\n",
    "    tpt = EasyTPT(\n",
    "        device,\n",
    "        base_prompt=BASE_PROMPT,\n",
    "        arch=ARCH,\n",
    "        splt_ctx=SPLT_CTX,\n",
    "        classnames=classnames,\n",
    "        ttt_steps=TTT_STEPS,\n",
    "        lr=LR,\n",
    "        align_steps=ALIGN_STEPS,\n",
    "        ensemble=ENSEMBLE,\n",
    "        confidence=CONFIDENCE,\n",
    "    )\n",
    "\n",
    "    cnt = 0\n",
    "    tpt_correct = 0\n",
    "    total_time = 0\n",
    "\n",
    "    idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "    SEED = 1\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for idx in idxs:\n",
    "        data = dataset[idx]\n",
    "        label = data[\"label\"]\n",
    "        imgs = data[\"img\"]\n",
    "        name = data[\"name\"]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        cnt += 1\n",
    "        with torch.no_grad():\n",
    "            tpt.reset()\n",
    "\n",
    "        out_id = tpt.predict(imgs)\n",
    "        tpt_predicted = classnames[out_id]\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        total_time += end - start\n",
    "        avg_time = total_time / cnt\n",
    "\n",
    "        if int(id_mapping[out_id]) == label:\n",
    "            emoji = \":D\"\n",
    "            tpt_correct += 1\n",
    "        else:\n",
    "            emoji = \":(\"\n",
    "\n",
    "        tpt_acc = tpt_correct / (cnt)\n",
    "\n",
    "        # if WANDB_SECRET != \"\":\n",
    "        #     wandb.log({\"tpt_acc\": tpt_acc})\n",
    "\n",
    "        if cnt % VERBOSE == 0:\n",
    "            print(emoji)\n",
    "            print(f\"TPT Accuracy: {round(tpt_acc, 3)}\")\n",
    "            print(f\"GT: \\t{name}\\nTPT: \\t{tpt_predicted}\")\n",
    "            print(\n",
    "                f\"after {cnt} samples, average time {round(avg_time, 3)}s ({round(1 / avg_time, 3)}it/s)\\n\"\n",
    "            )\n",
    "\n",
    "        if cnt == TEST_STOP:\n",
    "            print(f\"[TEST] Early stopping at {cnt} samples\")\n",
    "            break\n",
    "\n",
    "    del tpt\n",
    "\n",
    "    print(f\"[TEST] Final TPT Accuracy: {round(tpt_acc, 3)} over {cnt} samples\")\n",
    "\n",
    "    # if WANDB_SECRET != \"\":\n",
    "    #     wandb.finish()\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPT(device=\"cuda\", naug=64, arch=\"RN50\", A=True, ttt_steps=1, align_steps=0, top=0.1):\n",
    "    \"\"\"\n",
    "    Return the TPT model initialized with the given parameters\n",
    "\n",
    "    Args:\n",
    "        - device: device to use - default: cuda\n",
    "        - naug: number of augmentations to use - default: 64\n",
    "        - arch: backbone model to use - default: RN50\n",
    "        - A: use ImageNet A or ImageNet V2 - default: True\n",
    "        - ttt_steps: number of iterations for the TTT - default: 1\n",
    "        - align_steps: number of iterations for the alignment of the image embeddings - default: 0\n",
    "        - top: top confidence to select the augmented samples - default: 0.1\n",
    "    \"\"\"\n",
    "    # prepare TPT\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Using CPU this is no bueno\")\n",
    "    else:\n",
    "        print(\"Using GPU, brace yourself!\")\n",
    "\n",
    "    datasetRoot = \"datasets\"\n",
    "    imageNetA, _, imageNetACustomNames, imageNetAMap, imageNetV2, _, imageNetV2CustomNames, imageNetV2Map = tpt_get_datasets(datasetRoot, augs=naug, all_classes=False)\n",
    "    \n",
    "    if A:\n",
    "        dataset = imageNetA\n",
    "        classnames = imageNetACustomNames\n",
    "        mapping = imageNetAMap\n",
    "    else:\n",
    "        dataset = imageNetV2\n",
    "        classnames = imageNetV2CustomNames\n",
    "        mapping = imageNetV2Map\n",
    "    \n",
    "    tpt = EasyTPT(\n",
    "        base_prompt=\"A bad photo of a [CLS]\",\n",
    "        arch=arch,\n",
    "        classnames=classnames,\n",
    "        device=device,\n",
    "        ttt_steps=ttt_steps,\n",
    "        align_steps=align_steps,\n",
    "        confidence=top\n",
    "    )\n",
    "    \n",
    "    return tpt, dataset, mapping\n",
    "\n",
    "\n",
    "def memo(device=\"cuda\", prior_strength=0.94, naug=64, A=True, drop=0, ttt_steps=1, model=\"RN50\", top=0.1):\n",
    "    \"\"\"\n",
    "    Return the MEMO model initialized with the given parameters\n",
    "\n",
    "    Args:\n",
    "        - device: device to use - default: cuda\n",
    "        - prior_strength: strength of the prior for the BN layers - default: 0.94\n",
    "        - naug: number of augmentations to use - default: 64\n",
    "        - A: use ImageNet A or ImageNet V2 - default: True\n",
    "        - drop: dropout to use, by setting it to >0 the model will use the ensemble strategy - default: 0\n",
    "        - ttt_steps: number of iterations for the TTT - default: 1\n",
    "        - model: backbone model to use - default: RN50\n",
    "        - top: top confidence to select the augmented samples - default: 0.1\n",
    "    \"\"\"\n",
    "    load_model = {\n",
    "        \"RN50\": torch_models.resnet50,\n",
    "        \"RNXT\": torch_models.resnext50_32x4d\n",
    "    }\n",
    "    models_weights = {\n",
    "        \"RN50\": torch_models.ResNet50_Weights.DEFAULT,\n",
    "        \"RNXT\": torch_models.ResNeXt50_32X4D_Weights.DEFAULT\n",
    "    }\n",
    "    # prepare MEMO\n",
    "    imageNet_A, imageNet_V2 = memo_get_datasets(augmentation=('cut' if drop==0 else 'identity'), augs=naug)\n",
    "    dataset = imageNet_A if A else imageNet_V2\n",
    "\n",
    "    mapping = list(dataset.classnames.keys())\n",
    "    for i,id in enumerate(mapping):\n",
    "        mapping[i] = int(id)\n",
    "    \n",
    "    model = load_model[model](weights=models_weights[model])\n",
    "    model.layer4.add_module('dropout', nn.Dropout(drop))\n",
    "\n",
    "    memo = EasyMemo(\n",
    "        model, \n",
    "        device=device, \n",
    "        classes_mask=mapping, \n",
    "        prior_strength=prior_strength,\n",
    "        niter=ttt_steps,\n",
    "        ensemble=(drop>0),\n",
    "        top=top\n",
    "    )\n",
    "    \n",
    "    return memo, dataset, mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, datasets, temps, mapping, names,\n",
    "         device=\"cuda\", niter=1, top=0.1,\n",
    "         simple_ensemble=False, testSingleModels=False):\n",
    "    \"\"\"\n",
    "    Test the ensemble model on the datasets\n",
    "\n",
    "    Args:\n",
    "        - models: list of models\n",
    "        - datasets: list of datasets\n",
    "        - temps: list of temperatures for the models to rescale the logits\n",
    "        - names: names of the models\n",
    "        - mapping: mapping of the classes outputted by the models to the original 1000 classes\n",
    "        - device: device to use\n",
    "        - niter: number of iterations for the TTT\n",
    "        - top: top confidence to select the augmented samples\n",
    "        - simple_ensemble: use the simple ensemble, only marginalizing the ditributions\n",
    "        - testSingleModels: test the single models inside the ensamble\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    correct_no_back = 0\n",
    "    correctSingle = [0]*len(models)\n",
    "    cnt = 0\n",
    "\n",
    "    class_names = get_classes_names()\n",
    "\n",
    "    #shuffle the data\n",
    "    indx = np.random.permutation(range(len(datasets[0])))\n",
    "\n",
    "    model = Ensemble(models, temps=temps, \n",
    "                     device=device, test_single_models=testSingleModels, \n",
    "                     simple_ensemble=simple_ensemble)\n",
    "    print(\"Ensemble model created starting TTA, samples:\",len(indx))\n",
    "    for i in indx:\n",
    "        cnt += 1 \n",
    "        data = [datasets[j][i][\"img\"] for j in range(len(datasets))]\n",
    "        \n",
    "        labels = [datasets[j][i][\"label\"] for j in range(len(datasets))]\n",
    "        #check if the labels are the same\n",
    "        assert all(x == labels[0] for x in labels), \"Labels are not the same\"\n",
    "        label = labels[0]\n",
    "        name = datasets[0][i][\"name\"]\n",
    "\n",
    "        print (f\"Testing on {i} - name: {name} - label: {label}\")\n",
    "\n",
    "        models_out, pred_no_back, prediction = model(data, niter=niter, top=top)\n",
    "        models_out = [int(mapping[model_out]) for model_out in models_out]\n",
    "        prediction = int(mapping[prediction])\n",
    "        \n",
    "        if testSingleModels:\n",
    "            for i, model_out in enumerate(models_out):\n",
    "                if label == model_out:\n",
    "                    correctSingle[i] += 1\n",
    "                \n",
    "                print(f\"\\t{names[i]} model accuracy: {correctSingle[i]}/{cnt} - predicted class {model_out}: {class_names[model_out]} - tested: {cnt} / {len(datasets[0])}\")\n",
    "\n",
    "        if simple_ensemble:\n",
    "            pred_no_back = int(mapping[pred_no_back])\n",
    "            if label == pred_no_back:\n",
    "                correct_no_back += 1\n",
    "            print(f\"\\tSimple Ens accuracy: {correct_no_back}/{cnt} - predicted class {pred_no_back}: {class_names[pred_no_back]} - tested: {cnt} / {len(datasets[0])}\")\n",
    "\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "            \n",
    "        print(f\"\\tEnsemble accuracy: {correct}/{cnt} - predicted class {prediction}: {class_names[prediction]} - tested: {cnt} / {len(datasets[0])}\")\n",
    "\n",
    "#expand args\n",
    "def runTest(models_type, args, temps, names, naug=64, niter=1, top=0.1, device=\"cuda\", simple_ensemble=False, testSingleModels=False, imageNetA=True):\n",
    "    \"\"\"\n",
    "    Run the test on Ensemble model with the given arguments\n",
    "\n",
    "    Args:\n",
    "        - models_type: list of the models type to use\n",
    "        - args: list of the arguments for the models\n",
    "        - temps: list of temperatures for the models to rescale the logits\n",
    "        - names: names of the models\n",
    "        - naug: number of augmentations to use\n",
    "        - niter: number of iterations for the TTT\n",
    "        - top: top confidence to select the augmented samples\n",
    "        - device: device to use\n",
    "        - simple_ensemble: use the simple ensemble, only marginalizing the ditributions\n",
    "        - testSingleModels: test the single models inside the ensamble\n",
    "        - imageNetA: use ImageNet A or ImageNet V2\n",
    "    \"\"\"\n",
    "        \n",
    "    models = []\n",
    "    datasets = []\n",
    "    mapping = None\n",
    "    load_model = {\n",
    "        \"memo\": memo,\n",
    "        \"tpt\": TPT\n",
    "    }\n",
    "    for i in range(len(models_type)):\n",
    "        model, data, mapping = load_model[models_type[i]](**args[i], A=imageNetA, top=top, naug=naug)\n",
    "        models.append(model)\n",
    "        datasets.append(data)\n",
    "\n",
    "    test(models=models, datasets=datasets, temps=temps, mapping=mapping, names=names, \n",
    "        device=device, niter=niter, top=top, simple_ensemble=simple_ensemble, testSingleModels=testSingleModels)\n",
    "        \n",
    "    for model in models:\n",
    "        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path_tta = f\"{RESULTS_PATH}/TTA\"\n",
    "os.mkdir(results_path_tta, exist_ok=True)\n",
    "\n",
    "for ENStest in ENSTests:\n",
    "\n",
    "        f = open(f\"{results_path_tta}/{ENStest}.txt\", \"w\")\n",
    "        sys.stdout = f\n",
    "\n",
    "        print(f\"Running test: {ENStest}\")\n",
    "        ENStest = ENSTests[ENStest]\n",
    "        runTest(**ENStest)\n",
    "        \n",
    "\n",
    "        print(\"\\n-------------------\\n\")\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
