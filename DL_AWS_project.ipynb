{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-time Adaptation for Image Classification\n",
    "\n",
    "**Authors**: Davide Cavicchini, Laurence Bonat, Lorenzo Orsingher  \n",
    "**Date**: 01/06/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "1. [Introduction](#Introduction): Overview of the problem, proposed solutions, and some implementation details on how results are stored\n",
    "2. [Imports](#Imports): Imports the necessary libraries and modules\n",
    "3. [Parameters](#Parameters): Defines the tests to be performed and the parameters to be used, each macro technique (MEMO, TPT, Enesemble) will have a dedicated section to explain the parameters used\n",
    "4. [Data Loading](#Data-Loading): Load the data and prepare it for training\n",
    "5. [Models](#models): Talks in detail about the strategies used to adapt the model at test time, about the motivations behind them and the implementation details.\n",
    "6. [Results](#test-time-adaptation): Shows the results of the tests performed\n",
    "7. [Conclusions](#Conclusions): Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "This report presents a study of various deep learning techniques for image classification, specifically focusing on \"Test-time Prompt Tuning\" (TPT) and \"Test Time Robustness via Adaptation and Augmentation\" (MEMO). We introduce several contributions, including MEMO with dropout, a simple ensemble with probability marginalization, and an ensemble of multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Techniques\n",
    "### Baselines\n",
    "As baselines we decided to use the following models:\n",
    "- RN50: ResNet50, a 50-layer deep convolutional neural network trained on the ImageNet dataset. Both with V1 and V2 versions.\n",
    "- CLIP RN50: A model that uses a vision encoder and a language encoder to learn visual concepts from natural language supervision. We use the RN50 version.\n",
    "\n",
    "### [MEMO](https://arxiv.org/abs/2110.09506)\n",
    "Propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model’s average, or marginal, output distribution across the augmentations.\n",
    "\n",
    "### [Test-time Prompt Tuning (TPT)](https://arxiv.org/abs/2209.07511)\n",
    "For image classification, TPT optimizes the prompt given to CLIP by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample.\n",
    "\n",
    "## Our Contributions\n",
    "\n",
    "### TPT with Alignment Steps\n",
    "Another explored idea is to align the embeddings of the augmented images before passing them to the classifier.\n",
    "\n",
    "Since the augmented images should be representing the same thing to us, we first apply a gradient update to the image classifier to make the embeddings closer to each other. As with TPT we apply confidence selection to take into account only the relevant augmentations that hopefully do not destroy the content of the image.\n",
    "\n",
    "### Simple Ensemble with Probability Marginalization\n",
    "Both MEMO and TPT are trying to minimize the entropy of the model's average output distribution across the augmentations. This, in turn, should maximize the probability of the correct class, affecting the model's output to the original image.\n",
    "\n",
    "So the idea here is also quite straightforward: since we are maximizing the probability of the class predicted by the ensemble, why not just return this? No need to backpropagate the network.\n",
    "\n",
    "### Ensemble with Dropout\n",
    "We propose to use dropout to stochastically remove features that are not relevant for the classification, using networks such as ResNet50.\n",
    "\n",
    "The idea is naive: if an image should be classified as something, then most of the features extracted should point toward it, but there might be some noise from other features that influence the result or may even strongly polarize the classification. With dropout we hope to stochastically remove these features, while keeping the correct ones that (hopefully) are the majority.\n",
    "\n",
    "This approach can also be implemented to be extremely efficient since we only need to pass the last classification head multiple times, while the image has to go through the network only once.\n",
    "\n",
    "### Ensemble of Multiple Models\n",
    "Finally, we wanted to see if multiple models trained with different techniques could improve the performance. The idea is to have different models covering the out of distribution data of one another, leading to a more robust ensemble.\n",
    "\n",
    "## Results Storage\n",
    "After the execution, each experiment is stored in a separate directory, with the following structure:\n",
    "```bash\n",
    "└── results\n",
    "    └── results_{timestamp}\n",
    "        ├── TPT\n",
    "        │   ├── {experiment_name}.txt    - dump of the console output during the experiment\n",
    "        │   ├── {experiment_name}.json   - dump of the experiment configuration and obtained results\n",
    "        │   └── ...\n",
    "        ├── MEMO\n",
    "        │   ├── {experiment_name}.txt    - dump of the console output during the experiment\n",
    "        │   ├── {experiment_name}.json   - dump of the experiment configuration and obtained results\n",
    "        │   └── ...\n",
    "        └── Ensemble\n",
    "            ├── {experiment_name}.txt    - dump of the console output during the experiment\n",
    "            ├── {experiment_name}.json   - dump of the experiment configuration and obtained results\n",
    "            └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!wget https://raw.githubusercontent.com/DavidC001/MEMO-TPT-DL2024/main/dataloaders/wordNetIDs2Classes.csv --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import csv\n",
    "import boto3\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as torch_models\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.v2 import AugMix\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from copy import deepcopy\n",
    "from clip import load, tokenize\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results\", exist_ok=True)\n",
    "RESULTS_PATH = f\"results/results_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "#set the seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MEMO\n",
    "In this section we define the useful parameters that are used to run the test on MEMO. \n",
    "We define the device we use with memo and which tests we want to run with the following variables: `memo_tests`, `drop_tests`, `ensemble_tests`, `baseline_tests`. \n",
    "The parameters `augs_no_selection` and `augs_selection` are used to choose how many augmentation we apply if we use the topk selection or not.\n",
    "\n",
    "There are then three variables that define specific paths for the tests: \n",
    "- `DATASET_ROOT` is the root of the datasets, in this notebook it is simply ignored, and we use the AWS bucket\n",
    "- `DATASET_TO_TEST` is the dataset to test, can be either 'a' for ImageNet-A or 'v2' for ImageNetV2, 'both' for both\n",
    "- `INITIAL_WEIGHTS` is the weights to use to initialize the resnet model. For this case we have 'default' for defaults ones and 'v1' for v1 weights\n",
    "\n",
    "The dictionary `memo_base_test` is the default configuration for the MEMO tests, it is used as a base for all the tests and then we override the parameters that need to be changed. An overview of the parameters is given below:\n",
    "    \n",
    "- **memo**: Contains the parameters for the EasyMEMO class\n",
    "  - **device**: Device to use for the test\n",
    "  - **prior_strength**: Prior strength for the Batch Normalization layer that we modified\n",
    "  - **lr**: Learning rate for the optimizer\n",
    "  - **weight_decay**: Weight decay for the optimizer\n",
    "  - **opt**: Optimizer to use for the neural network\n",
    "  - **niter**: Number of optimization steps. The default is 1, and it is what is used in every single test. \n",
    "  - **top**: Percentage of the top augmentations to consider (confidence selection)\n",
    "  - **ensemble**: Whether to run the model in augmentation ensemble mode\n",
    "\n",
    "- **dataset**: Contains the parameters for the dataset\n",
    "  - **imageNetA**: Whether to use ImageNet-A or not\n",
    "  - **naug**: Number of augmentations to perform\n",
    "  - **dataset_root**: Root of the datasets\n",
    "  - **aug_type**: Type of augmentation to use between `augmix`, `cut` and `identity`\n",
    "- **weights**: Weights to use to initialize the resnet model. For this case we have 'default' for defaults ones and 'v1' for v1 weights\n",
    "- **run**: Whether to run the test or not\n",
    "- **drop**: Dropout to use for the simple ensemble model. If we set it to 0 we have identity layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "memo_tests = True\n",
    "drop_tests = True\n",
    "ensemble_tests = True\n",
    "baseline_tests = True\n",
    "augs_no_selection = 8\n",
    "augs_selection = 64\n",
    "\n",
    "# The root of the datasets, in this notebook it is simply ignored, and we use the AWS bucket\n",
    "DATASET_ROOT = 'datasets'\n",
    "# Which dataset to test: 'a' for ImageNet-A, 'v2' for ImageNet-V2, 'both' for both\n",
    "DATASET_TO_TEST = 'both'\n",
    "# Which Weights I want to use to initialize the resnet model. For this case we have 'default' for defaults ones and 'v1' for v1 weights\n",
    "INITIAL_WEIGHTS = 'default'\n",
    "\n",
    "memo_base_test = {\n",
    "    \"memo\": {\n",
    "        \"device\": memo_device,\n",
    "        \"prior_strength\": 1.0,\n",
    "        \"lr\": 0.005,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"opt\": 'sgd',\n",
    "        \"niter\": 1,\n",
    "        \"top\": 1,\n",
    "        \"ensemble\": False,\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"imageNetA\": True,\n",
    "        \"naug\": 1,\n",
    "        \"dataset_root\": DATASET_ROOT,\n",
    "        \"aug_type\": \"augmix\",\n",
    "    },\n",
    "    \"weights\": INITIAL_WEIGHTS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TPT\n",
    "\n",
    "To make the test suite as self-contained as possible, the tests are defined as lists of dictionaries where each dictionary contains all the parameters for a single test, the parameters are the same as the ones accessible via command line in the stand-alone version of the code in the `main.py` file.\n",
    "\n",
    "***tpt_base_test*** includes the default settings for all the parameters, each new test will inherit from this dictionary and override the parameters that need to be changed, in this way only the parameters that are different from the default settings need to be specified.\n",
    "\n",
    "- **name**: Name of the test, used to identify the experiment in the results folder and to briefly describe the test\n",
    "- **dataset**: Dataset to use for the test, can be either 'A' for ImageNet-A or 'V2' for ImageNetV2\n",
    "- **augs**: Number of augmentations of the orignal image to use for the test, the default value is 64\n",
    "- **ttt_steps**: Number of test-time tuning steps to be performed on the prompt\n",
    "- **align_steps**: Number of alignment steps to be performed on image embedding before classification\n",
    "- **ensemble**: whether to run the model in augmentation ensemble mode. When this parameter is set to true the model will skip the prompt tuning\n",
    "- **test_stop**: Number of test samples to be used in this run, the default value -1 means that all the dataset will be used\n",
    "- **confidence**: Confidence selection threshold for the TPT model, the default value is 0.10 \n",
    "- **base_prompt**: Base prompt to be used for the TPT model, the default value is 'A photo of a [CLS]' where [CLS] is the placeholder for the class to be predicted. The class token can be placed anywhere in the sentence\n",
    "- **arch**: Architecture of the backbone for CLIP's visual encoder\n",
    "- **splt_ctx**: Whether to keep the context vector separate between prompt prefix and suffix or not\n",
    "- **lr**: Learning rate for the prompt tuning\n",
    "- **device**: Device to use for the test\n",
    "\n",
    "For consistency, the parameters we picked for our tests are as close as possible to the ones used in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = 5\n",
    "\n",
    "tpt_base_test = {\n",
    "    \"name\": \"Base\",\n",
    "    \"dataset\": \"A\",\n",
    "    \"augs\": 64,\n",
    "    \"ttt_steps\": 1,\n",
    "    \"align_steps\": 0,\n",
    "    \"ensemble\": False,\n",
    "    \"test_stop\": -1,\n",
    "    \"confidence\": 0.10,\n",
    "    \"base_prompt\": \"A photo of a [CLS]\",\n",
    "    \"arch\": \"RN50\",\n",
    "    \"splt_ctx\": False,\n",
    "    \"lr\": 0.005,\n",
    "    \"device\": \"cuda:0\",\n",
    "}\n",
    "\n",
    "# test_stop stops the testing after a certain number of samples,\n",
    "# to run the entire dataset keep it at -1.\n",
    "# tests using V2 with image alignment or prompt tuning are too big to fit into the GPU, \n",
    "# look at EasyTPT/test.py for the configurations used to get the results in the report.\n",
    "tpt_tests = [\n",
    "        {\n",
    "            \"name\": \"TPT_baseline_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"augs\": 1,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_sel_A\",\n",
    "            \"dataset\": \"A\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_nosel_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"augs\": 8,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_sel_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 0.10,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_align_A\",\n",
    "            \"dataset\": \"A\",\n",
    "            \"align_steps\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_baseline_V2\",\n",
    "            \"dataset\": \"V2\",\n",
    "            \"augs\": 1,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_nosel_V2\",\n",
    "            \"dataset\": \"V2\",\n",
    "            \"augs\": 8,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TPT_ens_sel_V2\",\n",
    "            \"dataset\": \"V2\",\n",
    "            \"augs\": 64,\n",
    "            \"ensemble\": True,\n",
    "            \"confidence\": 0.1,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "Similarly, for the tests to perform with the ensemble, we use a dictionary with the following structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"TEST_NAME\": {\n",
    "        \"imageNetA\" : True, # if the test should be performed on ImageNet-A or on ImageNet-V2\n",
    "        \"naug\" : 64, # number of augmentations to perform\n",
    "        \"top\" : 0.1, # percentage of the top augmentations to consider (confidence selection)\n",
    "        \"niter\" : 1, # number of optimization steps\n",
    "        \"testSingleModels\" : True, # if we wanto to also compute the results for the single models\n",
    "        \"simple_ensemble\" : True, # if we want to also compute the results for the simple ensemble strategy\n",
    "        \"device\" : \"cuda\", # device to use for the computation\n",
    "            \n",
    "        \"models_type\" : [\"memo\", \"tpt\", \"...\"], # list of models types to use for the ensemble, can be \"memo\" or \"tpt\"\n",
    "        \"args\" : [ # arguments for each model\n",
    "            {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"}, # arguments for the first model\n",
    "            {\"device\": \"cuda\", \"ttt_steps\": 1, \"align_steps\": 0, \"arch\": \"RN50\"}, # arguments for the second model\n",
    "            \"...\"\n",
    "            ],\n",
    "        \"temps\" : [1.55, 0.7], # temperature rescaling to use for each model\n",
    "        \"names\" : [\"MEMO\", \"TPT\"], # names to use for each model\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSTests = {\n",
    "        \"ImageNet-A RN50 + RNXT\": {\n",
    "            \"imageNetA\" : True,\n",
    "            \"naug\" : 64,\n",
    "            \"top\" : 0.1,\n",
    "            \"niter\" : 1,\n",
    "            \"testSingleModels\" : True,\n",
    "            \"simple_ensemble\" : True,\n",
    "            \"device\" : \"cuda\",\n",
    "            \n",
    "            \"models_type\" : [\"memo\", \"memo\"],\n",
    "            \"args\" : [\n",
    "                {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "                {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RNXT\"}\n",
    "                ],\n",
    "            \"temps\" : [1, 1],\n",
    "            \"names\" : [\"MEMO RN50\", \"MEMO RNXT\"],\n",
    "        },\n",
    "\n",
    "        # THIS TEST IS TOO BIG TO RUN IN THE AWS INSTANCE\n",
    "        # \"ImageNet-V2 TPT RN50 + RNXT\": {\n",
    "        #     \"imageNetA\" : False,\n",
    "        #     \"naug\" : 64,\n",
    "        #     \"top\" : 0.2,\n",
    "        #     \"niter\" : 1,\n",
    "        #     \"testSingleModels\" : True,\n",
    "        #     \"simple_ensemble\" : True,\n",
    "        #     \"device\" : \"cuda\",\n",
    "            \n",
    "        #     \"models_type\" : [\"memo\", \"memo\"],\n",
    "        #     \"args\" : [\n",
    "        #         {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "        #         {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RNXT\"}\n",
    "        #         ],\n",
    "        #     \"temps\" : [1, 1],\n",
    "        #     \"names\" : [\"MEMO RN50\", \"MEMO RNXT\"],\n",
    "        # },\n",
    "\n",
    "        \"ImageNet-A TPT + MEMO\": {\n",
    "            \"imageNetA\" : True,\n",
    "            \"naug\" : 64,\n",
    "            \"top\" : 0.2,\n",
    "            \"niter\" : 1,\n",
    "            \"testSingleModels\" : True,\n",
    "            \"simple_ensemble\" : True,\n",
    "            \"device\" : \"cuda\",\n",
    "            \n",
    "            \"models_type\" : [\"memo\", \"tpt\"],\n",
    "            \"args\" : [\n",
    "                {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "                {\"device\": \"cuda\", \"ttt_steps\": 1, \"align_steps\": 0, \"arch\": \"RN50\"}\n",
    "                ],\n",
    "            \"temps\" : [1.55, 0.7],\n",
    "            \"names\" : [\"MEMO\", \"TPT\"],\n",
    "        },\n",
    "\n",
    "        # also too big to run in the AWS instance\n",
    "        # \"ImageNet-A RN50 + RNXT + TPT\": {\n",
    "        #     \"imageNetA\": True,\n",
    "        #     \"naug\": 64,\n",
    "        #     \"top\": 0.1,\n",
    "        #     \"niter\": 1,\n",
    "        #     \"testSingleModels\": True,\n",
    "        #     \"simple_ensemble\": True,\n",
    "        #     \"device\": \"cuda\",\n",
    "\n",
    "        #     \"models_type\": [\"memo\", \"memo\", \"tpt\"],\n",
    "        #     \"args\": [\n",
    "        #         {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RN50\"},\n",
    "        #         {\"device\": \"cuda\", \"drop\": 0, \"ttt_steps\": 1, \"model\": \"RNXT\"},\n",
    "        #         {\"device\": \"cuda\", \"ttt_steps\": 1, \"align_steps\": 0, \"arch\": \"RN50\"}\n",
    "        #     ],\n",
    "        #     \"temps\": [1, 1, 0.7],\n",
    "        #     \"names\": [\"MEMO-RN50\", \"MEMO-RNXT\", \"TPT\"],\n",
    "        #     \"dataset_root\": DATASET_ROOT,\n",
    "        # },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classes = [\"tench\", \"goldfish\", \"great white shark\", \"tiger shark\", \"hammerhead shark\", \"electric ray\", \"stingray\", \"rooster\", \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\", \"junco\", \"indigo bunting\", \"American robin\", \"bulbul\", \"jay\", \"magpie\", \"chickadee\", \"American dipper\", \"kite (bird of prey)\", \"bald eagle\", \"vulture\", \"great grey owl\", \"fire salamander\", \"smooth newt\", \"newt\", \"spotted salamander\", \"axolotl\", \"American bullfrog\", \"tree frog\", \"tailed frog\", \"loggerhead sea turtle\", \"leatherback sea turtle\", \"mud turtle\", \"terrapin\", \"box turtle\", \"banded gecko\", \"green iguana\", \"Carolina anole\", \"desert grassland whiptail lizard\", \"agama\", \"frilled-necked lizard\", \"alligator lizard\", \"Gila monster\", \"European green lizard\", \"chameleon\", \"Komodo dragon\", \"Nile crocodile\", \"American alligator\", \"triceratops\", \"worm snake\", \"ring-necked snake\", \"eastern hog-nosed snake\", \"smooth green snake\", \"kingsnake\", \"garter snake\", \"water snake\", \"vine snake\", \"night snake\", \"boa constrictor\", \"African rock python\", \"Indian cobra\", \"green mamba\", \"sea snake\", \"Saharan horned viper\", \"eastern diamondback rattlesnake\", \"sidewinder rattlesnake\", \"trilobite\", \"harvestman\", \"scorpion\", \"yellow garden spider\", \"barn spider\", \"European garden spider\", \"southern black widow\", \"tarantula\", \"wolf spider\", \"tick\", \"centipede\", \"black grouse\", \"ptarmigan\", \"ruffed grouse\", \"prairie grouse\", \"peafowl\", \"quail\", \"partridge\", \"african grey parrot\", \"macaw\", \"sulphur-crested cockatoo\", \"lorikeet\", \"coucal\", \"bee eater\", \"hornbill\", \"hummingbird\", \"jacamar\", \"toucan\", \"duck\", \"red-breasted merganser\", \"goose\", \"black swan\", \"tusker\", \"echidna\", \"platypus\", \"wallaby\", \"koala\", \"wombat\", \"jellyfish\", \"sea anemone\", \"brain coral\", \"flatworm\", \"nematode\", \"conch\", \"snail\", \"slug\", \"sea slug\", \"chiton\", \"chambered nautilus\", \"Dungeness crab\", \"rock crab\", \"fiddler crab\", \"red king crab\", \"American lobster\", \"spiny lobster\", \"crayfish\", \"hermit crab\", \"isopod\", \"white stork\", \"black stork\", \"spoonbill\", \"flamingo\", \"little blue heron\", \"great egret\", \"bittern bird\", \"crane bird\", \"limpkin\", \"common gallinule\", \"American coot\", \"bustard\", \"ruddy turnstone\", \"dunlin\", \"common redshank\", \"dowitcher\", \"oystercatcher\", \"pelican\", \"king penguin\", \"albatross\", \"grey whale\", \"killer whale\", \"dugong\", \"sea lion\", \"Chihuahua\", \"Japanese Chin\", \"Maltese\", \"Pekingese\", \"Shih Tzu\", \"King Charles Spaniel\", \"Papillon\", \"toy terrier\", \"Rhodesian Ridgeback\", \"Afghan Hound\", \"Basset Hound\", \"Beagle\", \"Bloodhound\", \"Bluetick Coonhound\", \"Black and Tan Coonhound\", \"Treeing Walker Coonhound\", \"English foxhound\", \"Redbone Coonhound\", \"borzoi\", \"Irish Wolfhound\", \"Italian Greyhound\", \"Whippet\", \"Ibizan Hound\", \"Norwegian Elkhound\", \"Otterhound\", \"Saluki\", \"Scottish Deerhound\", \"Weimaraner\", \"Staffordshire Bull Terrier\", \"American Staffordshire Terrier\", \"Bedlington Terrier\", \"Border Terrier\", \"Kerry Blue Terrier\", \"Irish Terrier\", \"Norfolk Terrier\", \"Norwich Terrier\", \"Yorkshire Terrier\", \"Wire Fox Terrier\", \"Lakeland Terrier\", \"Sealyham Terrier\", \"Airedale Terrier\", \"Cairn Terrier\", \"Australian Terrier\", \"Dandie Dinmont Terrier\", \"Boston Terrier\", \"Miniature Schnauzer\", \"Giant Schnauzer\", \"Standard Schnauzer\", \"Scottish Terrier\", \"Tibetan Terrier\", \"Australian Silky Terrier\", \"Soft-coated Wheaten Terrier\", \"West Highland White Terrier\", \"Lhasa Apso\", \"Flat-Coated Retriever\", \"Curly-coated Retriever\", \"Golden Retriever\", \"Labrador Retriever\", \"Chesapeake Bay Retriever\", \"German Shorthaired Pointer\", \"Vizsla\", \"English Setter\", \"Irish Setter\", \"Gordon Setter\", \"Brittany dog\", \"Clumber Spaniel\", \"English Springer Spaniel\", \"Welsh Springer Spaniel\", \"Cocker Spaniel\", \"Sussex Spaniel\", \"Irish Water Spaniel\", \"Kuvasz\", \"Schipperke\", \"Groenendael dog\", \"Malinois\", \"Briard\", \"Australian Kelpie\", \"Komondor\", \"Old English Sheepdog\", \"Shetland Sheepdog\", \"collie\", \"Border Collie\", \"Bouvier des Flandres dog\", \"Rottweiler\", \"German Shepherd Dog\", \"Dobermann\", \"Miniature Pinscher\", \"Greater Swiss Mountain Dog\", \"Bernese Mountain Dog\", \"Appenzeller Sennenhund\", \"Entlebucher Sennenhund\", \"Boxer\", \"Bullmastiff\", \"Tibetan Mastiff\", \"French Bulldog\", \"Great Dane\", \"St. Bernard\", \"husky\", \"Alaskan Malamute\", \"Siberian Husky\", \"Dalmatian\", \"Affenpinscher\", \"Basenji\", \"pug\", \"Leonberger\", \"Newfoundland dog\", \"Great Pyrenees dog\", \"Samoyed\", \"Pomeranian\", \"Chow Chow\", \"Keeshond\", \"brussels griffon\", \"Pembroke Welsh Corgi\", \"Cardigan Welsh Corgi\", \"Toy Poodle\", \"Miniature Poodle\", \"Standard Poodle\", \"Mexican hairless dog (xoloitzcuintli)\", \"grey wolf\", \"Alaskan tundra wolf\", \"red wolf or maned wolf\", \"coyote\", \"dingo\", \"dhole\", \"African wild dog\", \"hyena\", \"red fox\", \"kit fox\", \"Arctic fox\", \"grey fox\", \"tabby cat\", \"tiger cat\", \"Persian cat\", \"Siamese cat\", \"Egyptian Mau\", \"cougar\", \"lynx\", \"leopard\", \"snow leopard\", \"jaguar\", \"lion\", \"tiger\", \"cheetah\", \"brown bear\", \"American black bear\", \"polar bear\", \"sloth bear\", \"mongoose\", \"meerkat\", \"tiger beetle\", \"ladybug\", \"ground beetle\", \"longhorn beetle\", \"leaf beetle\", \"dung beetle\", \"rhinoceros beetle\", \"weevil\", \"fly\", \"bee\", \"ant\", \"grasshopper\", \"cricket insect\", \"stick insect\", \"cockroach\", \"praying mantis\", \"cicada\", \"leafhopper\", \"lacewing\", \"dragonfly\", \"damselfly\", \"red admiral butterfly\", \"ringlet butterfly\", \"monarch butterfly\", \"small white butterfly\", \"sulphur butterfly\", \"gossamer-winged butterfly\", \"starfish\", \"sea urchin\", \"sea cucumber\", \"cottontail rabbit\", \"hare\", \"Angora rabbit\", \"hamster\", \"porcupine\", \"fox squirrel\", \"marmot\", \"beaver\", \"guinea pig\", \"common sorrel horse\", \"zebra\", \"pig\", \"wild boar\", \"warthog\", \"hippopotamus\", \"ox\", \"water buffalo\", \"bison\", \"ram (adult male sheep)\", \"bighorn sheep\", \"Alpine ibex\", \"hartebeest\", \"impala (antelope)\", \"gazelle\", \"arabian camel\", \"llama\", \"weasel\", \"mink\", \"European polecat\", \"black-footed ferret\", \"otter\", \"skunk\", \"badger\", \"armadillo\", \"three-toed sloth\", \"orangutan\", \"gorilla\", \"chimpanzee\", \"gibbon\", \"siamang\", \"guenon\", \"patas monkey\", \"baboon\", \"macaque\", \"langur\", \"black-and-white colobus\", \"proboscis monkey\", \"marmoset\", \"white-headed capuchin\", \"howler monkey\", \"titi monkey\", \"Geoffroy's spider monkey\", \"common squirrel monkey\", \"ring-tailed lemur\", \"indri\", \"Asian elephant\", \"African bush elephant\", \"red panda\", \"giant panda\", \"snoek fish\", \"eel\", \"silver salmon\", \"rock beauty fish\", \"clownfish\", \"sturgeon\", \"gar fish\", \"lionfish\", \"pufferfish\", \"abacus\", \"abaya\", \"academic gown\", \"accordion\", \"acoustic guitar\", \"aircraft carrier\", \"airliner\", \"airship\", \"altar\", \"ambulance\", \"amphibious vehicle\", \"analog clock\", \"apiary\", \"apron\", \"trash can\", \"assault rifle\", \"backpack\", \"bakery\", \"balance beam\", \"balloon\", \"ballpoint pen\", \"Band-Aid\", \"banjo\", \"baluster / handrail\", \"barbell\", \"barber chair\", \"barbershop\", \"barn\", \"barometer\", \"barrel\", \"wheelbarrow\", \"baseball\", \"basketball\", \"bassinet\", \"bassoon\", \"swimming cap\", \"bath towel\", \"bathtub\", \"station wagon\", \"lighthouse\", \"beaker\", \"military hat (bearskin or shako)\", \"beer bottle\", \"beer glass\", \"bell tower\", \"baby bib\", \"tandem bicycle\", \"bikini\", \"ring binder\", \"binoculars\", \"birdhouse\", \"boathouse\", \"bobsleigh\", \"bolo tie\", \"poke bonnet\", \"bookcase\", \"bookstore\", \"bottle cap\", \"hunting bow\", \"bow tie\", \"brass memorial plaque\", \"bra\", \"breakwater\", \"breastplate\", \"broom\", \"bucket\", \"buckle\", \"bulletproof vest\", \"high-speed train\", \"butcher shop\", \"taxicab\", \"cauldron\", \"candle\", \"cannon\", \"canoe\", \"can opener\", \"cardigan\", \"car mirror\", \"carousel\", \"tool kit\", \"cardboard box / carton\", \"car wheel\", \"automated teller machine\", \"cassette\", \"cassette player\", \"castle\", \"catamaran\", \"CD player\", \"cello\", \"mobile phone\", \"chain\", \"chain-link fence\", \"chain mail\", \"chainsaw\", \"storage chest\", \"chiffonier\", \"bell or wind chime\", \"china cabinet\", \"Christmas stocking\", \"church\", \"movie theater\", \"cleaver\", \"cliff dwelling\", \"cloak\", \"clogs\", \"cocktail shaker\", \"coffee mug\", \"coffeemaker\", \"spiral or coil\", \"combination lock\", \"computer keyboard\", \"candy store\", \"container ship\", \"convertible\", \"corkscrew\", \"cornet\", \"cowboy boot\", \"cowboy hat\", \"cradle\", \"construction crane\", \"crash helmet\", \"crate\", \"infant bed\", \"Crock Pot\", \"croquet ball\", \"crutch\", \"cuirass\", \"dam\", \"desk\", \"desktop computer\", \"rotary dial telephone\", \"diaper\", \"digital clock\", \"digital watch\", \"dining table\", \"dishcloth\", \"dishwasher\", \"disc brake\", \"dock\", \"dog sled\", \"dome\", \"doormat\", \"drilling rig\", \"drum\", \"drumstick\", \"dumbbell\", \"Dutch oven\", \"electric fan\", \"electric guitar\", \"electric locomotive\", \"entertainment center\", \"envelope\", \"espresso machine\", \"face powder\", \"feather boa\", \"filing cabinet\", \"fireboat\", \"fire truck\", \"fire screen\", \"flagpole\", \"flute\", \"folding chair\", \"football helmet\", \"forklift\", \"fountain\", \"fountain pen\", \"four-poster bed\", \"freight car\", \"French horn\", \"frying pan\", \"fur coat\", \"garbage truck\", \"gas mask or respirator\", \"gas pump\", \"goblet\", \"go-kart\", \"golf ball\", \"golf cart\", \"gondola\", \"gong\", \"gown\", \"grand piano\", \"greenhouse\", \"radiator grille\", \"grocery store\", \"guillotine\", \"hair clip\", \"hair spray\", \"half-track\", \"hammer\", \"hamper\", \"hair dryer\", \"hand-held computer\", \"handkerchief\", \"hard disk drive\", \"harmonica\", \"harp\", \"combine harvester\", \"hatchet\", \"holster\", \"home theater\", \"honeycomb\", \"hook\", \"hoop skirt\", \"gymnastic horizontal bar\", \"horse-drawn vehicle\", \"hourglass\", \"iPod\", \"clothes iron\", \"carved pumpkin\", \"jeans\", \"jeep\", \"T-shirt\", \"jigsaw puzzle\", \"rickshaw\", \"joystick\", \"kimono\", \"knee pad\", \"knot\", \"lab coat\", \"ladle\", \"lampshade\", \"laptop computer\", \"lawn mower\", \"lens cap\", \"letter opener\", \"library\", \"lifeboat\", \"lighter\", \"limousine\", \"ocean liner\", \"lipstick\", \"slip-on shoe\", \"lotion\", \"music speaker\", \"loupe magnifying glass\", \"sawmill\", \"magnetic compass\", \"messenger bag\", \"mailbox\", \"tights\", \"one-piece bathing suit\", \"manhole cover\", \"maraca\", \"marimba\", \"mask\", \"matchstick\", \"maypole\", \"maze\", \"measuring cup\", \"medicine cabinet\", \"megalith\", \"microphone\", \"microwave oven\", \"military uniform\", \"milk can\", \"minibus\", \"miniskirt\", \"minivan\", \"missile\", \"mitten\", \"mixing bowl\", \"mobile home\", \"ford model t\", \"modem\", \"monastery\", \"monitor\", \"moped\", \"mortar and pestle\", \"graduation cap\", \"mosque\", \"mosquito net\", \"vespa\", \"mountain bike\", \"tent\", \"computer mouse\", \"mousetrap\", \"moving van\", \"muzzle\", \"metal nail\", \"neck brace\", \"necklace\", \"baby pacifier\", \"notebook computer\", \"obelisk\", \"oboe\", \"ocarina\", \"odometer\", \"oil filter\", \"pipe organ\", \"oscilloscope\", \"overskirt\", \"bullock cart\", \"oxygen mask\", \"product packet / packaging\", \"paddle\", \"paddle wheel\", \"padlock\", \"paintbrush\", \"pajamas\", \"palace\", \"pan flute\", \"paper towel\", \"parachute\", \"parallel bars\", \"park bench\", \"parking meter\", \"railroad car\", \"patio\", \"payphone\", \"pedestal\", \"pencil case\", \"pencil sharpener\", \"perfume\", \"Petri dish\", \"photocopier\", \"plectrum\", \"Pickelhaube\", \"picket fence\", \"pickup truck\", \"pier\", \"piggy bank\", \"pill bottle\", \"pillow\", \"ping-pong ball\", \"pinwheel\", \"pirate ship\", \"drink pitcher\", \"block plane\", \"planetarium\", \"plastic bag\", \"plate rack\", \"farm plow\", \"plunger\", \"Polaroid camera\", \"pole\", \"police van\", \"poncho\", \"pool table\", \"soda bottle\", \"plant pot\", \"potter's wheel\", \"power drill\", \"prayer rug\", \"printer\", \"prison\", \"missile\", \"projector\", \"hockey puck\", \"punching bag\", \"purse\", \"quill\", \"quilt\", \"race car\", \"racket\", \"radiator\", \"radio\", \"radio telescope\", \"rain barrel\", \"recreational vehicle\", \"fishing casting reel\", \"reflex camera\", \"refrigerator\", \"remote control\", \"restaurant\", \"revolver\", \"rifle\", \"rocking chair\", \"rotisserie\", \"eraser\", \"rugby ball\", \"ruler measuring stick\", \"sneaker\", \"safe\", \"safety pin\", \"salt shaker\", \"sandal\", \"sarong\", \"saxophone\", \"scabbard\", \"weighing scale\", \"school bus\", \"schooner\", \"scoreboard\", \"CRT monitor\", \"screw\", \"screwdriver\", \"seat belt\", \"sewing machine\", \"shield\", \"shoe store\", \"shoji screen / room divider\", \"shopping basket\", \"shopping cart\", \"shovel\", \"shower cap\", \"shower curtain\", \"ski\", \"balaclava ski mask\", \"sleeping bag\", \"slide rule\", \"sliding door\", \"slot machine\", \"snorkel\", \"snowmobile\", \"snowplow\", \"soap dispenser\", \"soccer ball\", \"sock\", \"solar thermal collector\", \"sombrero\", \"soup bowl\", \"keyboard space bar\", \"space heater\", \"space shuttle\", \"spatula\", \"motorboat\", \"spider web\", \"spindle\", \"sports car\", \"spotlight\", \"stage\", \"steam locomotive\", \"through arch bridge\", \"steel drum\", \"stethoscope\", \"scarf\", \"stone wall\", \"stopwatch\", \"stove\", \"strainer\", \"tram\", \"stretcher\", \"couch\", \"stupa\", \"submarine\", \"suit\", \"sundial\", \"sunglasses\", \"sunglasses\", \"sunscreen\", \"suspension bridge\", \"mop\", \"sweatshirt\", \"swim trunks / shorts\", \"swing\", \"electrical switch\", \"syringe\", \"table lamp\", \"tank\", \"tape player\", \"teapot\", \"teddy bear\", \"television\", \"tennis ball\", \"thatched roof\", \"front curtain\", \"thimble\", \"threshing machine\", \"throne\", \"tile roof\", \"toaster\", \"tobacco shop\", \"toilet seat\", \"torch\", \"totem pole\", \"tow truck\", \"toy store\", \"tractor\", \"semi-trailer truck\", \"tray\", \"trench coat\", \"tricycle\", \"trimaran\", \"tripod\", \"triumphal arch\", \"trolleybus\", \"trombone\", \"hot tub\", \"turnstile\", \"typewriter keyboard\", \"umbrella\", \"unicycle\", \"upright piano\", \"vacuum cleaner\", \"vase\", \"vaulted or arched ceiling\", \"velvet fabric\", \"vending machine\", \"vestment\", \"viaduct\", \"violin\", \"volleyball\", \"waffle iron\", \"wall clock\", \"wallet\", \"wardrobe\", \"military aircraft\", \"sink\", \"washing machine\", \"water bottle\", \"water jug\", \"water tower\", \"whiskey jug\", \"whistle\", \"hair wig\", \"window screen\", \"window shade\", \"Windsor tie\", \"wine bottle\", \"airplane wing\", \"wok\", \"wooden spoon\", \"wool\", \"split-rail fence\", \"shipwreck\", \"sailboat\", \"yurt\", \"website\", \"comic book\", \"crossword\", \"traffic or street sign\", \"traffic light\", \"dust jacket\", \"menu\", \"plate\", \"guacamole\", \"consomme\", \"hot pot\", \"trifle\", \"ice cream\", \"popsicle\", \"baguette\", \"bagel\", \"pretzel\", \"cheeseburger\", \"hot dog\", \"mashed potatoes\", \"cabbage\", \"broccoli\", \"cauliflower\", \"zucchini\", \"spaghetti squash\", \"acorn squash\", \"butternut squash\", \"cucumber\", \"artichoke\", \"bell pepper\", \"cardoon\", \"mushroom\", \"Granny Smith apple\", \"strawberry\", \"orange\", \"lemon\", \"fig\", \"pineapple\", \"banana\", \"jackfruit\", \"cherimoya (custard apple)\", \"pomegranate\", \"hay\", \"carbonara\", \"chocolate syrup\", \"dough\", \"meatloaf\", \"pizza\", \"pot pie\", \"burrito\", \"red wine\", \"espresso\", \"tea cup\", \"eggnog\", \"mountain\", \"bubble\", \"cliff\", \"coral reef\", \"geyser\", \"lakeshore\", \"promontory\", \"sandbar\", \"beach\", \"valley\", \"volcano\", \"baseball player\", \"bridegroom\", \"scuba diver\", \"rapeseed\", \"daisy\", \"yellow lady's slipper\", \"corn\", \"acorn\", \"rose hip\", \"horse chestnut seed\", \"coral fungus\", \"agaric\", \"gyromitra\", \"stinkhorn mushroom\", \"earth star fungus\", \"hen of the woods mushroom\", \"bolete\", \"corn cob\", \"toilet paper\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ImageNet-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images from the ImageNet-A dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, csvMapFile=\"wordNetIDs2Classes.csv\", transform=None\n",
    "    ):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        #print(objects)\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        mapping = {}\n",
    "        csv_file = csv.reader(open(csvMapFile, \"r\"))\n",
    "        for id, wordnet, name in csv_file:\n",
    "            if id == \"resnet_label\":\n",
    "                continue\n",
    "            mapping[int(wordnet)] = {\"id\": id, \"name\": name}\n",
    "\n",
    "        # print(mapping)\n",
    "        self.classnames = {}\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "\n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = int(path.parent.name[1:])\n",
    "            name = mapping[label][\"name\"]\n",
    "            self.classnames[mapping[label][\"id\"]] = name\n",
    "            label = int(mapping[label][\"id\"])\n",
    "\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, name, key))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, name, key = self.instances[idx]\n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            img_bytes.seek(0)  # Ensure the BytesIO object is at the start\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return {\"img\": img, \"label\": label, \"name\": name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ImageNet-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images from the ImageNet-V2 dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, csvMapFile=\"wordNetIDs2Classes.csv\", transform=None\n",
    "    ):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        mapping = {}\n",
    "        csv_file = csv.reader(open(csvMapFile, \"r\"))\n",
    "        for id, _, name in csv_file:\n",
    "            if id == \"resnet_label\":\n",
    "                continue\n",
    "            mapping[id] = name\n",
    "\n",
    "        self.classnames = {}\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "\n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "            name = mapping[label]\n",
    "            self.classnames[label] = name\n",
    "\n",
    "            label = int(label)\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, name, key))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, name, key = self.instances[idx]\n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            img_bytes.seek(0)  # Ensure the BytesIO object is at the start\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return {\"img\": img, \"label\": label, \"name\": name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EasyAugmenter\n",
    "\n",
    "Augmentations are a core part of the MEMO and TPT techniques, both of them rely on the idea of applying multiple augmentations to the same image and then adapt the model, in various way, to the average output of the model across the augmentations. \n",
    "\n",
    "The structure of EasyAugmenter is similar to the one used in the official TPT repository, with some adaptation to make it more flexible to the needs of the different techniques it's supposed to be used with. At the core it takes two predefined sets of augmentations: **preprocess** and **base_transform**. \n",
    "\n",
    "The **preprocess** transformations are applied as the last step in the chain, preprocess transforms are tied to the pretrained model used as backbone, they contain the normalization and conversions of the image needed to make it compatible with the model input and distribution.\n",
    "\n",
    "The **base_transform** transformations are a set of simple transformations such as crop and resize\n",
    "\n",
    "Finally it's possible to specify 3 different modalities of augmentations: `augmix`, `cut` and `identity`. `identity`, as the name suggests, doesn't apply any transformation to the image, the result will be a set of identical images, in our case this specific modality is used when MEMO works in the dropout mode. `cut` applies a random crops and resize to the image, while `augmix` applies a set of augmentations to the image, the augmentations are randomly selected from a predefined set of transformations from the AugMix module.\n",
    "\n",
    "In any case the result of the augmentation will be a list of size **n_views + 1** containing the augmented samples as well as the original (transformed) image in first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyAgumenter(object):\n",
    "    def __init__(self, base_transform, preprocess, augmentation, n_views=63):\n",
    "        \"\"\"\n",
    "        This class provides an easy way to apply custom augmentations to images, the when called \n",
    "        it will return a list of augmentations with the original image in first place.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        - base_transform (torchvision.transforms.Compose): The base transformation to apply to the images.\n",
    "        - preprocess (torchvision.transforms.Compose): The preprocessing transformation to apply to the images (will be applied last).\n",
    "        - augmentation (str): The type of augmentation to apply, can be 'augmix', 'identity' or 'cut'.\n",
    "        - n_views (int): The number of augmentations to apply to the image.\n",
    "        \n",
    "        Returns:\n",
    "        - (list) A list of images with the augmentations applied.\n",
    "        \"\"\"\n",
    "        self.base_transform = base_transform\n",
    "        self.preprocess = preprocess\n",
    "        self.n_views = n_views\n",
    "\n",
    "        if augmentation == 'augmix':\n",
    "\n",
    "            self.preaugment = transforms.Compose(\n",
    "                [\n",
    "                    AugMix(),\n",
    "                    transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "                    transforms.CenterCrop(224),\n",
    "                ]\n",
    "            )\n",
    "        elif augmentation == 'identity':\n",
    "            self.preaugment = self.base_transform\n",
    "        elif augmentation == 'cut':\n",
    "            self.preaugment = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Augmentation type not recognized')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = transforms.ToPILImage()(x)\n",
    "\n",
    "        image = self.preprocess(self.base_transform(x))\n",
    "\n",
    "        views = [self.preprocess(self.preaugment(x)) for _ in range(self.n_views)]\n",
    "\n",
    "        return [image] + views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(root, transform=None, csvMapFile=\"wordNetIDs2Classes.csv\"):\n",
    "    \"\"\"\n",
    "    Returns the dataloader of the dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "    root_A = \"imagenet-a\"\n",
    "    imageNet_A = ImageNetA(root_A, transform=transform, csvMapFile=csvMapFile)\n",
    "    root_V2 = \"imagenetv2-matched-frequency-format-val\"\n",
    "    imageNet_V2 = ImageNetV2(root_V2, transform=transform, csvMapFile=csvMapFile)\n",
    "\n",
    "    return imageNet_A, imageNet_V2\n",
    "\n",
    "def get_classes_names(csvMapFile=\"wordNetIDs2Classes.csv\"):\n",
    "    \"\"\"\n",
    "    Returns the class names of the dataset.\n",
    "\n",
    "    Args:\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "    \"\"\"\n",
    "    names = [\"\"]*1000\n",
    "    csv_file = csv.reader(open(csvMapFile, 'r'))\n",
    "    for id, wordnet, name in csv_file:\n",
    "        if id == 'resnet_label':\n",
    "            continue\n",
    "        names[int(id)] = name\n",
    "    \n",
    "    return names\n",
    "\n",
    "def memo_get_datasets(augmentation, augs=64, dataset_root=\"datasets\"):\n",
    "    \"\"\"\n",
    "    Returns the ImageNetA and ImageNetV2 datasets for the memo model\n",
    "    Args:\n",
    "        dataset_root: the root folder of all the datasets\n",
    "        augmentation (str): What type of augmentation to use in EasyAugmenter. Can be 'augmix', 'identity' or 'cut'\n",
    "        augs (int): The number of augmentations to compute. Must be greater than 1\n",
    "\n",
    "    Returns: The ImageNetA and ImageNetV2 datasets for the memo model, with the Augmentations already applied\n",
    "\n",
    "    \"\"\"\n",
    "    assert augs > 0, 'The number of augmentations must be greater than 0'\n",
    "    memo_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224)])\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform = EasyAgumenter(memo_transforms, preprocess, augmentation, augs - 1)\n",
    "    imageNet_A, imageNet_V2 = get_dataloaders(dataset_root, transform)\n",
    "    return imageNet_A, imageNet_V2\n",
    "\n",
    "\n",
    "def tpt_get_transforms(augs=64):\n",
    "\n",
    "    base_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_transform = EasyAgumenter(\n",
    "        base_transform,\n",
    "        preprocess,\n",
    "        n_views=augs - 1,\n",
    "    )\n",
    "\n",
    "    return data_transform\n",
    "\n",
    "\n",
    "def tpt_get_datasets(data_root, augmix=False, augs=64, all_classes=True):\n",
    "    \"\"\"\n",
    "    Returns the ImageNetA and ImageNetV2 datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - data_root (str): The root directory of the datasets.\n",
    "    - augmix (bool): Whether to use AugMix or not.\n",
    "    - augs (int): The number of augmentations to use.\n",
    "    - all_classes (bool): Whether to use all classes or not.\n",
    "\n",
    "    Returns:\n",
    "    - imageNet_A (ImageNetA): The ImageNetA dataset.\n",
    "    - ima_names (list): The original classnames in ImageNetA.\n",
    "    - ima_custom_names (list): The retouched  classnames in ImageNetA.\n",
    "    - ima_id_mapping (list): The mapping between the index of the classname and the ImageNet label\n",
    "\n",
    "    same for ImageNetV2\n",
    "\n",
    "    For instance the first element of ima_names corresponds to the label '90'.  After running the\n",
    "    inference run the predicted output through the ima_id_mapping to recover the correct class label.\n",
    "\n",
    "    out = tpt(inputs)\n",
    "    pred = out.argmax().item()\n",
    "    out_id = ima_id_mapping[pred]\n",
    "\n",
    "    \"\"\"\n",
    "    base_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_transform = EasyAgumenter(\n",
    "        base_transform,\n",
    "        preprocess,\n",
    "        augmentation=(\"augmix\" if augmix else \"cut\"),\n",
    "        n_views=augs - 1,\n",
    "    )\n",
    "\n",
    "    imageNet_A = ImageNetA(\n",
    "        \"imagenet-a\", transform=data_transform\n",
    "    )\n",
    "    imageNet_V2 = ImageNetV2(\n",
    "        \"imagenetv2-matched-frequency-format-val\",\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "    imv2_label_mapping = list(imageNet_V2.classnames.keys())\n",
    "    imv2_names = list(imageNet_V2.classnames.values())\n",
    "    imv2_custom_names = [imagenet_classes[int(i)] for i in imv2_label_mapping]\n",
    "\n",
    "    ima_label_mapping = list(imageNet_A.classnames.keys())\n",
    "    ima_names = list(imageNet_A.classnames.values())\n",
    "    ima_custom_names = [imagenet_classes[int(i)] for i in ima_label_mapping]\n",
    "\n",
    "    if all_classes:\n",
    "        ima_names += [name for name in imv2_names if name not in ima_names]\n",
    "        ima_custom_names += [\n",
    "            name for name in imv2_custom_names if name not in ima_custom_names\n",
    "        ]\n",
    "        ima_label_mapping += [\n",
    "            map for map in imv2_label_mapping if map not in ima_label_mapping\n",
    "        ]\n",
    "\n",
    "    return (\n",
    "        imageNet_A,\n",
    "        ima_names,\n",
    "        ima_custom_names,\n",
    "        ima_label_mapping,\n",
    "        imageNet_V2,\n",
    "        imv2_names,\n",
    "        imv2_custom_names,\n",
    "        imv2_label_mapping,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EasyModel\n",
    "This is the parent class for all the models used in the experiments, it's used as a template to define the structure of the models and the methods that are needed to run the tests. The class is composed of the following methods:\n",
    "- **select_confident_samples**: This method is used to select the topk most confident samples from the output of the model\n",
    "- **avg_entropy**: This method is used to compute the average entropy of the model output across the augmentations\n",
    "- **forward**: The forward pass of the model, it takes the input image and returns the output of the model\n",
    "- **predict**: The predict method is used to get the prediction of the model on the input image\n",
    "- **reset**: This method is used to reset the model to the initial state\n",
    "\n",
    "Each model that inherits from EasyModel should override theses methods to adapt them to the specific needs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EasyModel, self).__init__()\n",
    "\n",
    "    def select_confident_samples(self, logits, top):\n",
    "        \"\"\"\n",
    "        Performs confidence selection, will return the indexes of the\n",
    "        augmentations with the highest confidence as well as the filtered\n",
    "        logits\n",
    "\n",
    "        Parameters:\n",
    "        - logits (torch.Tensor): the logits of the model [NAUGS, NCLASSES]\n",
    "        - top (float): the percentage of top augmentations to use\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): the filtered logits of the model [NAUGS*top, NCLASSES]\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "        return logits[idx], idx\n",
    "    \n",
    "    def avg_entropy(self, outputs):\n",
    "        \"\"\"\n",
    "        Computes the average entropy of the model outputs\n",
    "\n",
    "        Parameters:\n",
    "        - outputs (torch.Tensor): the logits of the model [NAUGS, NCLASSES]\n",
    "        \n",
    "        Returns:\n",
    "        - avg_entropy (torch.Tensor): the average entropy of the model outputs [1]\n",
    "        \"\"\"\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(EasyModel, self).forward(x)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MEMO\n",
    "Here we see the implementation of our `EasyMemo` Class.\n",
    "`EasyMemo` inherits from `EasyModel` and implements the functions that are specific to the implementation described in the paper \"MEMO: Test Time Robustness via\n",
    "Adaptation and Augmentation\".\n",
    "The implementation we have is much cleaner, as everything is encapsulated in the `EasyMemo` Class.\n",
    "We implemented a simple and modular way to predict a sample. We wrap the model we choose, in our case a `ResNet50` neural network, and implement the **inference**, **forward** and **predict** steps so that we can easily switch between the different modes of operation.\n",
    "### MEMO Test Time Robustness via Adaptation and Augmentation\n",
    "In the MEMO paper, the authors propose a modified version of the batch normalization layer, where the running mean and variance are updated at each iteration with the mean and variance of the current modified batch. \n",
    "This is done to adapt the model to the current data distribution. The prior strength is used to control the influence of the current batch on the running statistics. \n",
    "The authors show that this method is effective in improving the robustness of the model to adversarial attacks and other perturbations.\n",
    "\n",
    "If we want to leave the original variance and mean, we just keep the prior strength to 1, and the model will use the original values without modifying them.\n",
    "#### Implementation details\n",
    "The implementation is quite simple, we only took the modified batch normalization layer and applied it to the model, then we pass the sample to the predict function, which applies backpropagation on the single test sample and updates the running statistics of the batch normalization layer with the chosen prior strength (e.g. 0.94 for the MEMO tests). \n",
    "The values used are those from the paper.\n",
    "\n",
    "### Confidence selection\n",
    "The confidence selection is a method used to select the most confident samples from the output of the model.\n",
    "The method is quite simple, we compute the entropy of the model output across the augmentations and select the top percentage of the samples to use for the final prediction.\n",
    "In this way, we can select the most confident samples and use them to make the final prediction.\n",
    "\n",
    "#### Implementation details\n",
    "In the **select_confident_samples** function we compute the entropy of the model output across the augmentations and select the top percentage of the samples to use for the final prediction.\n",
    "After we have done that, we can base our predict call on the logits that are the best for our prediction.\n",
    "\n",
    "### Ensemble\n",
    "To cut the time of inference we could simply use multiple augmentations of the same image and average the predictions. \n",
    "This is the idea behind the ensemble method, where we use a batch of multiple augmentations of the same image and average the predictions to get the final prediction.\n",
    "We can combine these methods with the confidence selection to obtain better results\n",
    "#### Augmentation ensemble(cut)\n",
    "This ensemble is done with the random cut augmentation, and turn out to be quite effective since the prediction is much more focused on different areas of the image.\n",
    "Many times, samples of the Imagenet-A dataset are images that have much noise in it and only a portion of the image is relevant to the class.\n",
    "By using different cuts, we focus on different areas of the image that we can test, and further improve the ability of the model to make a confident guess.\n",
    "\n",
    "##### Implementation details\n",
    "We use the `EasyAugmenter` class to apply the random cut augmentation to the image, and then we pass the augmented image to the model to get the logits. \n",
    "The results are given by a classic softmax layer.\n",
    "\n",
    "#### Dropout ensemble\n",
    "If an image should be classified as something, then most of the features extracted should point toward it, but there might be some noise from other features that influence the result or may even strongly polarize the classification.\n",
    "Using dropout we want to zero stochastically some features, hoping that the majority of the good ones will be kept, while the others will be removed. \n",
    "\n",
    "##### Implementation details\n",
    "With the dropout ensemble we add a dropout layer at the end of the residual blocks of the network, before the average pooling layer. \n",
    "Here we see where we added the dropout layer, that was added after the sequential block because it was faster to implement. \n",
    "```\n",
    "(...(...(...\n",
    "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        (relu): ReLU(inplace=True)\n",
    "      )\n",
    "      (dropout): Dropout(p=0, inplace=True)\n",
    "    )\n",
    "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
    "  )\n",
    "```\n",
    "Passing a batch to the model will result in a different output at each iteration, and we can average the results to get the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modified_bn_forward(self, input):\n",
    "    est_mean = torch.zeros(self.running_mean.shape, device=self.running_mean.device)\n",
    "    est_var = torch.ones(self.running_var.shape, device=self.running_var.device)\n",
    "    nn.functional.batch_norm(input, est_mean, est_var, None, None, True, 1.0, self.eps)\n",
    "    running_mean = self.prior * self.running_mean + (1 - self.prior) * est_mean\n",
    "    running_var = self.prior * self.running_var + (1 - self.prior) * est_var\n",
    "    return nn.functional.batch_norm(input, running_mean, running_var, self.weight, self.bias, False, 0, self.eps)\n",
    "\n",
    "\n",
    "class EasyMemo(EasyModel):\n",
    "    \"\"\"\n",
    "    A class to wrap a neural network with the MEMO TTA method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, device, classes_mask, prior_strength: float = 1.0, lr=0.005, weight_decay=0.0001, opt='sgd',\n",
    "                 niter=1, top=0.1, ensemble=False):\n",
    "        \"\"\"\n",
    "        Initializes the EasyMemo model with various arguments\n",
    "        Args:\n",
    "            net: The model to wrap with EasyMemo\n",
    "            device: The device to run the model on(usually 'CPU' or 'CUDA')\n",
    "            classes_mask: The classes to consider for the model(used for Imagenet-A)\n",
    "            prior_strength: The strength of the prior to use in the modified BN forward pass\n",
    "            lr: The Learning rate for the optimizer of the model\n",
    "            weight_decay: The weight decay for the optimizer of the model\n",
    "            opt: Which optimizer to use for this model between 'sgd' and 'adamw' for the respective optimizers\n",
    "            niter: The number of iterations to run the memo pass for\n",
    "            top: The percentage of the top logits to consider for confidence selection\n",
    "            ensemble: Whether to use the ensemble method or not\n",
    "        \"\"\"\n",
    "        super(EasyMemo, self).__init__()\n",
    "\n",
    "        self.ens = ensemble\n",
    "        self.device = device\n",
    "        self.prior_strength = prior_strength\n",
    "        self.net = net.to(device)\n",
    "        self.optimizer = self.get_optimizer(lr=lr, weight_decay=weight_decay, opt=opt)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.opt = opt\n",
    "        self.confidence_idx = None\n",
    "        self.memo_modify_bn_pass()\n",
    "        self.criterion = self.avg_entropy\n",
    "        self.niter = niter\n",
    "        self.top = top\n",
    "        self.initial_state = deepcopy(self.net.state_dict())\n",
    "        self.classes_mask = classes_mask\n",
    "\n",
    "    def forward(self, x, top=-1):\n",
    "        \"\"\"\n",
    "        Forward pass where we check which type of input we have and we call the inference on the input image Tensor\n",
    "        Args:\n",
    "            top: How many samples to select from the batch\n",
    "            x: A Tensor of shape (N, C, H, W) or a list of Tensors of shape (N, C, H, W)\n",
    "\n",
    "        Returns: The logits after the inference pass\n",
    "\n",
    "        \"\"\"\n",
    "        self.top = top if top > 0 else self.top\n",
    "        # print(f\"Shape forward: {x.shape}\")\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x).to(self.device)\n",
    "            # print(f\"Shape forward: {x.shape}\")\n",
    "            logits = self.inference(x)\n",
    "            logits, self.confidence_idx = self.select_confident_samples(logits, self.top)\n",
    "        else:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = x.to(self.device)\n",
    "            logits = self.inference(x)\n",
    "\n",
    "        # print(f\"[EasyMemo] input shape: {x.shape}\")\n",
    "        # print(f\"[EasyMemo] logits shape: {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Return the logits of the image in input x\n",
    "        Args:\n",
    "            x: A Tensor of shape (N, C, H, W) of an Image\n",
    "\n",
    "        Returns: The logits for that Tensor image\n",
    "\n",
    "        \"\"\"\n",
    "        if self.ens:\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "        outputs = self.net(x)\n",
    "\n",
    "        out_app = torch.zeros(outputs.shape[0], len(self.classes_mask)).to(self.device)\n",
    "        for i, out in enumerate(outputs):\n",
    "            out_app[i] = out[self.classes_mask]\n",
    "        return out_app\n",
    "\n",
    "    def predict(self, x, niter=1):\n",
    "        \"\"\"\n",
    "        Predicts the class of the input x, which is an image\n",
    "        Args:\n",
    "            niter: The number of iteration on which to run the memo pass\n",
    "            x: Tensor of shape (N, C, H, W)\n",
    "\n",
    "        Returns: The predicted classes\n",
    "\n",
    "        \"\"\"\n",
    "        self.niter = niter\n",
    "        nn.BatchNorm2d.prior = self.prior_strength\n",
    "\n",
    "        if self.ens:\n",
    "            self.net.train()\n",
    "            predicted = self.ensemble(x)\n",
    "        else:\n",
    "            self.net.eval()\n",
    "            for iteration in range(self.niter):\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(x)\n",
    "                loss = self.criterion(outputs)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.net(x[0].unsqueeze(0).to(self.device))\n",
    "                outs = torch.zeros(outputs.shape[0], len(self.classes_mask)).to(self.device)\n",
    "                for i, out in enumerate(outputs):\n",
    "                    outs[i] = out[self.classes_mask]\n",
    "                predicted = outs.argmax(1).item()\n",
    "\n",
    "        nn.BatchNorm2d.prior = 1.0\n",
    "        return predicted\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the model to its initial state\"\"\"\n",
    "        del self.optimizer\n",
    "        self.optimizer = self.get_optimizer(lr=self.lr, weight_decay=self.weight_decay, opt=self.opt)\n",
    "        self.confidence_idx = None\n",
    "        self.net.load_state_dict(deepcopy(self.initial_state))\n",
    "\n",
    "    def memo_modify_bn_pass(self):\n",
    "        print('modifying BN forward pass')\n",
    "        nn.BatchNorm2d.prior = 1.0\n",
    "        nn.BatchNorm2d.forward = _modified_bn_forward\n",
    "\n",
    "    def get_optimizer(self, lr=0.005, weight_decay=0.0001, opt='sgd'):\n",
    "        \"\"\"\n",
    "        Initializes the optimizer for the memo model\n",
    "        Args:\n",
    "            lr: The learning rate for the optimizer\n",
    "            weight_decay: The weight decay for the optimizer\n",
    "            opt: Which optimizer to use\n",
    "\n",
    "        Returns: The optimizer for the memo model\n",
    "\n",
    "        \"\"\"\n",
    "        if opt == 'sgd':\n",
    "            optimizer = optim.SGD(self.net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif opt == 'adamw':\n",
    "            optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer selected')\n",
    "        return optimizer\n",
    "\n",
    "    def ensemble(self, x):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            outputs = nn.functional.softmax(outputs, dim=1)\n",
    "            prediction = outputs.sum(0).argmax().item()\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPT\n",
    "\n",
    "This is the core of our implementation of Test-Time Prompt Tuning. \n",
    "\n",
    "`EasyTPT` inherits from `EasyModel` and acts as a wrapper around `EasyPromptLearner` as well as implementing the functions that are specific to each different mode of operation. The implementation is based on the paper \"Test-time Prompt Tuning for Image Classification\" and it's inspired to the original one by the authors, our adaptation is more polished and offers a set of two additional modes of operation and some improvements.\n",
    "\n",
    "### Alignment Mode\n",
    "This is the first of the two additional modes of operation, it can be triggered by setting 'align_steps' to a value greater than 0. The idea behind this modality is to pull closer the image embeddings by acting directly on CLIP's image encoder as this could result in lower variance between the augmented images and therefore in a more stable classification. The alignment is performed by maximizing the cosine similarity between the embeddings of the set of augmented images. In a similar fashion to the prompt learner, before running the actual classification step, we perform a forward pass of the augmented images through the image encoder and then we compute the cosine similarity between the embeddings, at this point the error is backpropagated to the image encoder and the weights are updated, we exclude the attention weights from the optimization process. This procedure is repeated for `align_steps` times before the classification step.\n",
    "\n",
    "This implementation could be seen as contrastive learning, where we try to make the embeddings of the augmented images closer to each other by confronting their self-similarity between the set. The results however are not as good as the ones obtained by the prompt learner, testing proved that the alignment step is not very beneficial and on top of that it's computationally expensive as we need to perform a backward pass on the image encoder. It is even memory hungry as we need to build its computational graph too.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "In test time adaptation the model is supposed to be unaware of the target domain so no information about past data should be avaiable. For this reason at each prediction we must reset the model to it's original state, this includes both the model parameters and the optimizer state, both of them are saved at model initialization and reloaded every time the model is reset.\n",
    "\n",
    "To perform alignment we are forced to use `clip.float()` to convert the model to float precision, this is due to the fact that when fine-tuning CLIP the use of different precision causes the gradients to show NaN values, it's not clear why this happens, but it could be due to numerical incompatibilities and numerical instability. The side effect of this is that the model is now using more memory and is slower to run, this is a trade-off that we have to accept to be able to perform alignment.\n",
    "\n",
    "Alignment too is subject to confidence selection: just like with prompt tuning we are interested the most confident predictions.\n",
    "\n",
    "### Ensemble Mode\n",
    "This is the second of the two additional modes of operation, it can be triggered by setting 'ensemble' to *True*. The idea behind this modality is staggering simple: instead of using the entropy of the predictions to tune the prompt in a way that the predictions are more confident and, hopefully, more accurate, we skip the prompt tuning step altogether and pick the class with the highest marginal probability, in a sense, we are skipping the \"middle man\" by taking directly the most confident marginalized prediction.\n",
    "\n",
    "This method can be seen as using an ensemble of models, where instead of running the same image through multiple models and averaging the predictions, we run multiple augmentations of the same image through the same model and average the predictions. This method is extremely efficient compared to prompt tuning both in terms of computational cost and memory usage, as we only need to pass the run some forwards passes and no learning is involved whatsoever.\n",
    "\n",
    "What's even more surprising is that this method is extremely effective, not only scoring better than the baseline, but outperforming the prompt tuning method too.\n",
    "\n",
    "#### Implementation Details\n",
    "\n",
    "The implementation is extremely straightforward, we run the augmented images through the model and then we softmax and average the prediction, the class with the highest probability is then returned as the final prediction.\n",
    "\n",
    "### Additional Improvements\n",
    "\n",
    "The two modes described above include some of the main differences between our implementation and the original one, however we have added a set of additional features and improvements to the model. Most of the variations implemented in the original version are present in our implementation too, such as the possibility to use different backbones for CLIP (RN50, ViT...) it's possible to tweak the learning rates, confidence selection, number of augmentations and test-time tuning steps.\n",
    "\n",
    "#### Split Context\n",
    "\n",
    "Unlike the original implementation we give the user the possibility to seamlessly specify where the prompt is placed in the context just by using the `[CLS]` token in the *base_prompt* specification. Split context is a technique that allows the user to specify the way the prompt is integrated with the context: wherever the `[CLS]` token is placed in the prompt, at that location the context is split in two parts, the first part is the prefix and the second part is the suffix. Split context allows the user to specify whether during the tuning phase the prompt should be treated as a single vector or as two separate vectors.\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "The original implementation was very fragmented and the prediction step as well as the test-time tuning step were not included in the main class. With our project we tried to be as self-contained as possible with modularity in mind. For this reason we have included a prediction function that takes care of the behaviour of the model at test time, according to the specified parameters.\n",
    "\n",
    "`predict`, `inference` and `forward` have very distinct roles:\n",
    "- **inference** is the function at the lower level, it includes the forward pass through CLIP image feature extractor and the custom text encoder\n",
    "- **forward** stand a step above and manages whether to run the confidence selection step and, according to the input parameters, how the inference should be performed\n",
    "- **predict** is the highest level function and it's the one that should be called by the user, it takes care of the alignment step, the ensemble step and the prompt tuning step, it's the most high-level function and it's all the user needs to perform classification on a sample.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODO ottimizzazioni di memoria e implementazione di base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EasyPromptLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is responsible for learning the prompt for the TPT model,\n",
    "    it takes the classnames and the base prompt and creates the prompt\n",
    "    for each class. The prompts get tokenized and embedded, the embeddings\n",
    "    of the base prompt are then used to create the context for each class.\n",
    "    It's possible to put the context in any part of the prompt\n",
    "    using the [CLS] token. It's also possible to choose wether to\n",
    "    split the context into separate learning parameters for the prefix and\n",
    "    suffix or to keep them together.\n",
    "\n",
    "    Parameters:\n",
    "    - device (str): the device to run the model\n",
    "    - clip (torch.nn.Module): the clip model\n",
    "    - base_prompt (str): the base prompt to use\n",
    "    - splt_ctx (bool): split the context or not\n",
    "    - classnames (list): the classnames to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        clip,\n",
    "        base_prompt=\"a photo of [CLS]\",\n",
    "        splt_ctx=False,\n",
    "        classnames=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.base_prompt = base_prompt\n",
    "        self.tkn_embedder = clip.token_embedding\n",
    "        self.tkn_embedder.requires_grad_(False)\n",
    "\n",
    "        self.split_ctx = splt_ctx\n",
    "\n",
    "        self.prepare_prompts(classnames)\n",
    "\n",
    "    def prepare_prompts(self, classnames):\n",
    "        \"\"\"\n",
    "        Prepares the prompts for the TPT model, this method tokenizes,\n",
    "        embeds and prepares the context for each class and the base prompt.\n",
    "\n",
    "        Parameters:\n",
    "        - classnames (list): the classnames to use\n",
    "        \"\"\"\n",
    "        print(\"[PromptLearner] Preparing prompts\")\n",
    "\n",
    "        self.classnames = classnames\n",
    "\n",
    "        # get number of classes\n",
    "        self.cls_num = len(self.classnames)\n",
    "\n",
    "        # get prompt text prefix and suffix\n",
    "        txt_prefix = self.base_prompt.split(\"[CLS]\")[0]\n",
    "        txt_suffix = self.base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # tokenize the prefix and suffix\n",
    "        tkn_prefix = tokenize(txt_prefix)\n",
    "        tkn_suffix = tokenize(txt_suffix)\n",
    "        tkn_pad = tokenize(\"\")\n",
    "        tkn_cls = tokenize(self.classnames)\n",
    "\n",
    "        # get the index of the last element of the prefix and suffix\n",
    "        idx = torch.arange(tkn_prefix.shape[1], 0, -1)\n",
    "        self.indp = torch.argmax((tkn_prefix == 0) * idx, 1, keepdim=True)\n",
    "        self.inds = torch.argmax((tkn_suffix == 0) * idx, 1, keepdim=True)\n",
    "\n",
    "        # token length for each class\n",
    "        self.indc = torch.argmax((tkn_cls == 0) * idx, 1, keepdim=True)\n",
    "\n",
    "        # get the prefix, suffix, SOT and EOT\n",
    "        self.tkn_sot = tkn_prefix[:, :1]\n",
    "        self.tkn_prefix = tkn_prefix[:, 1 : self.indp - 1]\n",
    "        self.tkn_suffix = tkn_suffix[:, 1 : self.inds - 1]\n",
    "        self.tkn_eot = tkn_suffix[:, self.inds - 1 : self.inds]\n",
    "        self.tkn_pad = tkn_pad[:, 2:]\n",
    "\n",
    "        # load segments to CUDA, be ready to be embedded\n",
    "        self.tkn_sot = self.tkn_sot.to(self.device)\n",
    "        self.tkn_prefix = self.tkn_prefix.to(self.device)\n",
    "        self.tkn_suffix = self.tkn_suffix.to(self.device)\n",
    "        self.tkn_eot = self.tkn_eot.to(self.device)\n",
    "        self.tkn_pad = self.tkn_pad.to(self.device)\n",
    "\n",
    "        self.tkn_cls = tkn_cls.to(self.device)\n",
    "\n",
    "        # gets the embeddings\n",
    "        with torch.no_grad():\n",
    "            self.emb_sot = self.tkn_embedder(self.tkn_sot)\n",
    "            self.emb_prefix = self.tkn_embedder(self.tkn_prefix)\n",
    "            self.emb_suffix = self.tkn_embedder(self.tkn_suffix)\n",
    "            self.emb_eot = self.tkn_embedder(self.tkn_eot)\n",
    "            self.emb_cls = self.tkn_embedder(self.tkn_cls)\n",
    "            self.emb_pad = self.tkn_embedder(self.tkn_pad)\n",
    "\n",
    "        # take out the embeddings of the class tokens (they are different lenghts)\n",
    "        self.all_cls = []\n",
    "        for i in range(self.cls_num):\n",
    "            self.all_cls.append(self.emb_cls[i][1 : self.indc[i] - 1])\n",
    "\n",
    "        # prepare the prompts, they are needed for text encoding\n",
    "        self.txt_prompts = [\n",
    "            self.base_prompt.replace(\"[CLS]\", cls) for cls in self.classnames\n",
    "        ]\n",
    "        self.tkn_prompts = tokenize(self.txt_prompts)\n",
    "\n",
    "        # set the inital context, this will be reused at every new inference\n",
    "        # this is the context that will be optimized\n",
    "\n",
    "        if self.split_ctx:\n",
    "            self.pre_init_state = self.emb_prefix.detach().clone()\n",
    "            self.suf_init_state = self.emb_suffix.detach().clone()\n",
    "            self.emb_prefix = nn.Parameter(self.emb_prefix)\n",
    "            self.emb_suffix = nn.Parameter(self.emb_suffix)\n",
    "            self.register_parameter(\"emb_prefix\", self.emb_prefix)\n",
    "            self.register_parameter(\"emb_suffix\", self.emb_suffix)\n",
    "        else:\n",
    "            self.ctx = torch.cat((self.emb_prefix, self.emb_suffix), dim=1)\n",
    "            self.ctx_init_state = self.ctx.detach().clone()\n",
    "            self.ctx = nn.Parameter(self.ctx)\n",
    "            self.register_parameter(\"ctx\", self.ctx)\n",
    "\n",
    "    def build_ctx(self):\n",
    "        \"\"\"\n",
    "        While the context will be optimized, the embedded classnames\n",
    "        must stay the same, this method builds the context for each class\n",
    "        at each forward pass, using the optimized context.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: the embedded prompt for each class\n",
    "        \"\"\"\n",
    "\n",
    "        prompts = []\n",
    "        for i in range(self.cls_num):\n",
    "\n",
    "            # get the size of the padding (length depends on the classname size)\n",
    "            pad_size = self.emb_cls.shape[1] - (\n",
    "                self.emb_prefix.shape[1]\n",
    "                + self.indc[i].item()\n",
    "                + self.emb_suffix.shape[1]\n",
    "            )\n",
    "\n",
    "            if self.split_ctx:\n",
    "                prefix = self.emb_prefix\n",
    "                suffix = self.emb_suffix\n",
    "            else:\n",
    "                prefix = self.ctx[:, : self.emb_prefix.shape[1]]\n",
    "                suffix = self.ctx[:, self.emb_prefix.shape[1] :]\n",
    "\n",
    "            # concatenates all elements to build the prompt\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.emb_sot,\n",
    "                    prefix,\n",
    "                    self.all_cls[i].unsqueeze(0),\n",
    "                    suffix,\n",
    "                    self.emb_eot,\n",
    "                    self.emb_pad[:, :pad_size],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self):\n",
    "        return self.build_ctx()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        This functions resets the context to the initial state, it\n",
    "        has to be run before each new inference to bring the context\n",
    "        to the initial state.\n",
    "        \"\"\"\n",
    "        if self.split_ctx:\n",
    "            self.emb_prefix.data.copy_(self.pre_init_state)  # to be optimized\n",
    "            self.emb_suffix.data.copy_(self.suf_init_state)  # to be optimized\n",
    "        else:\n",
    "            self.ctx.data.copy_(self.ctx_init_state)  # to be optimized\n",
    "\n",
    "\n",
    "class EasyTPT(EasyModel):\n",
    "    \"\"\"\n",
    "    This class is the main class for the TPT, it contains\n",
    "    the logic for running the TPT model in all its configurations,\n",
    "    as well as EasyPromptLearner, which is responsible for the\n",
    "    prompt learning.\n",
    "\n",
    "    Modes:\n",
    "    - Ensemble: in this mode the model won't preform tuning steps on the prompt,\n",
    "    instead it will run the inference on all the augmentations and take the prediction\n",
    "    that maximizes probability marginalized over the agumentations.\n",
    "    - Alignment: when align_steps > 0 the model will also perform align_steps tuning\n",
    "    steps on the image encoder in an effort to minimize the distance between the\n",
    "    embeddings of the augmentations.\n",
    "\n",
    "    Parameters:\n",
    "    - device (str): the device to run the model\n",
    "    - base_prompt (str): the base prompt to use\n",
    "    - arch (str): the architecture to use for CLIP\n",
    "    - splt_ctx (bool): split the context or not\n",
    "    - classnames (list): the classnames to use\n",
    "    - ensemble (bool): run TPT in ensemble mode\n",
    "    - ttt_steps (int): number of test time tuning steps\n",
    "    - lr (float): the learning rate\n",
    "    - align_steps (int): number of alignment steps\n",
    "    - confidence (float): confidence threshold for the confidence selection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        base_prompt=\"a photo of a [CLS]\",\n",
    "        arch=\"RN50\",\n",
    "        splt_ctx=False,\n",
    "        classnames=None,\n",
    "        ensemble=False,\n",
    "        ttt_steps=1,\n",
    "        lr=0.005,\n",
    "        align_steps=0,\n",
    "        confidence=0.10,\n",
    "    ):\n",
    "        super(EasyTPT, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        ###TODO: tobe parametrized\n",
    "        DOWNLOAD_ROOT = \"~/.cache/clip\"\n",
    "        ###\n",
    "\n",
    "        self.base_prompt = base_prompt\n",
    "        self.ttt_steps = ttt_steps\n",
    "        self.selected_idx = None\n",
    "        self.ensemble = ensemble\n",
    "        self.align_steps = align_steps\n",
    "        self.confidence = confidence\n",
    "\n",
    "        # Load clip\n",
    "        clip, self.preprocess = load(\n",
    "            arch, device=device, download_root=DOWNLOAD_ROOT, jit=False\n",
    "        )\n",
    "\n",
    "        if align_steps > 0:  # clip tuning must run in float\n",
    "            clip.float()\n",
    "\n",
    "        self.clip = clip\n",
    "        self.dtype = clip.dtype\n",
    "        self.image_encoder = clip.encode_image\n",
    "        # self.text_encoder = clip.encode_text\n",
    "\n",
    "        # freeze the parameters\n",
    "        for name, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        # create the prompt learner\n",
    "        self.prompt_learner = EasyPromptLearner(\n",
    "            device, clip, base_prompt, splt_ctx, classnames\n",
    "        )\n",
    "\n",
    "        # create optimizer and save the state\n",
    "        trainable_param = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"[EasyTPT TPT] Training parameter: {name}\")\n",
    "                trainable_param.append(param)\n",
    "        self.optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "        self.optim_state = deepcopy(self.optimizer.state_dict())\n",
    "\n",
    "        if align_steps > 0:\n",
    "            emb_trainable_param = []\n",
    "            # unfreeze the image encoder\n",
    "            for name, param in self.clip.visual.named_parameters():\n",
    "                # if parameter is not attnpoll\n",
    "                if \"attnpool\" not in name:\n",
    "                    param.requires_grad_(True)\n",
    "                    emb_trainable_param.append(param)\n",
    "                    print(f\"[EasyTPT Emb] Training parameter: {name}\")\n",
    "\n",
    "            self.emb_optimizer = torch.optim.AdamW(emb_trainable_param, 0.0001)\n",
    "            self.emb_optim_state = deepcopy(self.emb_optimizer.state_dict())\n",
    "            self.clip_init_state = deepcopy(self.clip.visual.state_dict())\n",
    "\n",
    "        if self.ensemble:\n",
    "            print(\"[EasyTPT] Running TPT in Ensemble mode\")\n",
    "\n",
    "        if self.align_steps > 0:\n",
    "            print(\"[EasyTPT] Running TPT with alignment\")\n",
    "\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(f\"[EasyTPT] Training parameter: {name}\")\n",
    "\n",
    "    def forward(self, x, top=-1):\n",
    "        \"\"\"\n",
    "        If x is a list of augmentations, run the confidence selection,\n",
    "        otherwise just run the inference\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor or list): the image(s) to run the inference. One\n",
    "        image for the final prediction or a list of augmentations for the\n",
    "        tuning steps.\n",
    "        - top (int): the top percentage of samples to select\n",
    "\n",
    "        Returns:\n",
    "        - logits (torch.Tensor): the logits of the inference\n",
    "        \"\"\"\n",
    "\n",
    "        if top == -1:\n",
    "            top = self.confidence\n",
    "\n",
    "        self.eval()\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x).to(self.device)\n",
    "            logits = self.inference(x)\n",
    "            if self.selected_idx is None:\n",
    "                logits, self.selected_idx = self.select_confident_samples(logits, top)\n",
    "            else:\n",
    "                logits = logits[self.selected_idx]\n",
    "        else:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = x.to(self.device)\n",
    "\n",
    "            logits = self.inference(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Basically CLIP's forward method, but with the custom\n",
    "        encoder to use our embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_feat = self.image_encoder(x)\n",
    "            image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        emb_prompts = self.prompt_learner()\n",
    "\n",
    "        txt_features = self.custom_encoder(emb_prompts, self.prompt_learner.tkn_prompts)\n",
    "        txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.clip.logit_scale.exp()\n",
    "        logits = logit_scale * image_feat @ txt_features.t()\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def predict(self, images, niter=1):\n",
    "\n",
    "        if self.ensemble:\n",
    "            with torch.no_grad():\n",
    "                out = self(images)\n",
    "                marginal_prob = F.softmax(out, dim=1).mean(0)\n",
    "                out_id = marginal_prob.argmax().item()\n",
    "        else:\n",
    "            if self.align_steps > 0:\n",
    "                self.align_embeddings(images)\n",
    "\n",
    "            for _ in range(niter):\n",
    "                out = self(images)\n",
    "                loss = self.avg_entropy(out)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                out = self(images[0])\n",
    "                out_id = out.argmax(1).item()\n",
    "                prediction = self.prompt_learner.classnames[out_id]\n",
    "\n",
    "        # return out_id, prediction\n",
    "        return out_id\n",
    "\n",
    "    def align_emb_loss(self, image_feat):\n",
    "\n",
    "        norm_feat = torch.nn.functional.normalize(image_feat, p=2, dim=1)\n",
    "\n",
    "        cos_sim = torch.mm(norm_feat, norm_feat.T)\n",
    "\n",
    "        # noself_mean = (cos_sim.sum() - torch.trace(cos_sim)) / (\n",
    "        #     cos_sim.numel() - cos_sim.shape[0]\n",
    "        # )\n",
    "        loss = 1 - cos_sim.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def align_embeddings(self, x):\n",
    "        \"\"\"\n",
    "        Aligns the embeddings of the image encoder\n",
    "        \"\"\"\n",
    "\n",
    "        self.forward(x)\n",
    "        self.clip.visual.train()\n",
    "        x = torch.stack(x).to(self.device)\n",
    "        selected_augs = torch.index_select(x, 0, self.selected_idx)\n",
    "        for _ in range(self.align_steps):\n",
    "            image_feat = self.clip.visual(selected_augs.type(self.dtype))\n",
    "            loss = self.align_emb_loss(image_feat)\n",
    "            self.emb_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # print(\"distance before: \", loss.item())\n",
    "            self.emb_optimizer.step()\n",
    "        image_feat = self.clip.visual(selected_augs.type(self.dtype))\n",
    "        loss = self.align_emb_loss(image_feat)\n",
    "        # print(\"distance after: \", loss.item())\n",
    "        self.clip.visual.eval()\n",
    "\n",
    "    def custom_encoder(self, prompts, tokenized_prompts):\n",
    "        \"\"\"\n",
    "        Custom clip text encoder, unlike the original clip encoder this one\n",
    "        takes the prompts embeddings from the prompt learner\n",
    "        \"\"\"\n",
    "        x = prompts + self.clip.positional_embedding\n",
    "        x = x.permute(1, 0, 2).type(self.dtype)  # NLD -> LND\n",
    "        x = self.clip.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip.ln_final(x).type(self.dtype)\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "            @ self.clip.text_projection\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the optimizer and the prompt learner to their initial state,\n",
    "        this has to be run before each new test\n",
    "        \"\"\"\n",
    "        self.optimizer.load_state_dict(deepcopy(self.optim_state))\n",
    "        self.prompt_learner.reset()\n",
    "        self.selected_idx = None\n",
    "\n",
    "        if self.align_steps > 0:\n",
    "            # print(\"[EasyTPT] Resetting embeddings optimizer\")\n",
    "            self.emb_optimizer.load_state_dict(deepcopy(self.emb_optim_state))\n",
    "            self.clip.visual.load_state_dict(deepcopy(self.clip_init_state))\n",
    "\n",
    "    def select_closest_samples(self, x, top):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = self.clip.visual(x.type(self.dtype))\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Compute cosine similarities\n",
    "            sims = F.cosine_similarity(feat[0].unsqueeze(0), feat[1:], dim=1)\n",
    "            vals, idxs = torch.topk(sims, int(sims.shape[0] * top))\n",
    "\n",
    "        return idxs\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Returns the optimizer\n",
    "\n",
    "        Returns:\n",
    "        - torch.optim: the optimizer\n",
    "        \"\"\"\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble\n",
    "This model allows to implement the core idea of the ensemble of multiple models. \n",
    "As already mentioned, the idea is to have different models trained with different techniques to cover the out of distribution data of one another, leading to a more robust ensemble.\n",
    "\n",
    "`EasyEnsemble` aggregates multiple `EasyModel` instances and allows to perform the ensemble of the models. This means that we can choose to use any of the previous models with any of the implemented backbones and configurations. This allows maximum flexibility in the choice of the models to use for the ensemble, allowing it to be tuned to fit the specific needs of the user being them memory constraints, computational power or the need for a more robust model.\n",
    "\n",
    "### Enseble of Multiple Models\n",
    "This is the core functionality of this module. It uses a similar idea to MEMO and TPT, which is to reduce the entropy of the marginal distribution obtained by augmenting the image.\n",
    "The difference is that we are feeding the augmentations to multiple models, each with different configurations, and then we are averaging the predictions of each model. This allows us to have a more robust model that can cover the out of distribution data of one another.\n",
    "\n",
    "The final output will be a weighted sum of the predictions, where the weights are computed based on the entropy of the distribution of each model. This allows the ensemble to give more importance to the models that are more confident in their predictions.\n",
    "However, to prevent some models to overpower others, we are also using a technique called \"temperature scaling\". This technique allows us to rescale the logits of each model by a certain factor, which is the temperature. This allows us to control the confidence of each model, and therefore the weight of each model in the final prediction.\n",
    "\n",
    "#### Implementation Details\n",
    "To compute the Ensemble output we follow these steps:\n",
    "1. Compute the logits for each model and augmentation   \n",
    "$outs = [model_0(augs), model_1(augs), ..., model_M(augs)]$\n",
    "2. Take the average logits for each model       \n",
    "$avg\\_outs = [mean(outs_0), mean(outs_1), ..., mean(outs_M)]$\n",
    "3. Rescale the logits by the temperature    \n",
    "$rescaled\\_outs = [avg\\_outs_0 / temp_0, avg\\_outs_1 / temp_1, ..., avg\\_outs_M / temp_M]$\n",
    "4. Compute the log softmax of the rescaled logits   \n",
    "$log\\_probs = log(softmax(rescaled\\_outs))$\n",
    "5. Compute the entropy of the distribution of each model as     \n",
    "$ent = [entropy(log\\_probs_0), entropy(log\\_probs_1), ..., entropy(log\\_probs_M)]$\n",
    "6. Compute a score for each model as    \n",
    "$ent\\_scale_i= (\\sum_j^M ent_j) / ent_i $    \n",
    "$scale_i= ent\\_scale_i / (\\sum_j^M ent\\_scale_j)$\n",
    "7. Compute the final prediction as the weighted sum of the distributions of each model  \n",
    "$final\\_pred = sum_i^M scale_i * log\\_probs_i$\n",
    "\n",
    "After obtaining the final prediction, we can compute the entropy of the distribution which we use to backpropagate through all the models, using the optimizer they provide.\n",
    "\n",
    "Finally, we can compute the final output by repeating the same steps and returning the class with the highest probability.\n",
    "\n",
    "### Single Models Test\n",
    "In the test arguments we can specify if we want to compute the results for the single models. This is useful to compare the performance of the ensemble with the performance of the single models.\n",
    "\n",
    "To compute the results for the single models we simply run the **inference** method for each model.\n",
    "\n",
    "### Simple Ensemble\n",
    "Similarly to the previous techniques, we also decided to implement the ensemble of augmentations idea using multiple models. \n",
    "This has the same advantages we have seen before, combining them with the advantage of having different models hopefully covering slightly different areas of the feature space.\n",
    "\n",
    "The implementation uses the same procedure of the ensemble of multiple models. However, we directly return the most probable class in the $final\\_pred$ tensor, without the need to backpropagate through the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble class. Implements an ensemble of models with entropy minimization.\n",
    "\n",
    "    Attributes:\n",
    "        models (list[EasyModel]): A list of models to be used in the ensemble.\n",
    "        temps (list): A list of temperature values corresponding to each model.\n",
    "        test_single_models (bool): Whether to test each individual model in addition to the ensemble.\n",
    "        simple_ensemble (bool): Whether to perform the entropy minimization step.\n",
    "        device (str): The device to be used for computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, models:list[EasyModel], temps, device=\"cuda\", test_single_models=False, simple_ensemble=False):\n",
    "        \"\"\"\n",
    "        Initializes an Ensemble object.\n",
    "\n",
    "        Args:\n",
    "            models (list[EasyModel]): A list of models to be used in the ensemble.\n",
    "            temps (list): A list of temperature values corresponding to each model.\n",
    "            device (str, optional): The device to be used for computation. Defaults to \"cuda\".\n",
    "            test_single_models (bool, optional): Whether to test each individual model in addition to the ensemble. Defaults to False.\n",
    "            simple_ensemble (bool, optional): Whether to perform the entropy minimization step. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.temps = temps\n",
    "        self.test_single_models = test_single_models\n",
    "        self.device = device\n",
    "        self.simple_ensemble = simple_ensemble\n",
    "\n",
    "    def entropy(self, logits):\n",
    "        \"\"\"\n",
    "        Computes the entropy of a set of logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits to compute the entropy of.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The entropy of the logits.\n",
    "        \"\"\"\n",
    "        return -(torch.exp(logits) * logits).sum(dim=-1)\n",
    "\n",
    "    def marginal_distribution(self, models_logits):\n",
    "        \"\"\"\n",
    "        Computes the marginal distribution of the ensemble.\n",
    "\n",
    "        Args:\n",
    "            models_logits (torch.Tensor): The logits of the models in the ensemble.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The marginal distribution of the ensemble.\n",
    "        \"\"\"\n",
    "        # average logits for each model\n",
    "        avg_models_distributions = torch.Tensor(models_logits.shape[0], models_logits.shape[2]).to(self.device)\n",
    "        for i, model_logits in enumerate(models_logits):\n",
    "            avg_outs = torch.logsumexp(model_logits, dim=0) - torch.log(torch.tensor(model_logits.shape[0]))\n",
    "            min_real = torch.finfo(avg_outs.dtype).min\n",
    "            avg_outs = torch.clamp(avg_outs, min=min_real)\n",
    "            avg_outs /= self.temps[i]\n",
    "            avg_models_distributions[i] = torch.log_softmax(avg_outs, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            entropies = torch.stack([self.entropy(logits) for logits in avg_models_distributions]).to(self.device)\n",
    "            sum_entropies = torch.sum(entropies, dim=0)\n",
    "            scale = torch.stack([sum_entropies/entopy for entopy in entropies]).to(self.device)\n",
    "            #normalize sum to 1\n",
    "            scale = scale / torch.sum(scale)\n",
    "\n",
    "        # print(\"\\t\\t[Ensemble] Entropies: \", entropies)\n",
    "        # print(\"\\t\\t[Ensemble] Scales: \", scale)\n",
    "\n",
    "        marginal_dist = torch.sum(torch.stack([scale[i].item() * avg_models_distributions[i] for i in range(len(avg_models_distributions))]), dim=0)\n",
    "\n",
    "        return marginal_dist\n",
    "\n",
    "    def get_models_outs(self, inputs, top=0.1):\n",
    "        \"\"\"\n",
    "        Computes the outputs of the models in the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The outputs of the models in the ensemble.\n",
    "        \"\"\"\n",
    "        model_outs = torch.stack([model(inputs[i], top).to(self.device) for i, model in enumerate(self.models)]).to(self.device)\n",
    "        return model_outs.to(self.device)\n",
    "\n",
    "    def get_models_predictions(self, inputs):\n",
    "        \"\"\"\n",
    "        Computes the predictions of the single models in the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of the predictions of the single models.\n",
    "        \"\"\"\n",
    "        models_pred = [model.predict(inputs[i]) for i, model in enumerate(self.models)]\n",
    "        return models_pred\n",
    "\n",
    "    def entropy_minimization(self, inputs, niter=1, top=0.1):\n",
    "        \"\"\"\n",
    "        Test time adaptation step. Minimizes the entropy of the ensemble's predictions.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            niter (int, optional): The number of iterations to perform. Defaults to 1.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        for i in range(niter):\n",
    "            outs = self.get_models_outs(inputs, top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "\n",
    "            loss = self.entropy(avg_logit)\n",
    "            loss.backward()\n",
    "            for model in self.models:\n",
    "                model.optimizer.step()\n",
    "                model.optimizer.zero_grad()\n",
    "\n",
    "    def forward(self, inputs, niter=1, top=0.1):\n",
    "        \"\"\"\n",
    "        Forward pass of the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            niter (int, optional): The number of iterations to perform. Defaults to 1.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            list, torch.Tensor, torch.Tensor: The predictions of the single models, the prediction of the ensemble without the entropy minimization step, and the prediction of the ensemble.\n",
    "        \"\"\"\n",
    "        # get models outputs\n",
    "        self.reset()\n",
    "        models_pred = self.get_models_predictions(inputs)\n",
    "\n",
    "        self.reset()\n",
    "        self.entropy_minimization(inputs, niter, top)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outs = self.get_models_outs([i[0] for i in inputs], top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "            prediction = torch.argmax(avg_logit, dim=0)\n",
    "\n",
    "        if self.simple_ensemble:\n",
    "            self.reset()    \n",
    "            for model in self.models: model.eval()\n",
    "            outs = self.get_models_outs(inputs, top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "            prediction_no_back = torch.argmax(avg_logit, dim=0)\n",
    "\n",
    "        return models_pred, prediction_no_back, prediction\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the models in the ensemble.\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MEMO\n",
    "We start with the baseline obtained with different weights initialization, since from when the MEMO paper was published, new weights were defined in torch.\n",
    "\n",
    "|          Test          | ImageNet-A | ImageNet-V2 |\n",
    "|:----------------------:|:----------:|:-----------:|\n",
    "|  baseline<br>default   |   16.67    |    69.95    |\n",
    "| baseline<br>weights-v1 |    0.03    |    63.13    |\n",
    "\n",
    "We see that the new weights give a lot better results in Imagenet-A, due to improved training. The results in Imagenet-V2 are still better than the previous weights.\n",
    "For all the test we used the RandomCrop Augmentation(cut), since we saw it performed generally better than Augmix augmentations for Imagenet-A\n",
    "\n",
    "### Results with weights-v1\n",
    "\n",
    "|              Test              | ImageNet-A | ImageNet-V2 | Delta A | Delta V2 | Iterations(it/s) |   Time (A - V2)   |\n",
    "|:------------------------------:|:----------:|:-----------:|:-------:|:--------:|:----------------:|:-----------------:|\n",
    "|            Baseline            |    0.03    |    63.13    |  0.00   |   0.00   |  22.73 - 22.73   |    5:30 - 7:20    |\n",
    "|              MEMO              |    1.92    |    64.47    |  1.89   |   1.34   |   8.19 - 8.06    |  15:15  - 20:40   |\n",
    "|     MEMO<br>topk selection     | **7.43 **  |    66.68    |  7.40   |   3.55   |   1.64 - 1.64    | 1:16:00 - 1:42:20 |\n",
    "|            Dropout             |    0.05    |    63.06    |  0.02   |  -0.07   |  14.28 - 13.16   |   8:45 - 12:40    |\n",
    "|   Dropout<br>topk selection    |    0.03    |    63.18    |  0.00   |   0.05   |   3.98 - 3.85    |   31:23 - 43:20   |\n",
    "|          Cut ensemble          |    2.00    |    65.90    |  1.97   |   2.77   |  14.71 - 13.88   |   8:10 - 12:00    |\n",
    "| Cut ensemble<br>topk selection |    6.37    |  **67.05**  |  6.34   |   3.92   |   4.00 - 3.87    |   31:45 - 43:00   |\n",
    "\n",
    "Here we see that dropout tests are not very effective since we start from a very low accuracy, so the dropout is not helping to achieve better accuracy. \n",
    "The MEMO tests are the best of all on Imagenet-A with accuracy of 7.43%, but the cut ensemble beats MEMO with accuracy of 67.05% on Imagenet-V2. \n",
    "The confidence selection helps deliver a better performance to both MEMO and cut ensemble because it chooses the samples in which is most confident between the many augmentations processed.\n",
    "\n",
    "### Results with default weights\n",
    "\n",
    "|              Test              | ImageNet-A | ImageNet-V2 | Delta A | Delta V2 | Iterations(it/s) | Time (A - V2) |\n",
    "|:------------------------------:|:----------:|:-----------:|:-------:|:--------:|:----------------:|:-------------:|\n",
    "|            Baseline            |   16.67    |    69.55    |  0.00   |   0.00   |  22.73 - 22.22   |  5.30 - 7:30  |\n",
    "|              MEMO              |   21.53    |    68.75    |  4.86   |   -0.8   |   9.86 - 10.05   | 12:40 - 16:35 |\n",
    "|     MEMO<br>topk selection     |   27.01    |    68.47    |  10.34  |  -1.08   |   3.52 - 3.45    | 35:30 - 48:22 |\n",
    "|            Dropout             |   17.29    |    69.40    |  0.62   |  -0.15   |  18.34 - 17.60   |  6:49 - 9:28  |\n",
    "|   Dropout<br>topk selection    |   18.44    |    69.69    |  1.77   |   0.14   |   4.26 - 3.96    | 29:20 - 42:02 |\n",
    "|          Cut ensemble          |   22.65    |  **70.29**  |  5.98   |   0.74   |  20.19 - 19.04   |  6:11 - 8:45  |\n",
    "| Cut ensemble<br>topk selection | **28.08**  |    69.77    |  11.41  |   0.22   |   5.50 - 5.17    | 22:43 - 32:15 |\n",
    "\n",
    "For the MEMO tests, we see that the confidence selection is much better than the default MEMO, and the dropout ensemble is better than the baseline, but it comes at a cost of being slower.\n",
    "The confidence selection is very effective in improving the performance of the MEMO and cut ensemble tests, as we can see from the results, and as explained above.\n",
    "\n",
    "The cut ensemble is the best of all in general, with a 28.08% accuracy on Imagenet-A and 69.77% on Imagenet-V2, even if on Imagenet-V2 the simple cut ensemble is better than every other one.\n",
    "\n",
    "In general the old weights are not as effective as the newer ones, since the newer ones are more refined.\n",
    "\n",
    "Note that all the time data for this section is obtained with the Sagemaker session provided to us. \n",
    "The times are based on an estimate given after how many iteration were calculated after 500 samples of the dataset.\n",
    "We calculated directly those for default weights, and put the same results for v1 weights, since the architecture is the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path_memo = f\"{RESULTS_PATH}/MEMO\"\n",
    "os.makedirs(results_path_memo, exist_ok=True)\n",
    "\n",
    "\n",
    "def MEMO_testing_step(test, arguments):\n",
    "    print(f\"Starting {test} evaluation...\")\n",
    "    device = arguments[\"memo\"][\"device\"]\n",
    "    mapping = arguments[\"memo\"][\"mapping\"]\n",
    "    prior_strength = arguments[\"memo\"][\"prior_strength\"]\n",
    "    lr = arguments[\"memo\"][\"lr\"]\n",
    "    weight_decay = arguments[\"memo\"][\"weight_decay\"]\n",
    "    opt = arguments[\"memo\"][\"opt\"]\n",
    "    niter = arguments[\"memo\"][\"niter\"]\n",
    "    top = arguments[\"memo\"][\"top\"]\n",
    "    ensemble = arguments[\"memo\"][\"ensemble\"]\n",
    "    dataset_root = arguments[\"dataset\"][\"dataset_root\"]\n",
    "    naugs = arguments[\"dataset\"][\"naug\"]\n",
    "    aug_type = arguments[\"dataset\"][\"aug_type\"]\n",
    "    weights = arguments[\"weights\"]\n",
    "\n",
    "    weights = ResNet50_Weights.IMAGENET1K_V1 if weights == 'v1' else ResNet50_Weights.DEFAULT\n",
    "    net = resnet50(weights=weights).to(device)\n",
    "    if \"drop\" in arguments.keys():\n",
    "        net.layer4.add_module('dropout', nn.Dropout(arguments[\"drop\"], inplace=True))\n",
    "\n",
    "    model = EasyMemo(net, device, mapping, prior_strength=prior_strength, top=top, ensemble=ensemble, lr=lr,\n",
    "                     weight_decay=weight_decay, opt=opt, niter=niter)\n",
    "    imageNet_A, imageNet_V2 = memo_get_datasets(aug_type, naugs, dataset_root)\n",
    "    dataset = imageNet_A if arguments['dataset']['imageNetA'] else imageNet_V2\n",
    "    \n",
    "    f = open(f\"{results_path_memo}/{test}.txt\", \"w\")\n",
    "    sys.stdout = f\n",
    "    \n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    index = np.random.permutation(range(len(dataset)))\n",
    "    # iterate = tqdm(index)\n",
    "    for i in index:\n",
    "        data = dataset[i]\n",
    "        image = data[\"img\"]\n",
    "        label = int(data[\"label\"])\n",
    "        \n",
    "        start = time.time()\n",
    "        prediction = model.predict(image)\n",
    "        model.reset()\n",
    "        end = time.time()\n",
    "        cnt += 1\n",
    "        total_time += end - start\n",
    "        avg_time = total_time / cnt\n",
    "        \n",
    "        correct += mapping[prediction] == label\n",
    "        # iterate.set_description(desc=f\"Current accuracy {(correct / cnt) * 100:.2f}\")\n",
    "    \n",
    "    print(f\"Final Accuracy: {(correct / cnt) * 100:.2f} over {cnt} samples\")\n",
    "    memo_acc = (correct / cnt) * 100\n",
    "    arguments[\"result\"] = round(memo_acc,3)\n",
    "    arguments[\"speed\"] = round(avg_time, 3)\n",
    "    json_dump = f\"{results_path_memo}/{test}.json\"\n",
    "    with open(json_dump, \"w\") as f:\n",
    "        json.dump(arguments, f)\n",
    "\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageNet_A, imageNet_V2 = memo_get_datasets('augmix', 1, DATASET_ROOT)\n",
    "mapping_a = [int(x) for x in imageNet_A.classnames.keys()]\n",
    "mapping_v2 = [int(x) for x in imageNet_V2.classnames.keys()]\n",
    "\n",
    "del imageNet_A, imageNet_V2\n",
    "\n",
    "memo_dict_tests = {\n",
    "    \"Baseline ImageNetA\": {\n",
    "        \"memo\": {\n",
    "            \"mapping\": mapping_a,\n",
    "            \"ensemble\": True,\n",
    "            \"top\": 1,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "        },\n",
    "        \"run\": baseline_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "        \"weights\": \"default\",\n",
    "    },\n",
    "    \"Baseline ImageNetV2\": {\n",
    "        \"memo\": {\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"ensemble\": True,\n",
    "            \"top\": 1,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "        },\n",
    "        \"run\": baseline_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "        \"weights\": \"default\",\n",
    "    },\n",
    "    \"Baseline ImageNetA ResNet50 weights V1\": {\n",
    "        \"memo\": {\n",
    "            \"mapping\": mapping_a,\n",
    "            \"ensemble\": True,\n",
    "            \"top\": 1,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "        },\n",
    "        \"weights\": \"v1\",\n",
    "        \"run\": baseline_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"Baseline ImageNetV2 ResNet50 weights V1\": {\n",
    "        \"memo\": {\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"ensemble\": True,\n",
    "            \"top\": 1,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "        },\n",
    "        \"weights\": \"v1\",\n",
    "        \"run\": baseline_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "    \"MEMO ImageNetA, without topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 1,\n",
    "            \"mapping\": mapping_a,\n",
    "            \"prior_strength\": 0.94\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "            \"naug\": augs_no_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"run\": memo_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"MEMO ImageNetV2, without topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 1,\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"prior_strength\": 0.94\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "            \"naug\": augs_no_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"run\": memo_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "    \"MEMO ImageNetA, with topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 0.1,\n",
    "            \"mapping\": mapping_a,\n",
    "            \"prior_strength\": 0.94\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "            \"naug\": augs_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"run\": memo_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"MEMO ImageNetV2, with topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 0.1,\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"prior_strength\": 0.94\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "            \"naug\": augs_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"run\": memo_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "    \"DROP ImageNetA, without topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 1,\n",
    "            \"mapping\": mapping_a,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "            \"naug\": augs_no_selection,\n",
    "            \"aug_type\": \"identity\",\n",
    "        },\n",
    "        \"drop\": 0.5,\n",
    "        \"run\": drop_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"DROP ImageNetV2, without topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 1,\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "            \"naug\": augs_no_selection,\n",
    "            \"aug_type\": \"identity\",\n",
    "        },\n",
    "        \"drop\": 0.5,\n",
    "        \"run\": drop_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "    \"DROP ImageNetA, with topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 0.1,\n",
    "            \"mapping\": mapping_a,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "            \"naug\": augs_selection,\n",
    "            \"aug_type\": \"identity\",\n",
    "        },\n",
    "        \"drop\": 0.5,\n",
    "        \"run\": drop_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"DROP ImageNetV2, with topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 0.1,\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "            \"naug\": augs_selection,\n",
    "            \"aug_type\": \"identity\",\n",
    "        },\n",
    "        \"drop\": 0.5,\n",
    "        \"run\": drop_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "    \"Cut ensemble ImageNetA, without topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 1,\n",
    "            \"mapping\": mapping_a,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "            \"naug\": augs_no_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"drop\": 0,\n",
    "        \"run\": ensemble_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"Cut ensemble ImageNetV2, without topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 1,\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "            \"naug\": augs_no_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"drop\": 0,\n",
    "        \"run\": ensemble_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "    \"Cut ensemble ImageNetA, with topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 0.1,\n",
    "            \"mapping\": mapping_a,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": True,\n",
    "            \"naug\": augs_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"drop\": 0,\n",
    "        \"run\": ensemble_tests and (DATASET_TO_TEST in [\"a\", \"both\"]),\n",
    "    },\n",
    "    \"Cut ensemble ImageNetV2, with topk selection\": {\n",
    "        \"memo\": {\n",
    "            \"top\": 0.1,\n",
    "            \"mapping\": mapping_v2,\n",
    "            \"ensemble\": True,\n",
    "        },\n",
    "        \"dataset\": {\n",
    "            \"imageNetA\": False,\n",
    "            \"naug\": augs_selection,\n",
    "            \"aug_type\": \"cut\",\n",
    "        },\n",
    "        \"drop\": 0,\n",
    "        \"run\": ensemble_tests and (DATASET_TO_TEST in [\"v2\", \"both\"]),\n",
    "    },\n",
    "}\n",
    "\n",
    "for t in memo_dict_tests:\n",
    "    if memo_dict_tests[t][\"run\"]:\n",
    "        arg = memo_base_test | memo_dict_tests[t]\n",
    "        arg['memo'] = memo_base_test['memo'] | memo_dict_tests[t]['memo']\n",
    "        arg['dataset'] = memo_base_test['dataset'] | memo_dict_tests[t]['dataset']\n",
    "        MEMO_testing_step(t, arg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TPT\n",
    "\n",
    "For consistency, and due to time and memory constraints, we decided to use only ResNet-50 as the backbone for CLIP. Our baselines include CLIP zero-shot and the TPT results from the original paper.\n",
    "\n",
    "|      Baselines      | ImageNet-A | ImageNet-V2 |\n",
    "|:-------------------:|:----------:|:-----------:|\n",
    "| CLIP <br> zero-shot |   21.91    |    51.2     |\n",
    "|     TPT (paper)     |   26.67    |    54.70    |\n",
    "\n",
    "For what concerns our TPT implementation, accuracy falls within half a percentage point of the original paper, this creates a solid baseline for our experiments in order to verify the effectiveness of the additional features we implemented. To make the comparison fair, we used the same number of augmentations and the same prompt as the original paper, same goes for learning rate and any hyperparameter that could be shared between the two models.\n",
    "\n",
    "|                   Test                   | ImageNet-A | ImageNet-V2 | Delta A  | Delta V2 | Iterations(it/s) |\n",
    "| :--------------------------------------: | :--------: | :---------: | :------: | :------: | :--------------: |\n",
    "|           CLIP <br> zero-shot            |    21.9    |    51.2     |   0.00   |   0.00   |   6.41 - 1.31    |\n",
    "|                TPT (ours)                |    26.4    |  **54.2**   |   4.5    |  **3**   |     1.52 - ~     |\n",
    "|               TPT ensemble               |    26.4    |    53.1     |   4.5    |   1.9    |   6.33 - 1.31    |\n",
    "| TPT ensemble + <br> confidence selection |  **32.2**  |    54.0     | **10.3** |   2.8    |   4.31 - 1.20    |\n",
    "|             TPT + alignment              |    17.1    |      ~      |   -4.8   |    ~     |     0.46 - ~     |\n",
    "\n",
    "### Experiments\n",
    "\n",
    "The actual parameters used in our experiments can be found in the **Parameters** section at the beginning. Our TPT baseline was run with a single prompt tuning step and the prompt \"A photo of a [CLS]\". \n",
    "\n",
    "The **ensemble** experiments were run without prompt tuning, the prediction consists in the class with the highest marginal probability over the set of augmentations, when using a single augmentation (the origin image) it's equivalent to a CLIP zero-shot prediction, this last modality was used for the CLIP zero-shot baseline. The first ensamble experiment used 8 augmentations meanwhile the second one used 64 augmentations with 10% confidence selection. \n",
    "\n",
    "The **alignment** experiments were run in many different modalities: not only different number of steps and learning rates, we also tried to freeze and unfreeze different parts of the visual encoder for both the RN50 and ViT backbones, the results were always disappointing. The specific alignment experiment presented in the table was run by tuning a RN50 backbone with the attention weights frozen.\n",
    "\n",
    "Results for **TPT + alignment** on ImageNet-V2 are not available as the model was just too large to fit in memory,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_path_tpt = f\"{RESULTS_PATH}/TPT\"\n",
    "os.makedirs(results_path_tpt, exist_ok=True)\n",
    "DATASET_ROOT = \"datasets\"\n",
    "\n",
    "\n",
    "for idx, settings in enumerate(tpt_tests):\n",
    "\n",
    "    test = tpt_base_test | settings\n",
    "\n",
    "    dataset_name = test[\"dataset\"]\n",
    "    test_name = test[\"name\"]\n",
    "    device = test[\"device\"]\n",
    "\n",
    "    BASE_PROMPT = test[\"base_prompt\"]\n",
    "    ARCH = test[\"arch\"]\n",
    "    SPLT_CTX = test[\"splt_ctx\"]\n",
    "    LR = test[\"lr\"]\n",
    "    AUGS = test[\"augs\"]\n",
    "    TTT_STEPS = test[\"ttt_steps\"]\n",
    "    ALIGN_STEPS = test[\"align_steps\"]\n",
    "    ENSEMBLE = test[\"ensemble\"]\n",
    "    TEST_STOP = test[\"test_stop\"]\n",
    "    CONFIDENCE = test[\"confidence\"]\n",
    "\n",
    "    f = open(f\"{results_path_tpt}/{test_name}.txt\", \"w\")\n",
    "    sys.stdout = f\n",
    "\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"[TEST] Running test {idx + 1} of {len(tpt_tests)}: {test_name} \\n{test}\")\n",
    "\n",
    "    print(f\"[TEST] loading datasets with {AUGS} augmentation...\")\n",
    "    datasetRoot = DATASET_ROOT\n",
    "    (\n",
    "        imageNetA,\n",
    "        _,\n",
    "        imageNetACustomNames,\n",
    "        imageNetAMap,\n",
    "        imageNetV2,\n",
    "        _,\n",
    "        imageNetV2CustomNames,\n",
    "        imageNetV2Map,\n",
    "    ) = tpt_get_datasets(datasetRoot, augs=AUGS, all_classes=False)\n",
    "    print(\"[TEST] datasets loaded.\")\n",
    "\n",
    "    if dataset_name == \"A\":\n",
    "        print(\"[TEST] using ImageNet A\")\n",
    "        dataset = imageNetA\n",
    "        classnames = imageNetACustomNames\n",
    "        id_mapping = imageNetAMap\n",
    "        del imageNetV2, imageNetV2CustomNames, imageNetV2Map\n",
    "    elif dataset_name == \"V2\":\n",
    "        print(\"[TEST] using ImageNet V2\")\n",
    "        dataset = imageNetV2\n",
    "        classnames = imageNetV2CustomNames\n",
    "        id_mapping = imageNetV2Map\n",
    "        del imageNetA, imageNetACustomNames, imageNetAMap\n",
    "\n",
    "    tpt = EasyTPT(\n",
    "        device,\n",
    "        base_prompt=BASE_PROMPT,\n",
    "        arch=ARCH,\n",
    "        splt_ctx=SPLT_CTX,\n",
    "        classnames=classnames,\n",
    "        ttt_steps=TTT_STEPS,\n",
    "        lr=LR,\n",
    "        align_steps=ALIGN_STEPS,\n",
    "        ensemble=ENSEMBLE,\n",
    "        confidence=CONFIDENCE,\n",
    "    )\n",
    "\n",
    "    cnt = 0\n",
    "    tpt_correct = 0\n",
    "    total_time = 0\n",
    "\n",
    "    idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "    SEED = 1\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for idx in idxs:\n",
    "        data = dataset[idx]\n",
    "        label = data[\"label\"]\n",
    "        imgs = data[\"img\"]\n",
    "        name = data[\"name\"]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        cnt += 1\n",
    "        with torch.no_grad():\n",
    "            tpt.reset()\n",
    "\n",
    "        out_id = tpt.predict(imgs)\n",
    "        tpt_predicted = classnames[out_id]\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        total_time += end - start\n",
    "        avg_time = total_time / cnt\n",
    "\n",
    "        if int(id_mapping[out_id]) == label:\n",
    "            emoji = \":D\"\n",
    "            tpt_correct += 1\n",
    "        else:\n",
    "            emoji = \":(\"\n",
    "\n",
    "        tpt_acc = tpt_correct / (cnt)\n",
    "\n",
    "        if cnt % VERBOSE == 0:\n",
    "            print(emoji)\n",
    "            print(f\"TPT Accuracy: {round(tpt_acc, 3)}\")\n",
    "            print(f\"GT: \\t{name}\\nTPT: \\t{tpt_predicted}\")\n",
    "            print(\n",
    "                f\"after {cnt} samples, average time {round(avg_time, 3)}s ({round(1 / avg_time, 3)}it/s)\\n\"\n",
    "            )\n",
    "\n",
    "        if cnt == TEST_STOP:\n",
    "            print(f\"[TEST] Early stopping at {cnt} samples\")\n",
    "            break\n",
    "\n",
    "    del tpt\n",
    "\n",
    "    print(f\"[TEST] Final TPT Accuracy: {round(tpt_acc, 3)} over {cnt} samples\")\n",
    "\n",
    "    test[\"result\"] = tpt_acc\n",
    "    test[\"speed\"] = round(avg_time, 3)\n",
    "    json_dump = f\"{results_path_tpt}/{test_name}.json\"\n",
    "    with open(json_dump, \"w\") as f:\n",
    "        json.dump(test, f)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "Here we present the results of the ensemble of multiple models. We used the MEMO and TPT models with different backbones to see if the ensemble could improve the performance of the models.\n",
    "\n",
    "|               Test               |                     Mode                     |               ImageNet-A               |                    Delta A                    |           ImageNet-V2           |            Delta V2             |\n",
    "| :------------------------------: | :------------------------------------------: | :------------------------------------: | :-------------------------------------------: | :-----------------------------: | :-----------------------------: |\n",
    "|      MEMO-RN50 + MEMO-RNXT       | Single Models<br>Ensemble<br>Simple Ensemble |     27.28 : 27.03<br>28.48<br>31.4     |        <br>1.20 : 1.45<br>4.12 : 4.37         | 68.48 : 69.88<br>69.63<br>70.96 | <br>1.15 : -0.25<br>2.48 ; 1.08 |\n",
    "|       MEMO-RN50 + TPT-RN50       | Single Models<br>Ensemble<br>Simple Ensemble |    27.33 : 27.99<br>30.49<br>34.97     |         <br>3.16 : 2.5<br>7.64 : 6.98         |           -<br>-<br>-           |           <br>-<br>-            |\n",
    "| MEMO-RN50 + MEMO-RNXT + TPT-RN50 | Single Models<br>Ensemble<br>Simple Ensemble | 27.1 : 27.17 : 28.37<br>27.74<br>**36.23** | <br>0.64 : 0.57 : -0.63<br>9.13 : 9.06 : 7.86 |           -<br>-<br>-           |           <br>-<br>-            |\n",
    "\n",
    "### Results with MEMO-RN50 + MEMO-RNXT\n",
    "\n",
    "These two models were trained with a similar loss and dataset, so we expect it to be the weaker of the ensembles.\n",
    "Nonetheless, the ensemble achieves a top1 accuracy higher than the two models finetuned separetely on ImageNet-A with an accuracy of 28.48% on Imagenet-A, while we suffer a bit on Imagenet-V2 with an accuracy of 69.63% on Imagenet-V2, lower than the resnext model finetuned using MEMO alone.\n",
    "\n",
    "As with the previous tests, we can see that the simpler strategy of aggregating the predictions of multiple augmentations as output to the model is the most effective strategy, with an accuracy of 31.4% on Imagenet-A and 70.96% on Imagenet-V2. This time we see an improvement across the board!\n",
    "\n",
    "The results are promising, and we can see that the ensemble of multiple models can improve the performance of the models even if they are trained with similar techniques.\n",
    "\n",
    "### Results with MEMO-RN50 + TPT-RN50\n",
    "\n",
    "This test was a lot more interesting since we are combining two models which are trained with two very different objectives.\n",
    "Thanks to this, we can see how the ensemble is able to improve on both models, probably because they are able to better cover the out of distribution data of one another.\n",
    "\n",
    "The ensemble of the two models is able to improve the performance of the models, with an accuracy of 30.49% on Imagenet-A. This is a significant improvement over the single models, and it shows that the ensemble can be a very effective strategy to improve the performance of the models. But the simple ensemble strategy is still the most effective, with an accuracy of 34.97% on Imagenet-A, outperforming the ensemble of the two models respectively by 7.64 and 6.98 percentage points.\n",
    "\n",
    "### Results with MEMO-RN50 + MEMO-RNXT + TPT-RN50\n",
    "Finally, we tried to combine three different models, each using different backbones or trained with a different training objective, to see if we could push even further the performance of the ensemble.\n",
    "\n",
    "The results confirmed our expectations, achieving an accuracy of 36.23% on Imagenet-A, the best result of all the tests. This shows that the ensemble of multiple models can be a very effective strategy to improve the performance of the models, even if they are trained with different techniques.\n",
    "\n",
    "A note is to be made with the results of the ensamble strategy with backpropagation, which was not able to outperform one of the models used in the ensemble, but by tinkering with the temperature values the results could probably be improved, as we did not had the time to test different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPT(device=\"cuda\", naug=64, arch=\"RN50\", A=True, ttt_steps=1, align_steps=0, top=0.1):\n",
    "    \"\"\"\n",
    "    Return the TPT model initialized with the given parameters\n",
    "\n",
    "    Args:\n",
    "        - device: device to use - default: cuda\n",
    "        - naug: number of augmentations to use - default: 64\n",
    "        - arch: backbone model to use - default: RN50\n",
    "        - A: use ImageNet A or ImageNet V2 - default: True\n",
    "        - ttt_steps: number of iterations for the TTT - default: 1\n",
    "        - align_steps: number of iterations for the alignment of the image embeddings - default: 0\n",
    "        - top: top confidence to select the augmented samples - default: 0.1\n",
    "    \"\"\"\n",
    "    # prepare TPT\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Using CPU this is no bueno\")\n",
    "    else:\n",
    "        print(\"Using GPU, brace yourself!\")\n",
    "\n",
    "    datasetRoot = \"datasets\"\n",
    "    imageNetA, _, imageNetACustomNames, imageNetAMap, imageNetV2, _, imageNetV2CustomNames, imageNetV2Map = tpt_get_datasets(datasetRoot, augs=naug, all_classes=False)\n",
    "    \n",
    "    if A:\n",
    "        dataset = imageNetA\n",
    "        classnames = imageNetACustomNames\n",
    "        mapping = imageNetAMap\n",
    "    else:\n",
    "        dataset = imageNetV2\n",
    "        classnames = imageNetV2CustomNames\n",
    "        mapping = imageNetV2Map\n",
    "    \n",
    "    tpt = EasyTPT(\n",
    "        base_prompt=\"A bad photo of a [CLS]\",\n",
    "        arch=arch,\n",
    "        classnames=classnames,\n",
    "        device=device,\n",
    "        ttt_steps=ttt_steps,\n",
    "        align_steps=align_steps,\n",
    "        confidence=top\n",
    "    )\n",
    "    \n",
    "    return tpt, dataset, mapping\n",
    "\n",
    "\n",
    "def memo(device=\"cuda\", prior_strength=0.94, naug=64, A=True, drop=0, ttt_steps=1, model=\"RN50\", top=0.1):\n",
    "    \"\"\"\n",
    "    Return the MEMO model initialized with the given parameters\n",
    "\n",
    "    Args:\n",
    "        - device: device to use - default: cuda\n",
    "        - prior_strength: strength of the prior for the BN layers - default: 0.94\n",
    "        - naug: number of augmentations to use - default: 64\n",
    "        - A: use ImageNet A or ImageNet V2 - default: True\n",
    "        - drop: dropout to use, by setting it to >0 the model will use the ensemble strategy - default: 0\n",
    "        - ttt_steps: number of iterations for the TTT - default: 1\n",
    "        - model: backbone model to use - default: RN50\n",
    "        - top: top confidence to select the augmented samples - default: 0.1\n",
    "    \"\"\"\n",
    "    load_model = {\n",
    "        \"RN50\": torch_models.resnet50,\n",
    "        \"RNXT\": torch_models.resnext50_32x4d\n",
    "    }\n",
    "    models_weights = {\n",
    "        \"RN50\": torch_models.ResNet50_Weights.DEFAULT,\n",
    "        \"RNXT\": torch_models.ResNeXt50_32X4D_Weights.DEFAULT\n",
    "    }\n",
    "    # prepare MEMO\n",
    "    imageNet_A, imageNet_V2 = memo_get_datasets(augmentation=('cut' if drop==0 else 'identity'), augs=naug)\n",
    "    dataset = imageNet_A if A else imageNet_V2\n",
    "\n",
    "    mapping = list(dataset.classnames.keys())\n",
    "    for i,id in enumerate(mapping):\n",
    "        mapping[i] = int(id)\n",
    "    \n",
    "    model = load_model[model](weights=models_weights[model])\n",
    "    model.layer4.add_module('dropout', nn.Dropout(drop))\n",
    "\n",
    "    memo = EasyMemo(\n",
    "        model, \n",
    "        device=device, \n",
    "        classes_mask=mapping, \n",
    "        prior_strength=prior_strength,\n",
    "        niter=ttt_steps,\n",
    "        ensemble=(drop>0),\n",
    "        top=top\n",
    "    )\n",
    "    \n",
    "    return memo, dataset, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, datasets, temps, mapping, names,\n",
    "         device=\"cuda\", niter=1, top=0.1,\n",
    "         simple_ensemble=False, testSingleModels=False, verbose=100):\n",
    "    \"\"\"\n",
    "    Test the ensemble model on the datasets\n",
    "\n",
    "    Args:\n",
    "        - models: list of models\n",
    "        - datasets: list of datasets\n",
    "        - temps: list of temperatures for the models to rescale the logits\n",
    "        - names: names of the models\n",
    "        - mapping: mapping of the classes outputted by the models to the original 1000 classes\n",
    "        - device: device to use\n",
    "        - niter: number of iterations for the TTT\n",
    "        - top: top confidence to select the augmented samples\n",
    "        - simple_ensemble: use the simple ensemble, only marginalizing the ditributions\n",
    "        - testSingleModels: test the single models inside the ensamble\n",
    "        - verbose: verbosity level\n",
    "    \n",
    "    Returns:\n",
    "        - results: dictionary with the results of the test\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    correct_no_back = 0\n",
    "    correctSingle = [0] * len(models)\n",
    "    cnt = 0\n",
    "\n",
    "    class_names = get_classes_names()\n",
    "\n",
    "    # shuffle the data\n",
    "    indx = np.random.permutation(range(len(datasets[0])))\n",
    "\n",
    "    model = Ensemble(models, temps=temps,\n",
    "                     device=device, test_single_models=testSingleModels,\n",
    "                     simple_ensemble=simple_ensemble)\n",
    "    print(\"Ensemble model created starting TTA, samples:\", len(indx))\n",
    "    for i in indx:\n",
    "        cnt += 1\n",
    "        data = [datasets[j][i][\"img\"] for j in range(len(datasets))]\n",
    "\n",
    "        labels = [datasets[j][i][\"label\"] for j in range(len(datasets))]\n",
    "        # check if the labels are the same\n",
    "        assert all(x == labels[0] for x in labels), \"Labels are not the same\"\n",
    "        label = labels[0]\n",
    "        name = datasets[0][i][\"name\"]\n",
    "\n",
    "        if(cnt%verbose==0): print(f\"Tested Samples: {cnt} / {len(datasets[0])} - current sample: {name}\")\n",
    "\n",
    "        models_out, pred_no_back, prediction = model(data, niter=niter, top=top)\n",
    "        models_out = [int(mapping[model_out]) for model_out in models_out]\n",
    "        prediction = int(mapping[prediction])\n",
    "\n",
    "        if testSingleModels:\n",
    "            for i, model_out in enumerate(models_out):\n",
    "                if label == model_out:\n",
    "                    correctSingle[i] += 1\n",
    "\n",
    "                if(cnt%verbose==0): \n",
    "                    print(\n",
    "                    f\"\\t{names[i]} model accuracy: {correctSingle[i]}/{cnt} - predicted class {model_out}: {class_names[model_out]} - tested: {cnt} / {len(datasets[0])}\")\n",
    "\n",
    "        if simple_ensemble:\n",
    "            pred_no_back = int(mapping[pred_no_back])\n",
    "            if label == pred_no_back:\n",
    "                correct_no_back += 1\n",
    "            if(cnt%verbose==0): \n",
    "                print(\n",
    "                f\"\\tSimple Ens accuracy: {correct_no_back}/{cnt} - predicted class {pred_no_back}: {class_names[pred_no_back]} - tested: {cnt} / {len(datasets[0])}\")\n",
    "\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "\n",
    "        if(cnt%verbose==0): \n",
    "            print(\n",
    "            f\"\\tEnsemble accuracy: {correct}/{cnt} - predicted class {prediction}: {class_names[prediction]} - tested: {cnt} / {len(datasets[0])}\")\n",
    "    \n",
    "    results = {\n",
    "        \"Ensemble\": correct / len(datasets[0]) * 100,\n",
    "        \"Simple Ens\": correct_no_back / len(datasets[0]) * 100,\n",
    "        \"Single Models\": [correctSingle[i] / len(datasets[0]) * 100 for i in range(len(models))]\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# expand args\n",
    "def runTest(models_type, args, temps, names, naug=64, niter=1, top=0.1, device=\"cuda\", simple_ensemble=False,\n",
    "            testSingleModels=False, imageNetA=True, verbose=100):\n",
    "    \"\"\"\n",
    "    Run the test on Ensemble model with the given arguments\n",
    "\n",
    "    Args:\n",
    "        - models_type: list of the models type to use\n",
    "        - args: list of the arguments for the models\n",
    "        - temps: list of temperatures for the models to rescale the logits\n",
    "        - names: names of the models\n",
    "        - naug: number of augmentations to use\n",
    "        - niter: number of iterations for the TTT\n",
    "        - top: top confidence to select the augmented samples\n",
    "        - device: device to use\n",
    "        - simple_ensemble: use the simple ensemble, only marginalizing the ditributions\n",
    "        - testSingleModels: test the single models inside the ensamble\n",
    "        - imageNetA: use ImageNet A or ImageNet V2\n",
    "        - verbose: verbosity level\n",
    "\n",
    "    Returns:\n",
    "        - results: dictionary with the results of the test\n",
    "    \"\"\"\n",
    "\n",
    "    models = []\n",
    "    datasets = []\n",
    "    mapping = None\n",
    "    load_model = {\n",
    "        \"memo\": memo,\n",
    "        \"tpt\": TPT\n",
    "    }\n",
    "    for i in range(len(models_type)):\n",
    "        model, data, mapping = load_model[models_type[i]](**args[i], A=imageNetA, top=top, naug=naug)\n",
    "        models.append(model)\n",
    "        datasets.append(data)\n",
    "\n",
    "    result = test(models=models, datasets=datasets, temps=temps, mapping=mapping, names=names,\n",
    "         device=device, niter=niter, top=top, simple_ensemble=simple_ensemble, testSingleModels=testSingleModels,\n",
    "         verbose=verbose)\n",
    "\n",
    "    for model in models:\n",
    "        del model\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path_tta = f\"{RESULTS_PATH}/Ensemble\"\n",
    "os.makedirs(results_path_tta, exist_ok=True)\n",
    "\n",
    "for ENStest in ENSTests:\n",
    "\n",
    "        f = open(f\"{results_path_tta}/{ENStest}.txt\", \"w\")\n",
    "        sys.stdout = f\n",
    "\n",
    "        print(f\"Running test: {ENStest}\")\n",
    "        test_params = ENSTests[ENStest]\n",
    "        test_params[\"verbose\"] = VERBOSE\n",
    "        result = runTest(**test_params)\n",
    "\n",
    "        #remove verbose from the results\n",
    "        test_params.pop(\"verbose\")\n",
    "        test_params[\"result\"] = result\n",
    "        json.dump(f\"{results_path_tpt}/{ENStest}.json\", test_params)\n",
    "        \n",
    "        print(\"\\tFinal Results:\")\n",
    "        for key in result:\n",
    "            print(f\"\\t\\t{key}: {result[key]}\")\n",
    "\n",
    "        print(\"\\n-------------------\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The entire project was created with flexibility in mind by using a modular structure that allows the user to easily switch between different models and backbones.\n",
    "\n",
    "In conclusion, for both MEMO and TPT we have been able to match the baselines set by the original papers and, in some cases, improve on them. TPT in ensemble mode surpassed the prompt tuning method by almost 6 points and the CLIP baseline by 10.3. MEMO had similar improvements, with the ensemble method outperforming the MEMO reference by 6.55 points on ImageNet-A and 1.54 on ImageNet-V2. \n",
    " \n",
    "What took us the most by surprise was the effectiveness of the ensemble of augmentations, which was able to outperform the reference techniques without the need of backpropagate through the network. The best results were obtained by ensebling RN50 with RNXT and CLIP (RN50) via the simple ensemble strategy which achieved an accuracy of 36.23% on ImageNet-A, 14.33 points higher than the CLIP zero-shot baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Laurence Bonat](https://github.com/blauer4)\n",
    "- [Davide Cavicchini](https://github.com/DavidC001)\n",
    "- [Lorenzo Orsingher](https://github.com/lorenzoorsingher)\n",
    "- [**Github repo**](https://github.com/DavidC001/MEMO-TPT-DL2024)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
