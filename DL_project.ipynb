{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.v2 import AugMix\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from clip import load, tokenize\n",
    "import cv2\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "imageNetA = True\n",
    "naug = 64\n",
    "top = 0.1\n",
    "niter = 1\n",
    "\n",
    "#set the seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class names mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_classes = [\"tench\", \"goldfish\", \"great white shark\", \"tiger shark\", \"hammerhead shark\", \"electric ray\", \"stingray\", \"rooster\", \"hen\", \"ostrich\", \"brambling\", \"goldfinch\", \"house finch\", \"junco\", \"indigo bunting\", \"American robin\", \"bulbul\", \"jay\", \"magpie\", \"chickadee\", \"American dipper\", \"kite (bird of prey)\", \"bald eagle\", \"vulture\", \"great grey owl\", \"fire salamander\", \"smooth newt\", \"newt\", \"spotted salamander\", \"axolotl\", \"American bullfrog\", \"tree frog\", \"tailed frog\", \"loggerhead sea turtle\", \"leatherback sea turtle\", \"mud turtle\", \"terrapin\", \"box turtle\", \"banded gecko\", \"green iguana\", \"Carolina anole\", \"desert grassland whiptail lizard\", \"agama\", \"frilled-necked lizard\", \"alligator lizard\", \"Gila monster\", \"European green lizard\", \"chameleon\", \"Komodo dragon\", \"Nile crocodile\", \"American alligator\", \"triceratops\", \"worm snake\", \"ring-necked snake\", \"eastern hog-nosed snake\", \"smooth green snake\", \"kingsnake\", \"garter snake\", \"water snake\", \"vine snake\", \"night snake\", \"boa constrictor\", \"African rock python\", \"Indian cobra\", \"green mamba\", \"sea snake\", \"Saharan horned viper\", \"eastern diamondback rattlesnake\", \"sidewinder rattlesnake\", \"trilobite\", \"harvestman\", \"scorpion\", \"yellow garden spider\", \"barn spider\", \"European garden spider\", \"southern black widow\", \"tarantula\", \"wolf spider\", \"tick\", \"centipede\", \"black grouse\", \"ptarmigan\", \"ruffed grouse\", \"prairie grouse\", \"peafowl\", \"quail\", \"partridge\", \"african grey parrot\", \"macaw\", \"sulphur-crested cockatoo\", \"lorikeet\", \"coucal\", \"bee eater\", \"hornbill\", \"hummingbird\", \"jacamar\", \"toucan\", \"duck\", \"red-breasted merganser\", \"goose\", \"black swan\", \"tusker\", \"echidna\", \"platypus\", \"wallaby\", \"koala\", \"wombat\", \"jellyfish\", \"sea anemone\", \"brain coral\", \"flatworm\", \"nematode\", \"conch\", \"snail\", \"slug\", \"sea slug\", \"chiton\", \"chambered nautilus\", \"Dungeness crab\", \"rock crab\", \"fiddler crab\", \"red king crab\", \"American lobster\", \"spiny lobster\", \"crayfish\", \"hermit crab\", \"isopod\", \"white stork\", \"black stork\", \"spoonbill\", \"flamingo\", \"little blue heron\", \"great egret\", \"bittern bird\", \"crane bird\", \"limpkin\", \"common gallinule\", \"American coot\", \"bustard\", \"ruddy turnstone\", \"dunlin\", \"common redshank\", \"dowitcher\", \"oystercatcher\", \"pelican\", \"king penguin\", \"albatross\", \"grey whale\", \"killer whale\", \"dugong\", \"sea lion\", \"Chihuahua\", \"Japanese Chin\", \"Maltese\", \"Pekingese\", \"Shih Tzu\", \"King Charles Spaniel\", \"Papillon\", \"toy terrier\", \"Rhodesian Ridgeback\", \"Afghan Hound\", \"Basset Hound\", \"Beagle\", \"Bloodhound\", \"Bluetick Coonhound\", \"Black and Tan Coonhound\", \"Treeing Walker Coonhound\", \"English foxhound\", \"Redbone Coonhound\", \"borzoi\", \"Irish Wolfhound\", \"Italian Greyhound\", \"Whippet\", \"Ibizan Hound\", \"Norwegian Elkhound\", \"Otterhound\", \"Saluki\", \"Scottish Deerhound\", \"Weimaraner\", \"Staffordshire Bull Terrier\", \"American Staffordshire Terrier\", \"Bedlington Terrier\", \"Border Terrier\", \"Kerry Blue Terrier\", \"Irish Terrier\", \"Norfolk Terrier\", \"Norwich Terrier\", \"Yorkshire Terrier\", \"Wire Fox Terrier\", \"Lakeland Terrier\", \"Sealyham Terrier\", \"Airedale Terrier\", \"Cairn Terrier\", \"Australian Terrier\", \"Dandie Dinmont Terrier\", \"Boston Terrier\", \"Miniature Schnauzer\", \"Giant Schnauzer\", \"Standard Schnauzer\", \"Scottish Terrier\", \"Tibetan Terrier\", \"Australian Silky Terrier\", \"Soft-coated Wheaten Terrier\", \"West Highland White Terrier\", \"Lhasa Apso\", \"Flat-Coated Retriever\", \"Curly-coated Retriever\", \"Golden Retriever\", \"Labrador Retriever\", \"Chesapeake Bay Retriever\", \"German Shorthaired Pointer\", \"Vizsla\", \"English Setter\", \"Irish Setter\", \"Gordon Setter\", \"Brittany dog\", \"Clumber Spaniel\", \"English Springer Spaniel\", \"Welsh Springer Spaniel\", \"Cocker Spaniel\", \"Sussex Spaniel\", \"Irish Water Spaniel\", \"Kuvasz\", \"Schipperke\", \"Groenendael dog\", \"Malinois\", \"Briard\", \"Australian Kelpie\", \"Komondor\", \"Old English Sheepdog\", \"Shetland Sheepdog\", \"collie\", \"Border Collie\", \"Bouvier des Flandres dog\", \"Rottweiler\", \"German Shepherd Dog\", \"Dobermann\", \"Miniature Pinscher\", \"Greater Swiss Mountain Dog\", \"Bernese Mountain Dog\", \"Appenzeller Sennenhund\", \"Entlebucher Sennenhund\", \"Boxer\", \"Bullmastiff\", \"Tibetan Mastiff\", \"French Bulldog\", \"Great Dane\", \"St. Bernard\", \"husky\", \"Alaskan Malamute\", \"Siberian Husky\", \"Dalmatian\", \"Affenpinscher\", \"Basenji\", \"pug\", \"Leonberger\", \"Newfoundland dog\", \"Great Pyrenees dog\", \"Samoyed\", \"Pomeranian\", \"Chow Chow\", \"Keeshond\", \"brussels griffon\", \"Pembroke Welsh Corgi\", \"Cardigan Welsh Corgi\", \"Toy Poodle\", \"Miniature Poodle\", \"Standard Poodle\", \"Mexican hairless dog (xoloitzcuintli)\", \"grey wolf\", \"Alaskan tundra wolf\", \"red wolf or maned wolf\", \"coyote\", \"dingo\", \"dhole\", \"African wild dog\", \"hyena\", \"red fox\", \"kit fox\", \"Arctic fox\", \"grey fox\", \"tabby cat\", \"tiger cat\", \"Persian cat\", \"Siamese cat\", \"Egyptian Mau\", \"cougar\", \"lynx\", \"leopard\", \"snow leopard\", \"jaguar\", \"lion\", \"tiger\", \"cheetah\", \"brown bear\", \"American black bear\", \"polar bear\", \"sloth bear\", \"mongoose\", \"meerkat\", \"tiger beetle\", \"ladybug\", \"ground beetle\", \"longhorn beetle\", \"leaf beetle\", \"dung beetle\", \"rhinoceros beetle\", \"weevil\", \"fly\", \"bee\", \"ant\", \"grasshopper\", \"cricket insect\", \"stick insect\", \"cockroach\", \"praying mantis\", \"cicada\", \"leafhopper\", \"lacewing\", \"dragonfly\", \"damselfly\", \"red admiral butterfly\", \"ringlet butterfly\", \"monarch butterfly\", \"small white butterfly\", \"sulphur butterfly\", \"gossamer-winged butterfly\", \"starfish\", \"sea urchin\", \"sea cucumber\", \"cottontail rabbit\", \"hare\", \"Angora rabbit\", \"hamster\", \"porcupine\", \"fox squirrel\", \"marmot\", \"beaver\", \"guinea pig\", \"common sorrel horse\", \"zebra\", \"pig\", \"wild boar\", \"warthog\", \"hippopotamus\", \"ox\", \"water buffalo\", \"bison\", \"ram (adult male sheep)\", \"bighorn sheep\", \"Alpine ibex\", \"hartebeest\", \"impala (antelope)\", \"gazelle\", \"arabian camel\", \"llama\", \"weasel\", \"mink\", \"European polecat\", \"black-footed ferret\", \"otter\", \"skunk\", \"badger\", \"armadillo\", \"three-toed sloth\", \"orangutan\", \"gorilla\", \"chimpanzee\", \"gibbon\", \"siamang\", \"guenon\", \"patas monkey\", \"baboon\", \"macaque\", \"langur\", \"black-and-white colobus\", \"proboscis monkey\", \"marmoset\", \"white-headed capuchin\", \"howler monkey\", \"titi monkey\", \"Geoffroy's spider monkey\", \"common squirrel monkey\", \"ring-tailed lemur\", \"indri\", \"Asian elephant\", \"African bush elephant\", \"red panda\", \"giant panda\", \"snoek fish\", \"eel\", \"silver salmon\", \"rock beauty fish\", \"clownfish\", \"sturgeon\", \"gar fish\", \"lionfish\", \"pufferfish\", \"abacus\", \"abaya\", \"academic gown\", \"accordion\", \"acoustic guitar\", \"aircraft carrier\", \"airliner\", \"airship\", \"altar\", \"ambulance\", \"amphibious vehicle\", \"analog clock\", \"apiary\", \"apron\", \"trash can\", \"assault rifle\", \"backpack\", \"bakery\", \"balance beam\", \"balloon\", \"ballpoint pen\", \"Band-Aid\", \"banjo\", \"baluster / handrail\", \"barbell\", \"barber chair\", \"barbershop\", \"barn\", \"barometer\", \"barrel\", \"wheelbarrow\", \"baseball\", \"basketball\", \"bassinet\", \"bassoon\", \"swimming cap\", \"bath towel\", \"bathtub\", \"station wagon\", \"lighthouse\", \"beaker\", \"military hat (bearskin or shako)\", \"beer bottle\", \"beer glass\", \"bell tower\", \"baby bib\", \"tandem bicycle\", \"bikini\", \"ring binder\", \"binoculars\", \"birdhouse\", \"boathouse\", \"bobsleigh\", \"bolo tie\", \"poke bonnet\", \"bookcase\", \"bookstore\", \"bottle cap\", \"hunting bow\", \"bow tie\", \"brass memorial plaque\", \"bra\", \"breakwater\", \"breastplate\", \"broom\", \"bucket\", \"buckle\", \"bulletproof vest\", \"high-speed train\", \"butcher shop\", \"taxicab\", \"cauldron\", \"candle\", \"cannon\", \"canoe\", \"can opener\", \"cardigan\", \"car mirror\", \"carousel\", \"tool kit\", \"cardboard box / carton\", \"car wheel\", \"automated teller machine\", \"cassette\", \"cassette player\", \"castle\", \"catamaran\", \"CD player\", \"cello\", \"mobile phone\", \"chain\", \"chain-link fence\", \"chain mail\", \"chainsaw\", \"storage chest\", \"chiffonier\", \"bell or wind chime\", \"china cabinet\", \"Christmas stocking\", \"church\", \"movie theater\", \"cleaver\", \"cliff dwelling\", \"cloak\", \"clogs\", \"cocktail shaker\", \"coffee mug\", \"coffeemaker\", \"spiral or coil\", \"combination lock\", \"computer keyboard\", \"candy store\", \"container ship\", \"convertible\", \"corkscrew\", \"cornet\", \"cowboy boot\", \"cowboy hat\", \"cradle\", \"construction crane\", \"crash helmet\", \"crate\", \"infant bed\", \"Crock Pot\", \"croquet ball\", \"crutch\", \"cuirass\", \"dam\", \"desk\", \"desktop computer\", \"rotary dial telephone\", \"diaper\", \"digital clock\", \"digital watch\", \"dining table\", \"dishcloth\", \"dishwasher\", \"disc brake\", \"dock\", \"dog sled\", \"dome\", \"doormat\", \"drilling rig\", \"drum\", \"drumstick\", \"dumbbell\", \"Dutch oven\", \"electric fan\", \"electric guitar\", \"electric locomotive\", \"entertainment center\", \"envelope\", \"espresso machine\", \"face powder\", \"feather boa\", \"filing cabinet\", \"fireboat\", \"fire truck\", \"fire screen\", \"flagpole\", \"flute\", \"folding chair\", \"football helmet\", \"forklift\", \"fountain\", \"fountain pen\", \"four-poster bed\", \"freight car\", \"French horn\", \"frying pan\", \"fur coat\", \"garbage truck\", \"gas mask or respirator\", \"gas pump\", \"goblet\", \"go-kart\", \"golf ball\", \"golf cart\", \"gondola\", \"gong\", \"gown\", \"grand piano\", \"greenhouse\", \"radiator grille\", \"grocery store\", \"guillotine\", \"hair clip\", \"hair spray\", \"half-track\", \"hammer\", \"hamper\", \"hair dryer\", \"hand-held computer\", \"handkerchief\", \"hard disk drive\", \"harmonica\", \"harp\", \"combine harvester\", \"hatchet\", \"holster\", \"home theater\", \"honeycomb\", \"hook\", \"hoop skirt\", \"gymnastic horizontal bar\", \"horse-drawn vehicle\", \"hourglass\", \"iPod\", \"clothes iron\", \"carved pumpkin\", \"jeans\", \"jeep\", \"T-shirt\", \"jigsaw puzzle\", \"rickshaw\", \"joystick\", \"kimono\", \"knee pad\", \"knot\", \"lab coat\", \"ladle\", \"lampshade\", \"laptop computer\", \"lawn mower\", \"lens cap\", \"letter opener\", \"library\", \"lifeboat\", \"lighter\", \"limousine\", \"ocean liner\", \"lipstick\", \"slip-on shoe\", \"lotion\", \"music speaker\", \"loupe magnifying glass\", \"sawmill\", \"magnetic compass\", \"messenger bag\", \"mailbox\", \"tights\", \"one-piece bathing suit\", \"manhole cover\", \"maraca\", \"marimba\", \"mask\", \"matchstick\", \"maypole\", \"maze\", \"measuring cup\", \"medicine cabinet\", \"megalith\", \"microphone\", \"microwave oven\", \"military uniform\", \"milk can\", \"minibus\", \"miniskirt\", \"minivan\", \"missile\", \"mitten\", \"mixing bowl\", \"mobile home\", \"ford model t\", \"modem\", \"monastery\", \"monitor\", \"moped\", \"mortar and pestle\", \"graduation cap\", \"mosque\", \"mosquito net\", \"vespa\", \"mountain bike\", \"tent\", \"computer mouse\", \"mousetrap\", \"moving van\", \"muzzle\", \"metal nail\", \"neck brace\", \"necklace\", \"baby pacifier\", \"notebook computer\", \"obelisk\", \"oboe\", \"ocarina\", \"odometer\", \"oil filter\", \"pipe organ\", \"oscilloscope\", \"overskirt\", \"bullock cart\", \"oxygen mask\", \"product packet / packaging\", \"paddle\", \"paddle wheel\", \"padlock\", \"paintbrush\", \"pajamas\", \"palace\", \"pan flute\", \"paper towel\", \"parachute\", \"parallel bars\", \"park bench\", \"parking meter\", \"railroad car\", \"patio\", \"payphone\", \"pedestal\", \"pencil case\", \"pencil sharpener\", \"perfume\", \"Petri dish\", \"photocopier\", \"plectrum\", \"Pickelhaube\", \"picket fence\", \"pickup truck\", \"pier\", \"piggy bank\", \"pill bottle\", \"pillow\", \"ping-pong ball\", \"pinwheel\", \"pirate ship\", \"drink pitcher\", \"block plane\", \"planetarium\", \"plastic bag\", \"plate rack\", \"farm plow\", \"plunger\", \"Polaroid camera\", \"pole\", \"police van\", \"poncho\", \"pool table\", \"soda bottle\", \"plant pot\", \"potter's wheel\", \"power drill\", \"prayer rug\", \"printer\", \"prison\", \"missile\", \"projector\", \"hockey puck\", \"punching bag\", \"purse\", \"quill\", \"quilt\", \"race car\", \"racket\", \"radiator\", \"radio\", \"radio telescope\", \"rain barrel\", \"recreational vehicle\", \"fishing casting reel\", \"reflex camera\", \"refrigerator\", \"remote control\", \"restaurant\", \"revolver\", \"rifle\", \"rocking chair\", \"rotisserie\", \"eraser\", \"rugby ball\", \"ruler measuring stick\", \"sneaker\", \"safe\", \"safety pin\", \"salt shaker\", \"sandal\", \"sarong\", \"saxophone\", \"scabbard\", \"weighing scale\", \"school bus\", \"schooner\", \"scoreboard\", \"CRT monitor\", \"screw\", \"screwdriver\", \"seat belt\", \"sewing machine\", \"shield\", \"shoe store\", \"shoji screen / room divider\", \"shopping basket\", \"shopping cart\", \"shovel\", \"shower cap\", \"shower curtain\", \"ski\", \"balaclava ski mask\", \"sleeping bag\", \"slide rule\", \"sliding door\", \"slot machine\", \"snorkel\", \"snowmobile\", \"snowplow\", \"soap dispenser\", \"soccer ball\", \"sock\", \"solar thermal collector\", \"sombrero\", \"soup bowl\", \"keyboard space bar\", \"space heater\", \"space shuttle\", \"spatula\", \"motorboat\", \"spider web\", \"spindle\", \"sports car\", \"spotlight\", \"stage\", \"steam locomotive\", \"through arch bridge\", \"steel drum\", \"stethoscope\", \"scarf\", \"stone wall\", \"stopwatch\", \"stove\", \"strainer\", \"tram\", \"stretcher\", \"couch\", \"stupa\", \"submarine\", \"suit\", \"sundial\", \"sunglasses\", \"sunglasses\", \"sunscreen\", \"suspension bridge\", \"mop\", \"sweatshirt\", \"swim trunks / shorts\", \"swing\", \"electrical switch\", \"syringe\", \"table lamp\", \"tank\", \"tape player\", \"teapot\", \"teddy bear\", \"television\", \"tennis ball\", \"thatched roof\", \"front curtain\", \"thimble\", \"threshing machine\", \"throne\", \"tile roof\", \"toaster\", \"tobacco shop\", \"toilet seat\", \"torch\", \"totem pole\", \"tow truck\", \"toy store\", \"tractor\", \"semi-trailer truck\", \"tray\", \"trench coat\", \"tricycle\", \"trimaran\", \"tripod\", \"triumphal arch\", \"trolleybus\", \"trombone\", \"hot tub\", \"turnstile\", \"typewriter keyboard\", \"umbrella\", \"unicycle\", \"upright piano\", \"vacuum cleaner\", \"vase\", \"vaulted or arched ceiling\", \"velvet fabric\", \"vending machine\", \"vestment\", \"viaduct\", \"violin\", \"volleyball\", \"waffle iron\", \"wall clock\", \"wallet\", \"wardrobe\", \"military aircraft\", \"sink\", \"washing machine\", \"water bottle\", \"water jug\", \"water tower\", \"whiskey jug\", \"whistle\", \"hair wig\", \"window screen\", \"window shade\", \"Windsor tie\", \"wine bottle\", \"airplane wing\", \"wok\", \"wooden spoon\", \"wool\", \"split-rail fence\", \"shipwreck\", \"sailboat\", \"yurt\", \"website\", \"comic book\", \"crossword\", \"traffic or street sign\", \"traffic light\", \"dust jacket\", \"menu\", \"plate\", \"guacamole\", \"consomme\", \"hot pot\", \"trifle\", \"ice cream\", \"popsicle\", \"baguette\", \"bagel\", \"pretzel\", \"cheeseburger\", \"hot dog\", \"mashed potatoes\", \"cabbage\", \"broccoli\", \"cauliflower\", \"zucchini\", \"spaghetti squash\", \"acorn squash\", \"butternut squash\", \"cucumber\", \"artichoke\", \"bell pepper\", \"cardoon\", \"mushroom\", \"Granny Smith apple\", \"strawberry\", \"orange\", \"lemon\", \"fig\", \"pineapple\", \"banana\", \"jackfruit\", \"cherimoya (custard apple)\", \"pomegranate\", \"hay\", \"carbonara\", \"chocolate syrup\", \"dough\", \"meatloaf\", \"pizza\", \"pot pie\", \"burrito\", \"red wine\", \"espresso\", \"tea cup\", \"eggnog\", \"mountain\", \"bubble\", \"cliff\", \"coral reef\", \"geyser\", \"lakeshore\", \"promontory\", \"sandbar\", \"beach\", \"valley\", \"volcano\", \"baseball player\", \"bridegroom\", \"scuba diver\", \"rapeseed\", \"daisy\", \"yellow lady's slipper\", \"corn\", \"acorn\", \"rose hip\", \"horse chestnut seed\", \"coral fungus\", \"agaric\", \"gyromitra\", \"stinkhorn mushroom\", \"earth star fungus\", \"hen of the woods mushroom\", \"bolete\", \"corn cob\", \"toilet paper\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetA(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images from the ImageNet-A dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, csvMapFile=\"dataloaders/wordNetIDs2Classes.csv\", transform=None\n",
    "    ):\n",
    "        paths = []\n",
    "        labels = []\n",
    "        names = []\n",
    "\n",
    "        mapping = {}\n",
    "        csv_file = csv.reader(open(csvMapFile, \"r\"))\n",
    "        for id, wordnet, name in csv_file:\n",
    "            if id == \"resnet_label\":\n",
    "                continue\n",
    "            mapping[int(wordnet)] = {\"id\": id, \"name\": name}\n",
    "\n",
    "        # print(mapping)\n",
    "        self.classnames = {}\n",
    "        for classes in os.listdir(root):\n",
    "            if classes == \"README.txt\":\n",
    "                continue\n",
    "            for img in os.listdir(os.path.join(root, classes)):\n",
    "                paths.append(os.path.join(root, classes, img).replace(\"\\\\\", \"/\"))\n",
    "                # remove n and leading 0s\n",
    "                class_id = int(classes[1:])\n",
    "                labels.append(int(mapping[class_id][\"id\"]))\n",
    "                names.append(mapping[class_id][\"name\"])\n",
    "                self.classnames[mapping[class_id][\"id\"]] = mapping[class_id][\"name\"]\n",
    "\n",
    "        self.data = {\"paths\": paths, \"labels\": labels, \"names\": names}\n",
    "        self.transform = transform\n",
    "\n",
    "    def getClassesNames(self):\n",
    "        \"\"\"\n",
    "        Returns the class names of the dataset.\n",
    "        \"\"\"\n",
    "        return set(self.data[\"names\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"paths\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data[\"paths\"][idx]\n",
    "        label = self.data[\"labels\"][idx]\n",
    "        name = self.data[\"names\"][idx]\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return {\"img\": img, \"label\": label, \"name\": name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetV2(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images from the ImageNet-V2 dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root, csvMapFile=\"dataloaders/wordNetIDs2Classes.csv\", transform=None\n",
    "    ):\n",
    "        paths = []\n",
    "        labels = []\n",
    "        names = []\n",
    "\n",
    "        mapping = {}\n",
    "        csv_file = csv.reader(open(csvMapFile, \"r\"))\n",
    "        for id, _, name in csv_file:\n",
    "            if id == \"resnet_label\":\n",
    "                continue\n",
    "            mapping[id] = name\n",
    "\n",
    "        self.classnames = {}\n",
    "        for classes in os.listdir(root):\n",
    "            for img in os.listdir(os.path.join(root, classes)):\n",
    "                paths.append(os.path.join(root, classes, img).replace(\"\\\\\", \"/\"))\n",
    "                labels.append(int(classes))\n",
    "                names.append(mapping[classes])\n",
    "                self.classnames[classes] = mapping[classes]\n",
    "\n",
    "        self.data = {\"paths\": paths, \"labels\": labels, \"names\": names}\n",
    "        self.transform = transform\n",
    "\n",
    "    def getClassesNames(self):\n",
    "        \"\"\"\n",
    "        Returns the class names of the dataset.\n",
    "        \"\"\"\n",
    "        return set(self.data[\"names\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"paths\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data[\"paths\"][idx]\n",
    "        label = self.data[\"labels\"][idx]\n",
    "        name = self.data[\"names\"][idx]\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return {\"img\": img, \"label\": label, \"name\": name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyAgumenter(object):\n",
    "    def __init__(self, base_transform, preprocess, augmentation, n_views=63):\n",
    "        self.base_transform = base_transform\n",
    "        self.preprocess = preprocess\n",
    "        self.n_views = n_views\n",
    "\n",
    "        if augmentation == 'augmix':\n",
    "\n",
    "            self.preaugment = transforms.Compose(\n",
    "                [\n",
    "                    AugMix(),\n",
    "                    transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "                    transforms.CenterCrop(224),\n",
    "                ]\n",
    "            )\n",
    "        elif augmentation == 'identity':\n",
    "            self.preaugment = self.base_transform\n",
    "        elif augmentation == 'cut':\n",
    "            self.preaugment = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Augmentation type not recognized')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = transforms.ToPILImage()(x)\n",
    "\n",
    "        image = self.preprocess(self.base_transform(x))\n",
    "\n",
    "        views = [self.preprocess(self.preaugment(x)) for _ in range(self.n_views)]\n",
    "\n",
    "        return [image] + views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(root, transform=None, csvMapFile=\"dataloaders/wordNetIDs2Classes.csv\"):\n",
    "    \"\"\"\n",
    "    Returns the dataloader of the dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an image and returns a transformed version. Defaults to None.\n",
    "    \"\"\"\n",
    "    root_A = os.path.join(root, \"imagenet-a\")\n",
    "    imageNet_A = ImageNetA(root_A, transform=transform, csvMapFile=csvMapFile)\n",
    "    root_V2 = os.path.join(root, \"imagenetv2-matched-frequency-format-val\")\n",
    "    imageNet_V2 = ImageNetV2(root_V2, transform=transform, csvMapFile=csvMapFile)\n",
    "\n",
    "    return imageNet_A, imageNet_V2\n",
    "\n",
    "def get_classes_names(csvMapFile=\"dataloaders/wordNetIDs2Classes.csv\"):\n",
    "    \"\"\"\n",
    "    Returns the class names of the dataset.\n",
    "\n",
    "    Args:\n",
    "        csvMapFile (str, optional): The path to the CSV file containing the mapping of WordNet IDs to class names. Defaults to \"dataloaders/wordNetIDs2Classes.csv\".\n",
    "    \"\"\"\n",
    "    names = [\"\"]*1000\n",
    "    csv_file = csv.reader(open(csvMapFile, 'r'))\n",
    "    for id, wordnet, name in csv_file:\n",
    "        if id == 'resnet_label':\n",
    "            continue\n",
    "        names[int(id)] = name\n",
    "    \n",
    "    return names\n",
    "\n",
    "def memo_get_datasets(augmentation, augs=64):\n",
    "    \"\"\"\n",
    "    Returns the ImageNetA and ImageNetV2 datasets for the memo model\n",
    "    Args:\n",
    "        augmentation (str): What type of augmentation to use in EasyAugmenter. Can be 'augmix', 'identity' or 'cut'\n",
    "        augs (int): The number of augmentations to compute. Must be greater than 1\n",
    "\n",
    "    Returns: The ImageNetA and ImageNetV2 datasets for the memo model, with the Augmentations already applied\n",
    "\n",
    "    \"\"\"\n",
    "    assert augs > 1, 'The number of augmentations must be greater than 1'\n",
    "    memo_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224)])\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    transform = EasyAgumenter(memo_transforms, preprocess, augmentation, augs - 1)\n",
    "    imageNet_A, imageNet_V2 = get_dataloaders('datasets', transform)\n",
    "    return imageNet_A, imageNet_V2\n",
    "\n",
    "def tpt_get_transforms(augs=64):\n",
    "\n",
    "    base_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_transform = EasyAgumenter(\n",
    "        base_transform,\n",
    "        preprocess,\n",
    "        n_views=augs - 1,\n",
    "    )\n",
    "\n",
    "    return data_transform\n",
    "\n",
    "\n",
    "def tpt_get_datasets(data_root, augmix=False, augs=64, all_classes=True):\n",
    "    \"\"\"\n",
    "    Returns the ImageNetA and ImageNetV2 datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - data_root (str): The root directory of the datasets.\n",
    "    - augmix (bool): Whether to use AugMix or not.\n",
    "    - augs (int): The number of augmentations to use.\n",
    "    - all_classes (bool): Whether to use all classes or not.\n",
    "\n",
    "    Returns:\n",
    "    - imageNet_A (ImageNetA): The ImageNetA dataset.\n",
    "    - ima_names (list): The original classnames in ImageNetA.\n",
    "    - ima_custom_names (list): The retouched  classnames in ImageNetA.\n",
    "    - ima_id_mapping (list): The mapping between the index of the classname and the ImageNet label\n",
    "\n",
    "    same for ImageNetV2\n",
    "\n",
    "    For instance the first element of ima_names corresponds to the label '90'.  After running the\n",
    "    inference run the predicted output through the ima_id_mapping to recover the correct class label.\n",
    "\n",
    "    out = tpt(inputs)\n",
    "    pred = out.argmax().item()\n",
    "    out_id = ima_id_mapping[pred]\n",
    "\n",
    "    \"\"\"\n",
    "    base_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    data_transform = EasyAgumenter(\n",
    "        base_transform,\n",
    "        preprocess,\n",
    "        augmentation=(\"augmix\" if augmix else \"cut\"),\n",
    "        n_views=augs - 1,\n",
    "    )\n",
    "\n",
    "    imageNet_A = ImageNetA(\n",
    "        os.path.join(data_root, \"imagenet-a\"), transform=data_transform\n",
    "    )\n",
    "    imageNet_V2 = ImageNetV2(\n",
    "        os.path.join(data_root, \"imagenetv2-matched-frequency-format-val\"),\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "    imv2_label_mapping = list(imageNet_V2.classnames.keys())\n",
    "    imv2_names = list(imageNet_V2.classnames.values())\n",
    "    imv2_custom_names = [imagenet_classes[int(i)] for i in imv2_label_mapping]\n",
    "\n",
    "    ima_label_mapping = list(imageNet_A.classnames.keys())\n",
    "    ima_names = list(imageNet_A.classnames.values())\n",
    "    ima_custom_names = [imagenet_classes[int(i)] for i in ima_label_mapping]\n",
    "\n",
    "    if all_classes:\n",
    "        ima_names += [name for name in imv2_names if name not in ima_names]\n",
    "        ima_custom_names += [\n",
    "            name for name in imv2_custom_names if name not in ima_custom_names\n",
    "        ]\n",
    "        ima_label_mapping += [\n",
    "            map for map in imv2_label_mapping if map not in ima_label_mapping\n",
    "        ]\n",
    "\n",
    "    return (\n",
    "        imageNet_A,\n",
    "        ima_names,\n",
    "        ima_custom_names,\n",
    "        ima_label_mapping,\n",
    "        imageNet_V2,\n",
    "        imv2_names,\n",
    "        imv2_custom_names,\n",
    "        imv2_label_mapping,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyPromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        clip,\n",
    "        base_prompt=\"a photo of [CLS]\",\n",
    "        splt_ctx=False,\n",
    "        classnames=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.base_prompt = base_prompt\n",
    "        self.tkn_embedder = clip.token_embedding\n",
    "        # set requires_grad to False\n",
    "        self.tkn_embedder.requires_grad_(False)\n",
    "\n",
    "        self.split_ctx = splt_ctx\n",
    "\n",
    "        self.prepare_prompts(classnames)\n",
    "\n",
    "    def prepare_prompts(self, classnames):\n",
    "        print(\"[PromptLearner] Preparing prompts\")\n",
    "\n",
    "        self.classnames = classnames\n",
    "        # self.classnames = [cls.split(\",\")[0] for cls in self.classnames]\n",
    "\n",
    "        # get numbr of classes\n",
    "        self.cls_num = len(self.classnames)\n",
    "\n",
    "        # get prompt text prefix and suffix\n",
    "        txt_prefix = self.base_prompt.split(\"[CLS]\")[0]\n",
    "        txt_suffix = self.base_prompt.split(\"[CLS]\")[1]\n",
    "\n",
    "        # tokenize the prefix and suffix\n",
    "        tkn_prefix = tokenize(txt_prefix)\n",
    "        tkn_suffix = tokenize(txt_suffix)\n",
    "        tkn_pad = tokenize(\"\")\n",
    "        tkn_cls = tokenize(self.classnames)\n",
    "\n",
    "        # get the index of the last element of the prefix and suffix\n",
    "        idx = torch.arange(tkn_prefix.shape[1], 0, -1)\n",
    "        self.indp = torch.argmax((tkn_prefix == 0) * idx, 1, keepdim=True)\n",
    "        self.inds = torch.argmax((tkn_suffix == 0) * idx, 1, keepdim=True)\n",
    "\n",
    "        # token length for each class\n",
    "        self.indc = torch.argmax((tkn_cls == 0) * idx, 1, keepdim=True)\n",
    "\n",
    "        # get the prefix, suffix, SOT and EOT\n",
    "        self.tkn_sot = tkn_prefix[:, :1]\n",
    "        self.tkn_prefix = tkn_prefix[:, 1 : self.indp - 1]\n",
    "        self.tkn_suffix = tkn_suffix[:, 1 : self.inds - 1]\n",
    "        self.tkn_eot = tkn_suffix[:, self.inds - 1 : self.inds]\n",
    "        self.tkn_pad = tkn_pad[:, 2:]\n",
    "\n",
    "        # load segments to CUDA, be ready to be embedded\n",
    "        self.tkn_sot = self.tkn_sot.to(self.device)\n",
    "        self.tkn_prefix = self.tkn_prefix.to(self.device)\n",
    "        self.tkn_suffix = self.tkn_suffix.to(self.device)\n",
    "        self.tkn_eot = self.tkn_eot.to(self.device)\n",
    "        self.tkn_pad = self.tkn_pad.to(self.device)\n",
    "\n",
    "        self.tkn_cls = tkn_cls.to(self.device)\n",
    "\n",
    "        # gets the embeddings\n",
    "        with torch.no_grad():\n",
    "            self.emb_sot = self.tkn_embedder(self.tkn_sot)\n",
    "            self.emb_prefix = self.tkn_embedder(self.tkn_prefix)\n",
    "            self.emb_suffix = self.tkn_embedder(self.tkn_suffix)\n",
    "            self.emb_eot = self.tkn_embedder(self.tkn_eot)\n",
    "            self.emb_cls = self.tkn_embedder(self.tkn_cls)\n",
    "            self.emb_pad = self.tkn_embedder(self.tkn_pad)\n",
    "\n",
    "        # take out the embeddings of the class tokens (they are different lenghts)\n",
    "        self.all_cls = []\n",
    "        for i in range(self.cls_num):\n",
    "            self.all_cls.append(self.emb_cls[i][1 : self.indc[i] - 1])\n",
    "\n",
    "        # prepare the prompts, they are needed for text encoding\n",
    "        self.txt_prompts = [\n",
    "            self.base_prompt.replace(\"[CLS]\", cls) for cls in self.classnames\n",
    "        ]\n",
    "        self.tkn_prompts = tokenize(self.txt_prompts)\n",
    "\n",
    "        # set the inital context, this will be reused at every new inference\n",
    "        # this is the context that will be optimized\n",
    "\n",
    "        if self.split_ctx:\n",
    "            self.pre_init_state = self.emb_prefix.detach().clone()\n",
    "            self.suf_init_state = self.emb_suffix.detach().clone()\n",
    "            self.emb_prefix = nn.Parameter(self.emb_prefix)\n",
    "            self.emb_suffix = nn.Parameter(self.emb_suffix)\n",
    "            self.register_parameter(\"emb_prefix\", self.emb_prefix)\n",
    "            self.register_parameter(\"emb_suffix\", self.emb_suffix)\n",
    "        else:\n",
    "            self.ctx = torch.cat((self.emb_prefix, self.emb_suffix), dim=1)\n",
    "            self.ctx_init_state = self.ctx.detach().clone()\n",
    "            self.ctx = nn.Parameter(self.ctx)\n",
    "            self.register_parameter(\"ctx\", self.ctx)\n",
    "\n",
    "    def build_ctx(self):\n",
    "        prompts = []\n",
    "        for i in range(self.cls_num):\n",
    "            pad_size = self.emb_cls.shape[1] - (\n",
    "                self.emb_prefix.shape[1]\n",
    "                + self.indc[i].item()\n",
    "                + self.emb_suffix.shape[1]\n",
    "            )\n",
    "\n",
    "            if self.split_ctx:\n",
    "                prefix = self.emb_prefix\n",
    "                suffix = self.emb_suffix\n",
    "            else:\n",
    "                prefix = self.ctx[:, : self.emb_prefix.shape[1]]\n",
    "                suffix = self.ctx[:, self.emb_prefix.shape[1] :]\n",
    "\n",
    "            prompt = torch.cat(\n",
    "                (\n",
    "                    self.emb_sot,\n",
    "                    prefix,\n",
    "                    self.all_cls[i].unsqueeze(0),\n",
    "                    suffix,\n",
    "                    self.emb_eot,\n",
    "                    self.emb_pad[:, :pad_size],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "        prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        return self.build_ctx()\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        if self.split_ctx:\n",
    "            self.emb_prefix.data.copy_(self.pre_init_state)  # to be optimized\n",
    "            self.emb_suffix.data.copy_(self.suf_init_state)  # to be optimized\n",
    "        else:\n",
    "            self.ctx.data.copy_(self.ctx_init_state)  # to be optimized\n",
    "\n",
    "\n",
    "class EasyTPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        base_prompt=\"a photo of a [CLS]\",\n",
    "        arch=\"RN50\",\n",
    "        splt_ctx=False,\n",
    "        classnames=None,\n",
    "        ttt_steps=1,\n",
    "        augs=64,\n",
    "        lr=0.005,\n",
    "    ):\n",
    "        super(EasyTPT, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        ###TODO: tobe parametrized\n",
    "        DOWNLOAD_ROOT = \"~/.cache/clip\"\n",
    "        ###\n",
    "\n",
    "        self.base_prompt = base_prompt\n",
    "        self.ttt_steps = ttt_steps\n",
    "        self.augs = augs\n",
    "        self.selected_idx = None\n",
    "\n",
    "        # Load clip\n",
    "        clip, self.preprocess = load(arch, device=device, download_root=DOWNLOAD_ROOT)\n",
    "        self.clip = clip\n",
    "        self.dtype = clip.dtype\n",
    "        self.image_encoder = clip.encode_image\n",
    "        self.text_encoder = clip.encode_text\n",
    "\n",
    "        # freeze the parameters\n",
    "        for name, param in self.named_parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        # create the prompt learner\n",
    "        self.prompt_learner = EasyPromptLearner(\n",
    "            device, clip, base_prompt, splt_ctx, classnames\n",
    "        )\n",
    "\n",
    "        # create optimizer and save the state\n",
    "        trainable_param = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"[EasyTPT] Training parameter: {name}\")\n",
    "                trainable_param.append(param)\n",
    "        self.optimizer = torch.optim.AdamW(trainable_param, lr)\n",
    "        self.optim_state = deepcopy(self.optimizer.state_dict())\n",
    "\n",
    "        # breakpoint()\n",
    "\n",
    "    def forward(self, x, top=0.10):\n",
    "        \"\"\"\n",
    "        If x is a list of augmentations, run the confidence selection,\n",
    "        otherwise just run the inference\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        # breakpoint()\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x).to(self.device)\n",
    "            logits = self.inference(x)\n",
    "            if self.selected_idx is not None:\n",
    "                logits = logits[self.selected_idx]\n",
    "            else:\n",
    "                logits, self.selected_idx = self.select_confident_samples(logits, top)\n",
    "        else:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = x.to(self.device)\n",
    "            logits = self.inference(x)\n",
    "        \n",
    "        # print (f\"[EasyTPT] input shape: {x.shape}\")\n",
    "        # print(\"[EasyTPT] logits shape: \", logits.shape)\n",
    "        return logits\n",
    "\n",
    "    def inference(self, x):\n",
    "        with torch.no_grad():\n",
    "            image_feat = self.image_encoder(x)\n",
    "            image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        emb_prompts = self.prompt_learner()\n",
    "\n",
    "        txt_features = self.custom_encoder(emb_prompts, self.prompt_learner.tkn_prompts)\n",
    "        txt_features = txt_features / txt_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.clip.logit_scale.exp()\n",
    "        logits = logit_scale * image_feat @ txt_features.t()\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def custom_encoder(self, prompts, tokenized_prompts):\n",
    "        \"\"\"\n",
    "        Custom clip text encoder, unlike the original clip encoder this one\n",
    "        takes the prompts embeddings from the prompt learner\n",
    "        \"\"\"\n",
    "        x = prompts + self.clip.positional_embedding\n",
    "        x = x.permute(1, 0, 2).type(self.dtype)  # NLD -> LND\n",
    "        x = self.clip.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip.ln_final(x).type(self.dtype)\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = (\n",
    "            x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)]\n",
    "            @ self.clip.text_projection\n",
    "        )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the optimizer and the prompt learner to their initial state,\n",
    "        this has to be run before each new test\n",
    "        \"\"\"\n",
    "        self.optimizer.load_state_dict(deepcopy(self.optim_state))\n",
    "        self.prompt_learner.reset()\n",
    "        self.selected_idx = None\n",
    "\n",
    "    def select_confident_samples(self, logits, top):\n",
    "        \"\"\"\n",
    "        Performs confidence selection, will return the indexes of the\n",
    "        augmentations with the highest confidence as well as the filtered\n",
    "        logits\n",
    "\n",
    "        Parameters:\n",
    "        - logits (torch.Tensor): the logits of the model [NAUGS, NCLASSES]\n",
    "        - top (float): the percentage of top augmentations to use\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        idx = torch.argsort(batch_entropy, descending=False)[\n",
    "            : int(batch_entropy.size()[0] * top)\n",
    "        ]\n",
    "        return logits[idx], idx\n",
    "\n",
    "    def tpt_avg_entropy(self, outputs):\n",
    "        logits = outputs - outputs.logsumexp(\n",
    "            dim=-1, keepdim=True\n",
    "        )  # logits = outputs.log_softmax(dim=1) [N, 1000]\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(\n",
    "            logits.shape[0]\n",
    "        )  # avg_logits = logits.mean(0) [1, 1000]\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def predict(self, images, niter=1):\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        for _ in range(niter):\n",
    "            out = self(images)\n",
    "            loss = self.tpt_avg_entropy(out)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            out = self(images[0])\n",
    "            out_id = out.argmax(1).item()\n",
    "            prediction = self.prompt_learner.classnames[out_id]\n",
    "\n",
    "        # return out_id, prediction\n",
    "        return out_id\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        Returns the optimizer\n",
    "\n",
    "        Returns:\n",
    "        - torch.optim: the optimizer\n",
    "        \"\"\"\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo_marginal_entropy(outputs):\n",
    "    logits = outputs - outputs.logsumexp(dim=-1, keepdim=True)\n",
    "    avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0])\n",
    "    min_real = torch.finfo(avg_logits.dtype).min\n",
    "    avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "\n",
    "def _modified_bn_forward(self, input):\n",
    "    est_mean = torch.zeros(self.running_mean.shape, device=self.running_mean.device)\n",
    "    est_var = torch.ones(self.running_var.shape, device=self.running_var.device)\n",
    "    nn.functional.batch_norm(input, est_mean, est_var, None, None, True, 1.0, self.eps)\n",
    "    running_mean = self.prior * self.running_mean + (1 - self.prior) * est_mean\n",
    "    running_var = self.prior * self.running_var + (1 - self.prior) * est_var\n",
    "    return nn.functional.batch_norm(input, running_mean, running_var, self.weight, self.bias, False, 0, self.eps)\n",
    "\n",
    "\n",
    "class EasyMemo(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to wrap a neural network with the MEMO TTA method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, device, classes_mask, prior_strength: float = 1.0, lr=0.005, weight_decay=0.0001, opt='sgd',\n",
    "                 niter=1, top=0.1, drop=False):\n",
    "        \"\"\"\n",
    "        Initializes the EasyMemo model with various arguments\n",
    "        Args:\n",
    "            net: The model to wrap with EasyMemo\n",
    "            device: The device to run the model on(usually 'CPU' or 'CUDA')\n",
    "            classes_mask: The classes to consider for the model(used for Imagenet-A)\n",
    "            prior_strength: The strength of the prior to use in the modified BN forward pass\n",
    "            lr: The Learning rate for the optimizer of the model\n",
    "            weight_decay: The weight decay for the optimizer of the model\n",
    "            opt: Which optimizer to use for this model between 'sgd' and 'adamw' for the respective optimizers\n",
    "            niter: The number of iterations to run the memo pass for\n",
    "            top: The percentage of the top logits to consider for confidence selection\n",
    "        \"\"\"\n",
    "        super(EasyMemo, self).__init__()\n",
    "\n",
    "        self.drop = drop\n",
    "        if self.drop:\n",
    "            net.layer4.add_module('dropout', nn.Dropout(0.5, inplace=True))\n",
    "        self.device = device\n",
    "        self.prior_strength = prior_strength\n",
    "        self.net = net.to(device)\n",
    "        self.optimizer = self.memo_optimizer_model(lr=lr, weight_decay=weight_decay, opt=opt)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.opt = opt\n",
    "        self.confidence_idx = None\n",
    "        self.memo_modify_bn_pass()\n",
    "        self.criterion = memo_marginal_entropy\n",
    "        self.niter = niter\n",
    "        self.top = top\n",
    "        self.initial_state = deepcopy(self.net.state_dict())\n",
    "        self.classes_mask = classes_mask\n",
    "\n",
    "    def forward(self, x, top=-1):\n",
    "        \"\"\"\n",
    "        Forward pass where we check which type of input we have and we call the inference on the input image Tensor\n",
    "        Args:\n",
    "            top: How many samples to select from the batch\n",
    "            x: A Tensor of shape (N, C, H, W) or a list of Tensors of shape (N, C, H, W)\n",
    "\n",
    "        Returns: The logits after the inference pass\n",
    "\n",
    "        \"\"\"\n",
    "        self.top = top if top > 0 else self.top\n",
    "        # print(f\"Shape forward: {x.shape}\")\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x).to(self.device)\n",
    "            # print(f\"Shape forward: {x.shape}\")\n",
    "            logits = self.inference(x)\n",
    "            logits, self.confidence_idx = self.topk_selection(logits)\n",
    "        else:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.unsqueeze(0)\n",
    "            x = x.to(self.device)\n",
    "            logits = self.inference(x)\n",
    "\n",
    "        # print(f\"[EasyMemo] input shape: {x.shape}\")\n",
    "        # print(f\"[EasyMemo] logits shape: {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Return the logits of the image in input x\n",
    "        Args:\n",
    "            x: A Tensor of shape (N, C, H, W) of an Image\n",
    "\n",
    "        Returns: The logits for that Tensor image\n",
    "\n",
    "        \"\"\"\n",
    "        self.net.eval()\n",
    "        outputs = self.net(x)\n",
    "\n",
    "        out_app = torch.zeros(outputs.shape[0], len(self.classes_mask)).to(self.device)\n",
    "        for i, out in enumerate(outputs):\n",
    "            out_app[i] = out[self.classes_mask]\n",
    "        return out_app\n",
    "\n",
    "    def predict(self, x, niter=1):\n",
    "        \"\"\"\n",
    "        Predicts the class of the input x, which is an image\n",
    "        Args:\n",
    "            niter: The number of iteration on which to run the memo pass\n",
    "            x: Tensor of shape (N, C, H, W)\n",
    "\n",
    "        Returns: The predicted classes\n",
    "\n",
    "        \"\"\"\n",
    "        self.niter = niter\n",
    "        if self.drop:\n",
    "            self.net.train()\n",
    "        else:\n",
    "            self.net.eval()\n",
    "\n",
    "        for iteration in range(self.niter):\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.forward(x)\n",
    "            outputs, _ = self.topk_selection(outputs)\n",
    "            loss = self.criterion(outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(x[0].unsqueeze(0).to(self.device))\n",
    "            outs = torch.zeros(outputs.shape[0], len(self.classes_mask)).to(self.device)\n",
    "            for i, out in enumerate(outputs):\n",
    "                outs[i] = out[self.classes_mask]\n",
    "            predicted = outs.argmax(1).item()\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the model to its initial state\"\"\"\n",
    "        del self.optimizer\n",
    "        self.optimizer = self.memo_optimizer_model(lr=self.lr, weight_decay=self.weight_decay, opt=self.opt)\n",
    "        self.confidence_idx = None\n",
    "        self.net.load_state_dict(deepcopy(self.initial_state))\n",
    "\n",
    "    def memo_modify_bn_pass(self):\n",
    "        print('modifying BN forward pass')\n",
    "        nn.BatchNorm2d.prior = self.prior_strength\n",
    "        nn.BatchNorm2d.forward = _modified_bn_forward\n",
    "\n",
    "    def memo_optimizer_model(self, lr=0.005, weight_decay=0.0001, opt='sgd'):\n",
    "        \"\"\"\n",
    "        Initializes the optimizer for the memo model\n",
    "        Args:\n",
    "            lr: The learning rate for the optimizer\n",
    "            weight_decay: The weight decay for the optimizer\n",
    "            opt: Which optimizer to use\n",
    "\n",
    "        Returns: The optimizer for the memo model\n",
    "\n",
    "        \"\"\"\n",
    "        if opt == 'sgd':\n",
    "            optimizer = optim.SGD(self.net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif opt == 'adamw':\n",
    "            optimizer = optim.AdamW(self.net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer selected')\n",
    "        return optimizer\n",
    "\n",
    "    def memo_adapt_single(self, inputs):\n",
    "        \"\"\"\n",
    "        A single step of memo adaptation\n",
    "        Args:\n",
    "            inputs: A tensor of shape (N, C, H, W)\n",
    "\n",
    "        \"\"\"\n",
    "        self.net.eval()\n",
    "        assert self.niter > 0 and isinstance(self.niter, int), 'niter must be a positive integer'\n",
    "        for iteration in range(self.niter):\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net(inputs)\n",
    "            loss = self.criterion(outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def memo_test_single(self, image, label):\n",
    "        \"\"\"\n",
    "        Tests the model on a single image and returns the correctness and confidence\n",
    "        Args:\n",
    "            image: A tensor of shape (N, C, H, W)\n",
    "            label: The correct label for the test\n",
    "\n",
    "        Returns: The correctness and confidence of the prediction\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(image.to(device=self.device))\n",
    "            _, predicted = outputs.max(1)\n",
    "            confidence = nn.functional.softmax(outputs, dim=1).squeeze()[predicted].item()\n",
    "        correctness = 1 if predicted.item() == label else 0\n",
    "        return correctness, confidence\n",
    "\n",
    "    def topk_selection(self, logits):\n",
    "        \"\"\"\n",
    "        Selects the top k logits based on the batch entropy\n",
    "        Args:\n",
    "            logits: A tensor of shape (N, C)\n",
    "\n",
    "        Returns: The filtered logits and the indices of the selected logits\n",
    "\n",
    "        \"\"\"\n",
    "        batch_entropy = -(logits.softmax(1) * logits.log_softmax(1)).sum(1)\n",
    "        selected_idx = torch.argsort(batch_entropy, descending=False)[: int(batch_entropy.size()[0] * self.top)]\n",
    "        return logits[selected_idx], selected_idx\n",
    "\n",
    "    def dropout_train(self, x):\n",
    "        self.net.train()\n",
    "        outputs = self.forward(x)\n",
    "        outputs, _ = self.topk_selection(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble class. Implements an ensemble of models with entropy minimization.\n",
    "\n",
    "    Attributes:\n",
    "        models (list): A list of models to be used in the ensemble.\n",
    "        temps (list): A list of temperature values corresponding to each model.\n",
    "        test_single_models (bool): Whether to test each individual model in addition to the ensemble.\n",
    "        device (str): The device to be used for computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, models, temps, device=\"cuda\", test_single_models=False):\n",
    "        \"\"\"\n",
    "        Initializes an Ensemble object.\n",
    "\n",
    "        Args:\n",
    "            models (list): A list of models to be used in the ensemble.\n",
    "            temps (list): A list of temperature values corresponding to each model.\n",
    "            device (str, optional): The device to be used for computation. Defaults to \"cuda\".\n",
    "            test_single_models (bool, optional): Whether to test each individual model in addition to the ensemble. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.temps = temps\n",
    "        self.test_single_models = test_single_models\n",
    "        self.device = device\n",
    "\n",
    "    def entropy(self, logits):\n",
    "        \"\"\"\n",
    "        Computes the entropy of a set of logits.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): The logits to compute the entropy of.\n",
    "        \"\"\"\n",
    "        return -(torch.exp(logits) * logits).sum(dim=-1)\n",
    "\n",
    "    def marginal_distribution(self, models_logits):\n",
    "        \"\"\"\n",
    "        Computes the marginal distribution of the ensemble.\n",
    "\n",
    "        Args:\n",
    "            models_logits (torch.Tensor): The logits of the models in the ensemble.\n",
    "        \"\"\"\n",
    "        # average logits for each model\n",
    "        avg_models_logits = torch.Tensor(models_logits.shape[0], models_logits.shape[2]).to(self.device)\n",
    "        for i, model_logits in enumerate(models_logits):\n",
    "            avg_outs = torch.logsumexp(model_logits, dim=0) - torch.log(torch.tensor(model_logits.shape[0]))\n",
    "            min_real = torch.finfo(avg_outs.dtype).min\n",
    "            avg_outs = torch.clamp(avg_outs, min=min_real)\n",
    "            avg_outs /= self.temps[i]\n",
    "            avg_models_logits[i] = torch.log_softmax(avg_outs, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            entropies = torch.stack([self.entropy(logits) for logits in avg_models_logits]).to(self.device)\n",
    "            sum_entropies = torch.sum(entropies, dim=0)\n",
    "            scale = torch.stack([sum_entropies/entopy for entopy in entropies]).to(self.device)\n",
    "            #normalize sum to 1\n",
    "            scale = scale / torch.sum(scale)\n",
    "\n",
    "        print(\"\\t\\t[Ensemble] Entropies: \", entropies)\n",
    "        print(\"\\t\\t[Ensemble] Scales: \", scale)\n",
    "\n",
    "        avg_logits = torch.sum(torch.stack([scale[i].item() * avg_models_logits[i] for i in range(len(avg_models_logits))]), dim=0)\n",
    "\n",
    "        return avg_logits\n",
    "\n",
    "    def get_models_outs(self, inputs, top=0.1):\n",
    "        \"\"\"\n",
    "        Computes the outputs of the models in the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        model_outs = torch.stack([model(inputs[i], top).to(self.device) for i, model in enumerate(self.models)]).to(self.device)\n",
    "        return model_outs.to(self.device)\n",
    "\n",
    "    def get_models_predictions(self, inputs):\n",
    "        \"\"\"\n",
    "        Computes the predictions of the single models in the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "        \"\"\"\n",
    "        models_pred = [model.predict(inputs[i]) for i, model in enumerate(self.models)]\n",
    "        return models_pred\n",
    "\n",
    "    def entropy_minimization(self, inputs, niter=1, top=0.1):\n",
    "        \"\"\"\n",
    "        Test time adaptation step. Minimizes the entropy of the ensemble's predictions.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            niter (int, optional): The number of iterations to perform. Defaults to 1.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        for i in range(niter):\n",
    "            outs = self.get_models_outs(inputs, top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "\n",
    "            loss = self.entropy(avg_logit)\n",
    "            loss.backward()\n",
    "            for model in self.models:\n",
    "                model.optimizer.step()\n",
    "                model.optimizer.zero_grad()\n",
    "\n",
    "    def forward(self, inputs, niter=1, top=0.1):\n",
    "        \"\"\"\n",
    "        Forward pass of the ensemble.\n",
    "\n",
    "        Args:\n",
    "            inputs (list): A list of inputs to be fed to the models.\n",
    "            niter (int, optional): The number of iterations to perform. Defaults to 1.\n",
    "            top (float, optional): The top percentage of the outputs to be used. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        # get models outputs\n",
    "        self.reset()\n",
    "        models_pred = self.get_models_predictions(inputs)\n",
    "\n",
    "        self.reset()\n",
    "        self.entropy_minimization(inputs, niter, top)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outs = self.get_models_outs(inputs, top)\n",
    "            avg_logit = self.marginal_distribution(outs)\n",
    "            prediction = torch.argmax(avg_logit, dim=0)\n",
    "\n",
    "        return models_pred, prediction\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the models in the ensemble.\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPT(device=\"cuda\", naug=30, base_prompt=\"A bad photo of a [CLS].\", arch=\"RN50\", splt_ctx= True, A=True):\n",
    "    # prepare TPT\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Using CPU this is no bueno\")\n",
    "    else:\n",
    "        print(\"Using GPU, brace yourself!\")\n",
    "\n",
    "    datasetRoot = \"datasets\"\n",
    "    imageNetA, _, imageNetACustomNames, imageNetAMap, imageNetV2, _, imageNetV2CustomNames, imageNetV2Map = tpt_get_datasets(datasetRoot, augs=naug, all_classes=False)\n",
    "    \n",
    "    if A:\n",
    "        dataset = imageNetA\n",
    "        classnames = imageNetACustomNames\n",
    "        mapping = imageNetAMap\n",
    "    else:\n",
    "        dataset = imageNetV2\n",
    "        classnames = imageNetV2CustomNames\n",
    "        mapping = imageNetV2Map\n",
    "    \n",
    "    tpt = EasyTPT(\n",
    "        base_prompt=base_prompt,\n",
    "        arch=arch,\n",
    "        splt_ctx=splt_ctx,\n",
    "        classnames=classnames,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return tpt, dataset, mapping\n",
    "\n",
    "def memo(device=\"cuda\", prior_strength=0.94, naug=30, A=True):\n",
    "    # prepare MEMO\n",
    "    imageNet_A, imageNet_V2 = memo_get_datasets(augmentation='cut', augs=naug)\n",
    "    dataset = imageNet_A if A else imageNet_V2\n",
    "\n",
    "    mapping = list(dataset.classnames.keys())\n",
    "    for i,id in enumerate(mapping):\n",
    "        mapping[i] = int(id)\n",
    "    \n",
    "    rn50 = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    memo = EasyMemo(rn50, device=device, classes_mask=mapping, prior_strength=prior_strength)\n",
    "    \n",
    "    return memo, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tpt_model:EasyTPT, memo_model, tpt_data, mapping, memo_data, device=\"cuda\", niter=1, top=0.1):\n",
    "    correct = 0\n",
    "    correctSingle = [0, 0]\n",
    "    cnt = 0\n",
    "\n",
    "    class_names = get_classes_names()\n",
    "    models_names = [\"TPT\", \"MEMO\"]\n",
    "\n",
    "    TPT_temp = 1.55\n",
    "    MEMO_temp = 0.7\n",
    "    temps = [TPT_temp, MEMO_temp]\n",
    "\n",
    "    testSingleModels = True\n",
    "\n",
    "    #shuffle the data\n",
    "    indx = np.random.permutation(range(len(tpt_data)))\n",
    "\n",
    "    model = Ensemble(models=[tpt_model, memo_model], temps=temps, \n",
    "                     device=device, test_single_models=testSingleModels)\n",
    "\n",
    "    for i in indx:\n",
    "        cnt += 1  \n",
    "\n",
    "        img_TPT = tpt_data[i][\"img\"]\n",
    "        img_MEMO = memo_data[i][\"img\"]\n",
    "        data = [img_TPT, img_MEMO]\n",
    "        \n",
    "        label = int(tpt_data[i][\"label\"])\n",
    "        label2 = int(memo_data[i][\"label\"])\n",
    "        assert label == label2 #check if the labels are the same\n",
    "        name = tpt_data[i][\"name\"]\n",
    "\n",
    "        print (f\"Testing on {i} - name: {name} - label: {label}\")\n",
    "\n",
    "        models_out, prediction = model(data, niter=niter, top=0.1)\n",
    "        models_out = [int(mapping[model_out]) for model_out in models_out]\n",
    "        prediction = int(mapping[prediction])\n",
    "        \n",
    "        if testSingleModels:\n",
    "            for i, model_out in enumerate(models_out):\n",
    "                if label == model_out:\n",
    "                    correctSingle[i] += 1\n",
    "                \n",
    "                print(f\"\\t{models_names[i]} model accuracy: {correctSingle[i]}/{cnt} - predicted class {model_out}: {class_names[model_out]} - tested: {cnt} / {len(tpt_data)}\")\n",
    "\n",
    "        if label == prediction:\n",
    "            correct += 1\n",
    "            \n",
    "        print(f\"\\tEnsemble accuracy: {correct}/{cnt} - predicted class {prediction}: {class_names[prediction]} - tested: {cnt} / {len(tpt_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpt_model, tpt_data, mapping = TPT(\"cuda\", naug=naug, A=imageNetA)\n",
    "    \n",
    "memo_model, memo_data = memo(\"cuda\", naug=naug, A=imageNetA)\n",
    "\n",
    "if (imageNetA):\n",
    "    print(\"Testing on ImageNet-A\")\n",
    "else:\n",
    "    print(\"Testing on ImageNet-V2\")\n",
    "\n",
    "test(tpt_model, memo_model, tpt_data, mapping, memo_data, device, niter, top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
