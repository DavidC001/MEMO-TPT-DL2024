{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqY3_GDa969x"
   },
   "source": [
    "# Contrastive Language-Image Pretraining (CLIP)\n",
    "\n",
    "Today, we will delve into the intersection of computer vision and natural language processing, which has recently gained significant importance. The development of robust vision and language models has been crucial in enabling this intersection.\n",
    "\n",
    "CLIP is a noteworthy example of such a model. It can comprehend natural language descriptions of images and generate image embeddings catering to a range of downstream tasks. The model has been trained on a vast dataset of images and their corresponding captions, which has facilitated it to acquire a comprehensive representation of both images and language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C39Q3fh7-lXM"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's start with the installation of the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEi90DRG-qMI",
    "outputId": "c564b0aa-24ed-43ba-95a4-3b88db6f2522"
   },
   "outputs": [],
   "source": [
    "!pip install -q ftfy regex tqdm scikit-learn scikit-image\n",
    "!pip install -q git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb6wAMSs-xmJ"
   },
   "source": [
    "We can now import all the pertinent packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVMQm4Ly-syT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from clip import clip\n",
    "\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# \n",
    "# import IPython.display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U8M-Jol_BEO"
   },
   "source": [
    "First, we can visualise the available pre-trained models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7Nb-9Ed-97Z",
    "outputId": "0d5a1796-93ff-4785-f62b-a8f594fc2aaf"
   },
   "outputs": [],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbEj7Ik6_Ks2"
   },
   "source": [
    "...and load one of them. For this, we can use the `clip.load()` function and specify the name of the model we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-2Oxcxg_JkP"
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbdwSQTe_cqW"
   },
   "source": [
    "By inspecting some variables, we can better understand the structure of the architecture we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20Y3dZo7_jfH",
    "outputId": "880689be-1f16-4599-ed1b-401c40058be7"
   },
   "outputs": [],
   "source": [
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZTXLOQQArep"
   },
   "source": [
    "## Zero-shot evaluation\n",
    "\n",
    "CLIP's extraordinary feature is its zero-shot capability, which enables it to perform tasks without any additional training or fine-tuning on specific datasets.\n",
    "\n",
    "This ability is made possible by CLIP's pre-training on a large corpus of text and images, which empowers it to acquire a comprehensive and flexible representation of language and visual concepts. Consequently, CLIP can perform an extensive range of tasks without any additional training, making it a powerful tool for various applications in NLP and computer vision.\n",
    "\n",
    "To assess this capability, we need to prepare some test samples. Earlier, by employing the `clip.load()` function, we obtained two outputsâ€”the pre-trained CLIP model and a preprocess object. On examining the object, we can infer that it contains a list of transformations to apply on input images to make them compatible with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfWiaoJPAtjz",
    "outputId": "ef4129b0-a1f1-46e3-bd15-a1fe6c162587"
   },
   "outputs": [],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A1Sg-DMCQ_7"
   },
   "source": [
    "Therefore, we can use the `preprocess` transform to convert `PIL.Image`s into tensors that are compatible with CLIP.\n",
    "\n",
    "However, for the textual counterpart, we cannot apply the same transformation. Instead, we can use the `clip.tokenize()` function, which accepts textual input and converts it into an integer tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIVMvAO6CmbX",
    "outputId": "15fdb05e-09a8-4dda-e603-43fdbf367cb2"
   },
   "outputs": [],
   "source": [
    "clip.tokenize(\"tokenize me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLCGbPcAC3Vz"
   },
   "source": [
    "We will now examine CLIP's performance on images and textual descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjhXwFARFyJC"
   },
   "source": [
    "Next, we can define a list of images that we plan to test and also visualize them along with their textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "mgDG1CXaDX6F",
    "outputId": "3efbc867-07a3-4739-92df-9551926c8f18"
   },
   "outputs": [],
   "source": [
    "# Create a description for some images\n",
    "DESCRIPTIONS = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
    "    \"coffee\": \"a cup of coffee on a saucer\"\n",
    "}\n",
    "\n",
    "def get_data():\n",
    "    images = []\n",
    "    texts = []\n",
    "    \n",
    "    # Get all the filenames in the data directory\n",
    "    data_dir = Path(skimage.data_dir)\n",
    "    filenames = [\n",
    "        filename for filename in data_dir.glob('*')\n",
    "            if filename.suffix in {'.png', '.jpg'}\n",
    "        ]\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # Skip images we do not care about\n",
    "        name = filename.stem\n",
    "        if name not in DESCRIPTIONS:\n",
    "            continue\n",
    "\n",
    "        images.append(filename)\n",
    "        texts.append(DESCRIPTIONS[name])\n",
    "\n",
    "    return images, texts\n",
    "\n",
    "def visualise_data(images_path, texts):\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    \n",
    "    for i, (image_path, text) in enumerate(zip(images_path, texts)):\n",
    "        # Load and visualize the image along with its text description\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(text)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.tight_layout()\n",
    "\n",
    "images_path, texts = get_data()\n",
    "visualise_data(images_path, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3dUG0XCIhLB"
   },
   "source": [
    "To obtain the features of the images, we need to preprocess them using the methods that we discussed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOevf0udIlTw"
   },
   "outputs": [],
   "source": [
    "def encode_data(images_path, texts):\n",
    "    # Preprocess the images to transform from filenames to images to tensors\n",
    "    images = [preprocess(Image.open(image_path)) for image_path in images_path]\n",
    "\n",
    "    # Preprocess the texts to transform from text to tensors\n",
    "    images = torch.tensor(np.stack(images)).cuda()\n",
    "    text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "    \n",
    "    # Encode the inputs\n",
    "    with torch.no_grad():\n",
    "        images_z = model.encode_image(images).float()\n",
    "        texts_z = model.encode_text(text_tokens).float()\n",
    "\n",
    "    return images_z, texts_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NriwdIEfJfiZ"
   },
   "source": [
    "We can now evaluate the similarity between the set of images and the set of textual descriptions that we created. We can expect that the similarity will be higher for the pairs that we plotted earlier.\n",
    "\n",
    "Initially, **it is vital** to normalize the features to make them \"compatible\" and facilitate reasoning in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJ2EaEMMJuNK",
    "outputId": "dd5df5bc-c483-425b-a331-6779063de3a1"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(images_z, texts_z):\n",
    "    # Normalise the image and the text\n",
    "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Evaluate the cosine similarity between the sets of features\n",
    "    similarity = (images_z @ texts_z.T)\n",
    "\n",
    "    return similarity.cpu()\n",
    "\n",
    "images_path, texts = get_data()\n",
    "images_z, texts_z = encode_data(images_path, texts)\n",
    "similarity = cosine_similarity(images_z, texts_z)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVt8Et6rKQ-Z"
   },
   "source": [
    "As anticipated, the similarity is higher on the diagonal, i.e., for the pairs that we defined earlier. We can also visualize these values to make it more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "I-J9mlVwKhXA",
    "outputId": "968d5f47-408c-4096-9385-e9ae2e791202"
   },
   "outputs": [],
   "source": [
    "def visualise_similarity(similarity, images_path, texts):\n",
    "    # Flip similarity (just for visualization)\n",
    "    similarity = similarity.permute(1, 0)\n",
    "    \n",
    "    similarity = similarity.numpy()\n",
    "    count = len(texts)\n",
    "\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "    plt.yticks(range(count), texts, fontsize=18)\n",
    "    plt.xticks([])\n",
    "    \n",
    "    for i, image_path in enumerate(images_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "        \n",
    "    # Print the scores\n",
    "    for x in range(similarity.shape[1]):\n",
    "        for y in range(similarity.shape[0]):\n",
    "            plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "    \n",
    "    # Update spines\n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        plt.gca().spines[side].set_visible(False)\n",
    "    \n",
    "    # Change plot limits\n",
    "    plt.xlim([-0.5, count - 0.5])\n",
    "    plt.ylim([count + 0.5, -2])\n",
    "    \n",
    "    # Set title\n",
    "    plt.title(\"Cosine similarity between text and image features\", size=20)\n",
    "\n",
    "visualise_similarity(similarity, images_path, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_E9EjC_MahU"
   },
   "source": [
    "It is evident that these scores provide valuable information, which could also be used for a classification task. For instance, consider the scenario where instead of descriptions such as \"a cup of coffee on a saucer,\" we had descriptions such as \"a photo of a mug,\" \"a photo of a horse,\" and \"a photo of a person.\" These three descriptions can effectively enable us to identify whether the image contains a mug, a horse, or a person.\n",
    "\n",
    "This highlights the potent zero-shot capabilities of CLIP, as it enables us to classify any image by simply creating a list of objects of interest and converting them into textual descriptions or prompts. By subsequently comparing these descriptions with the image and collecting similarity scores, we can directly classify an image as a mug if the score for the mug is the highest in the description set.\n",
    "\n",
    "Let's explore this further with an example on CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "gFR6TeMsNy3F",
    "outputId": "11cc80b3-9cde-4ddf-90a1-528cb40075f7"
   },
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"mnist\": torchvision.datasets.MNIST,\n",
    "    \"cifar10\": torchvision.datasets.CIFAR10,\n",
    "}\n",
    "\n",
    "def embed_dataset_classnames(dataset_name):\n",
    "    # Create the list of descriptions and tokenize them\n",
    "    dataset = DATASETS[dataset_name](\"./data\", transform=preprocess, download=True, train=False)\n",
    "    classnames = dataset.classes\n",
    "    descriptions = [f\"a photo of a {label}.\" for label in classnames]\n",
    "    text_tokens = clip.tokenize(descriptions).cuda()\n",
    "    \n",
    "    # Get the normalized textual features\n",
    "    with torch.no_grad():\n",
    "        texts_z = model.encode_text(text_tokens).float()\n",
    "        texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return classnames, descriptions, texts_z\n",
    "\n",
    "def visualise_probabilities(images_path, classnames, texts_p, k=5):\n",
    "    topk_p, topk_labels = texts_p.cpu().topk(k, dim=-1)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    for i, image_path in enumerate(images_path):\n",
    "        # Read the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "        # Visualise the image\n",
    "        plt.subplot(4, 4, 2 * i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Visualise the probabilities for the image\n",
    "        plt.subplot(4, 4, 2 * i + 2)\n",
    "        y = np.arange(topk_p.shape[-1])\n",
    "        plt.grid()\n",
    "        plt.barh(y, topk_p[i])\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        plt.yticks(y, [classnames[index] for index in topk_labels[i].numpy()])\n",
    "        plt.xlabel(\"probability\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Get the text descriptions and their embeddings\n",
    "classes, texts, texts_z = embed_dataset_classnames(\"cifar10\")\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Prompts (text): {texts}\")\n",
    "print(f\"Prompts (embedded): {texts_z.shape}\")\n",
    "\n",
    "# Evaluate the softmax from the cosine similarities\n",
    "similarity = cosine_similarity(images_z, texts_z)\n",
    "texts_p = (100 * similarity).softmax(dim=-1)\n",
    "\n",
    "# Visualise the similarity\n",
    "visualise_similarity(similarity, images_path, texts)\n",
    "\n",
    "# Visualise the top-5 predictions\n",
    "# visualise_probabilities(images_path, texts, texts_p, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McvsmgMeRSDC"
   },
   "source": [
    "It is noteworthy that the list of descriptions functions as if they were a classifier. Until now, we have observed and trained neural networks that can generally be broken down into an encoder network and a classifier network. However, as we saw in the last lab, if we have to adapt a pre-trained neural network for another task, we have to drop the classifier and retrain it from scratch.\n",
    "\n",
    "But, by design, CLIP has no classifier and can classify by merely changing the list of textual descriptions we use. Therefore, the list of textual descriptions can also be considered a flexible classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXFSH_xPSDbT"
   },
   "source": [
    "## Some considerations\n",
    "\n",
    "Thus far, we employed textual descriptions to classify images. However, the opposite can also be accomplished. What we did so far was compare an image with a list of textual descriptions and select the most similar text for that image. But, we can also do the exact opposite.\n",
    "\n",
    "This type of application may be useful in other scenarios, but it is merely an idea of what you can achieve by reasoning with the components you possess. Another example is that you can compare the similarity between images and images or between text and text to comprehend their degree of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RwdiM7nSlNw",
    "outputId": "52a7ba1b-d89d-46b0-cfc1-cd04b09b091c"
   },
   "outputs": [],
   "source": [
    "# Compare how text is similar to other text\n",
    "similarity = cosine_similarity(texts_z, texts_z)\n",
    "print(similarity)\n",
    "\n",
    "# Compare how images are similar one with the other\n",
    "similarity = cosine_similarity(images_z, images_z)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-N7zLBjVSjj"
   },
   "source": [
    "Furthermore, until now, we considered only a single \"prompt\" for each class name (i.e., \"a photo of a [CLS]\"), but we can use multiple templates to generate more textual features. For instance, in CLIP [1], they define a list of 80 templates for the ImageNet dataset, demonstrating that using the mean representation of the textual features leads to improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "id": "02C2GY5nXMN-",
    "outputId": "6fd16bbf-d938-4f95-95fa-c37fa002f34e"
   },
   "outputs": [],
   "source": [
    "def embed_dataset_classnames(dataset_name, templates=[\"a photo of a {}.\"]):\n",
    "    # Create the list of descriptions and tokenize them\n",
    "    dataset = DATASETS[dataset_name](\"./data\", transform=preprocess, download=True, train=False)\n",
    "    classnames = dataset.classes\n",
    "    \n",
    "    texts_z_views = []\n",
    "    for template in templates:\n",
    "        descriptions = [template.format(c) for c in classnames]\n",
    "        text_tokens = clip.tokenize(descriptions).cuda()\n",
    "        \n",
    "        # Get the normalized textual features\n",
    "        with torch.no_grad():\n",
    "            texts_z = model.encode_text(text_tokens).float()\n",
    "            texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "            texts_z_views.append(texts_z)\n",
    "\n",
    "    # Evaluate the mean representation\n",
    "    texts_z = torch.stack(texts_z_views).mean(dim=0)\n",
    "\n",
    "    # Renormalise\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return classnames, texts_z\n",
    "\n",
    "# Get the text descriptions and their embeddings\n",
    "texts, texts_z = embed_dataset_classnames(\n",
    "  \"cifar10\",\n",
    "  templates=[\"a photo of a {}\", \"a low-res picture of a {}\"]\n",
    ")\n",
    "\n",
    "# Evaluate the softmax from the cosine similarities\n",
    "similarity = cosine_similarity(images_z, texts_z)\n",
    "texts_p = (100 * similarity).softmax(dim=-1)\n",
    "\n",
    "# Visualise the top-5 predictions\n",
    "visualise_probabilities(images_path, texts, texts_p, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH6409jVbSvF"
   },
   "source": [
    "# Finetune CLIP\n",
    "\n",
    "Although CLIP lacks a linear layer for classifying samples, there are various ways to adapt CLIP for downstream tasks. While it's generally not recommended to completely fine-tune the entire network (as we did in the last lab with the AlexNet examples), there are several methods to adapt CLIP for transfer learning.\n",
    "\n",
    "One straightforward solution is to add a linear layer on top of the visual features of CLIP to classify a specific dataset.\n",
    "\n",
    "Let's consider an example with MNIST. To do this, we will reuse code from previous lab lessons.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to modify the get_data function from the old labs to support different datasets and custom transforms (since CLIP has its own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kl7Pz4LXcBpn"
   },
   "outputs": [],
   "source": [
    "def get_data(dataset_name, batch_size=64, transform=None, test_batch_size=256):\n",
    "    dataset = DATASETS[dataset_name]\n",
    "    \n",
    "    if not transform:\n",
    "        # Convert the PIL images to Tensors\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    \n",
    "    # Load data\n",
    "    full_training_data = dataset('./data', train=True, transform=transform, download=True)\n",
    "    test_data = dataset('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    # Create train and validation splits\n",
    "    num_samples = len(full_training_data)\n",
    "    training_samples = int(num_samples * 0.5 + 1)\n",
    "    validation_samples = num_samples - training_samples\n",
    "    \n",
    "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
    "    \n",
    "    # Initialize dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=8)\n",
    "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=8)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NSJITODfJUb"
   },
   "source": [
    "Next, we write a custom `test_step` function to evaluate how well zero-shot CLIP performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyscPrc6dWR4"
   },
   "outputs": [],
   "source": [
    "def test_step_zero_shot_clip(net, data_loader, texts_z, device='cuda'):\n",
    "    samples = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(data_loader), total=len(data_loader), position=0, leave=True):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            images_z = model.encode_image(inputs).float()\n",
    "            outputs = (100 * images_z @ texts_z.T).softmax(dim=-1)\n",
    "\n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkNIVT-HkPX5"
   },
   "source": [
    "And we evaluate the performance of zero-shot CLIP on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUl-n-4Wdepw",
    "outputId": "feab44d6-6b4e-4d3e-af7e-32247d6212ae"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"cifar10\"\n",
    "\n",
    "_, _, test_loader = get_data(dataset_name, transform=preprocess, batch_size=128)\n",
    "texts, texts_z = embed_dataset_classnames(dataset_name)\n",
    "test_accuracy = test_step_zero_shot_clip(model, test_loader, texts_z)\n",
    "\n",
    "print(f\"Test accuracy {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB5Y3rdTftVf"
   },
   "source": [
    "Now, let's attempt to add a linear layer on top of the visual encoder of CLIP and see if we can enhance performance.\n",
    "\n",
    "The initial step is to create a custom neural network that builds upon the visual encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knK9SPQmf7Dz"
   },
   "outputs": [],
   "source": [
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        model, _ = clip.load(\"RN50\")\n",
    "\n",
    "        self.encoder = model.visual.float()\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-fIg2Mxg6SH"
   },
   "source": [
    "Next, we can define the other components of training, such as the optimizer, the loss function, and the training and testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnhRXPQGcaDI"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr, wd, momentum):\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": lr}\n",
    "    ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdLqrPalccKt"
   },
   "outputs": [],
   "source": [
    "def get_cost_function():\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "    return cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ4LfbYVcglS"
   },
   "outputs": [],
   "source": [
    "def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    \n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "    \n",
    "    # Iterate over the training set\n",
    "    pbar = tqdm(data_loader, desc=\"Training\", position=0, leave=True, total=len(data_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameters update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples * 100)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=\"cuda\"):\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "    \n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    pbar = tqdm(data_loader, desc=\"Testing\", position=0, leave=True, total=len(data_loader))\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test set\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            # Load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            # Loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "            \n",
    "            # Fetch prediction and loss value\n",
    "            samples += inputs.shape[0]\n",
    "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_postfix(test_acc=cumulative_accuracy / samples * 100)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibf7SWZYhDT9"
   },
   "source": [
    "## Put it all together!\n",
    "\n",
    "We need a compact procedure to apply all the components and functions defined so far into the actual optimization procedure. In particular, we want our model to iterate over training step and test step for multiple epochs, tracking the partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrWTF3OzhKE2"
   },
   "outputs": [],
   "source": [
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "def main(\n",
    "    dataset_name=\"cifar10\",\n",
    "    batch_size=16,\n",
    "    num_classes=10,\n",
    "    device='cuda:0',\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp1\",\n",
    "):\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(dataset_name, transform=preprocess, batch_size=batch_size)\n",
    "    \n",
    "    # Instantiate the network and move it to the chosen device (GPU)\n",
    "    net = OurCLIP(num_classes=num_classes).to(device)\n",
    "\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "    \n",
    "    # Define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "    \n",
    "    # Computes evaluation results before training\n",
    "    print(\"Before training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "    \n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    \n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    print(\"After training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "    \n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    \n",
    "    # closes the logger\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZAR2vfdhdg5"
   },
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uB7Dt49PhhUh",
    "outputId": "4cef600b-64ae-4ab8-fe3f-9aff8984074e"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Optimization (CoOp)\n",
    "We will now see how to implement CoOp and use it to improve the performance of CLIP on downstream datasets.\n",
    "CoOp aims to perform prompt tuning by learning prompts while keeping CLIP frozen. To do so, we need to make some slight changes to the CLIP pipeline, and we will introduce some new components to perform prompt tuning.\n",
    "\n",
    "NOTE: the following is a simpler version of the original code of CoOp available [here](https://github.com/KaiyangZhou/CoOp/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "\n",
    "_tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The text encoder\n",
    "While we don't want to finetune the text encoder, we still need to process the \"CoOp\" prompts (i.e., those we want to learn) through the encoder. Since CLIP is not designed for this, we will wrap the original text encoder of CLIP into our own custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our `PromptLearner`, a class that holds the learnable prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "\n",
    "        # Use given words to initialize context vectors\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            if csc:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
    "\n",
    "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f\"Initial context: '{prompt_prefix}'\")\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        # These are the `prompts` we want to optimize\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        # print(\"+++\")\n",
    "        # print(\"Prompts:\")\n",
    "        # for p in prompts:\n",
    "        #     print(p)\n",
    "        # print(\"+++\")\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx\n",
    "        \n",
    "        # If CoOp, expand the ctx for all classes\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "        \n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,     # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,     # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,      # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,     # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,   # (1, name_len, dim)\n",
    "                        ctx_i,     # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine our CLIP model with the updated TextEncoder and the new PromptLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCLIP(nn.Module):\n",
    "    def __init__(self, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
    "        super().__init__()\n",
    "        clip_model, _ = clip.load(\"RN50\")\n",
    "        # clip_model = clip_model.cpu()\n",
    "        clip_model = clip_model.float()\n",
    "        \n",
    "        self.prompt_learner = PromptLearner(clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr, wd, momentum):\n",
    "    optimizer = torch.optim.SGD([\n",
    "        {\"params\": model.parameters()}\n",
    "    ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def main_coop(\n",
    "    dataset_name=\"cifar10\",\n",
    "    batch_size=16,\n",
    "    num_classes=10,\n",
    "    device=\"cuda:0\",\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    "    epochs=2,\n",
    "    run_name=\"exp1\",\n",
    "    n_ctx=4,\n",
    "    ctx_init=\"\",\n",
    "    class_token_position=\"end\",\n",
    "    csc=False,\n",
    "):\n",
    "    # Create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_data(dataset_name, transform=preprocess, batch_size=batch_size)\n",
    "    classnames, _ = embed_dataset_classnames(dataset_name)\n",
    "    \n",
    "    # Instantiate the network and move it to the chosen device (GPU)\n",
    "    net = OurCLIP(\n",
    "        classnames=classnames, n_ctx=n_ctx, ctx_init=ctx_init, class_token_position=class_token_position, csc=csc\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"Turning off gradients in both the image and the text encoder\")\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "    \n",
    "    print(f\"Total parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # Instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "    \n",
    "    # Define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "    \n",
    "    # Computes evaluation results before training\n",
    "    print(\"Before training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "    \n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    \n",
    "    # For each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        log_values(writer, e, train_loss, train_accuracy, \"train\")\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "    # Compute final evaluation results\n",
    "    print(\"After training:\")\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "    \n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n",
    "    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n",
    "    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n",
    "    \n",
    "    # Closes the logger\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_coop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfhG0evjBTVs"
   },
   "source": [
    "# References\n",
    "\n",
    "1. Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
